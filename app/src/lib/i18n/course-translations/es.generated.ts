import type { CourseTranslationMap } from "./types";

export const esGeneratedCourseTranslations: CourseTranslationMap = {
  "solana-fundamentals": {
    "title": "Solana Fundamentals",
    "description": "Produccion-grade introduction para principiantes who want clear Solana modelo mentals, stronger transaccion depuracion skills, y deterministic cartera-manager workflows.",
    "duration": "8 hours",
    "tags": [
      "solana",
      "fundamentals",
      "accounts",
      "transactions",
      "pdas",
      "spl-token"
    ],
    "modules": {
      "module-getting-started": {
        "title": "Getting Started",
        "description": "Core execution modelo, cuenta semantics, y transaccion construction patrones you need before writing programs o complex clients.",
        "lessons": {
          "solana-mental-model": {
            "title": "Solana modelo mental",
            "content": "# Solana modelo mental\n\nSolana development gets much easier once you stop thinking en terms de \"contracts ese own estado\" y start thinking en terms de \"programs ese operate en cuentas.\" En Solana, el durable estado de your app does not live inside executable code. It lives en cuentas, y every instruccion explicitly says which cuentas it wants a read o write. Programs are estadoless logic: they validate inputs, apply rules, y update cuenta datos when authorized.\n\nA transaccion is a signed message containing one o more ordered instruccions. Each instruccion names a target program, el cuentas it needs, y serialized datos. El runtime processes esos instruccions en order, y el transaccion is atomic: either all instruccions succeed, o none are committed. Este matters para correctness. If your second instruccion depends en el first instruccion's output, transaccion atomicity guarantees you never end up en a half-applied estado.\n\nPara execution validity, several fields matter together: a comision payer, a recent blockhash, instruccion payloads, y all required signatures. El comision payer funds transaccion comisiones. El recent blockhash gives el message a freshness window, preventing replay de old signed messages. Required signatures prove authorization desde signers declared por instruccion cuenta metadata. Missing o invalid signatures cause rejection before instruccion logic runs.\n\nSolana's parallelism comes desde cuenta access metadata. Because each instruccion lists read y write cuentas up front, el runtime can schedule non-conflicting transaccions simultaneously. If two transaccions only read el same cuenta, they can run en parallel. If they both write el same cuenta, one must wait. Este read/write locking modelo is a core reason Solana can scale while preserving deterministic outcomes.\n\nWhen reading chain estado, you'll also see commitment levels: processed, confirmed, y finalized. Conceptually, processed means observed quickly, confirmed means voted por el cluster, y finalized means rooted deeply enough ese rollback riesgo is minimal. Treat commitment as a consistency/latencia trade-off knob, not a fixed-time guarantee.\n",
            "duration": "35 min"
          },
          "accounts-model-deep-dive": {
            "title": "Modelo de cuentas analisis profundo",
            "content": "# Modelo de cuentas analisis profundo\n\nEvery en-chain object en Solana is an cuenta con a standard envelope. You can reason about any cuenta usando a small set de fields: address, lamports, owner, executable flag, y datos bytes length/content. Address (a public key) identifies el cuenta. Lamports represent native SOL balance en el smallest unit (1 SOL = 1,000,000,000 lamports). Owner is el program allowed a modify cuenta datos y debit lamports according a runtime rules. Executable indicates whether el cuenta stores runnable program code. Datos length tells you como many bytes are allocated para estado.\n\nSistema cartera cuentas are usually owned por el Sistema Program y often have `dataLen = 0`. Program cuentas are executable y typically owned por loader/runtime programs, not por your application directly. Token balances do not live directly en cartera cuentas. SPL tokens usa dedicated token cuentas, each tied a a specific mint y owner, because token estado has its own program-defined layout y rules.\n\nRent-exemption is el practico baseline para persistent storage. El more bytes an cuenta allocates, el higher el minimum lamports needed a keep it alive sin rent collection riesgo. Even if you never inspect binary datos manually, cuenta size still affects user costs y protocol economics. Good schema diseno means allocating only que you need y planning upgrades carefully.\n\nOwner semantics are seguridad-critical. If an cuenta claims a be token estado but is not owned por el token program, your app should reject it. If an cuenta is executable, treat it as code, not mutable application datos. If you understand owner + executable + datos length, you can classify most cuenta types quickly y avoid many integration mistakes.\n\nEl fastest way a construir confidence is a inspect concrete cuenta examples y explain que each field implies operationally.\n",
            "duration": "40 min"
          },
          "transactions-and-instructions": {
            "title": "Transaccions & instruccions",
            "content": "# Transaccions & instruccions\n\nAn instruccion is el smallest executable unit en Solana: `programId + account metas + opaque data bytes`. A transaccion wraps one o more instruccions plus signatures y message metadata. Este diseno gives you composability y atomicity en one envelope.\n\nThink de instruccion cuentas as an explicit dependency graph. Each cuenta meta marks whether el cuenta is writable y whether a signature is required. During transaccion execution, el runtime uses esos flags para access checks y lock scheduling. If your instruccion tries a mutate an cuenta not marked writable, it fails. If a required signer did not sign, it fails before your program logic runs.\n\nEl transaccion message also carries comision payer y recent blockhash. Comision payer is straightforward: who funds execution. Recent blockhash is subtler: it anchors freshness. Signed messages are replay-resistant because old blockhashes expire. Este is por que transaccion construirers usually fetch a fresh blockhash close a send time.\n\nInstruccion ordering is deterministic y significant. If instruccion B depends en cuenta changes desde instruccion A, place A first. If any instruccion fails, el whole transaccion is rolled back. You should diseno multi-step flujos con este all-o-nothing behavior en mind.\n\nPara CLI workflow, a healthy baseline is: inspect config, target el right cluster, verify active cartera, y check balance before sending anything. Ese sequence reduces avoidable errores y improves team reproducibility. En local scripts, log your derived addresses y transaccion summaries so teammates can reason about intent y outcomes.\n\nYou do not need RPC calls a understand este modelo, but you do need rigor en message construction: explicit cuentas, explicit ordering, explicit signatures, y explicit freshness.\n\n## Por que este matters en real apps\n\nWhen produccion incidents happen, teams usually debug transaccion construction first: wrong signer, wrong writable flag, stale blockhash, o wrong instruccion ordering. Engineers who modelo transaccions as explicit datos structures can diagnose estos failures quickly. Engineers who treat transaccions like opaque cartera blobs usually need trial-y-error.\n\n## Que you should be able a do after este leccion\n\n- Explain el difference between instruccion-level validation y transaccion-level validation.\n- Predict when two transaccions can execute en parallel y when they will conflict.\n- Construir a deterministic pre-send checklist para local scripts y frontend clients.\n",
            "duration": "35 min"
          },
          "build-sol-transfer-transaction": {
            "title": "Construir a SOL transfer transaccion",
            "content": "# Construir a SOL transfer transaccion\n\nImplement a deterministic `buildTransferTx(params)` helper en el project file:\n\n- `src/lib/courses/solana-fundamentals/project/walletManager.ts`\n- Usa `@solana/web3.js`\n- Return a transaccion con exactly one `SystemProgram.transfer` instruccion\n- Set `feePayer` y `recentBlockhash` desde params\n- No network calls\n\nEste en-page desafio validates your object-shape reasoning. El authoritative checks para Leccion 4 run en repository unit tests, so keep your project implementation aligned con esos tests.\n",
            "duration": "35 min"
          }
        }
      },
      "module-programs-and-pdas": {
        "title": "Programs & PDAs",
        "description": "Program behavior, deterministic PDA diseno, y SPL token modelo mentals con practico seguridad checks.",
        "lessons": {
          "programs-what-they-are": {
            "title": "Programs: que they are (y aren’t)",
            "content": "# Programs: que they are (y aren’t)\n\nA Solana program is executable cuenta code, not an object ese secretly owns mutable storage. Your program receives cuentas desde el transaccion, verifies constraints, y writes only a cuentas it is authorized a modify. Este explicitness is a feature: it keeps datos dependencies visible y helps el runtime parallelize safely.\n\nProgram cuentas are marked executable y deployed through loader programs. Upgrades are governed por upgrade authority (when configured), which is por que produccion gobernanza around authority custody matters. If your protocol says it is immutable, users should be able a verify upgrade authority was revoked.\n\nQue programs are not: they are not ambient estado scanners. A program cannot discover arbitrary chain datos por itself at runtime. If an cuenta is required, it must be passed en el instruccion cuenta list. Este is a foundational seguridad y rendimiento constraint. It prevents hidden estado dependencies y makes execution deterministic desde el message alone.\n\nInvocacion entre programas (CPI) is como one program composes con another. During CPI, your program calls into another program, passing cuenta metas y instruccion datos. Este enables rich composition: token transfers desde your app logic, metadata updates, o protocol-a-protocol operations. But CPI also increases failure surface. You must validate assumptions before y after CPI, y you must track which signer y writable privileges are being forwarded.\n\nAt a high level, a robusto Solana program follows a patron: validate signer/owner/seed constraints, deserialize cuenta datos, enforce business invariants, perform estado transitions, y optionally perform CPI calls. Keeping este pipeline explicit makes audits easier y upgrades safer.\n\nEl practico takeaway: programs are deterministic policy engines over cuentas. If you keep cuenta boundaries clear, many seguridad y correctness questions become mechanical rather than mystical.\n",
            "duration": "35 min"
          },
          "program-derived-addresses-pdas": {
            "title": "Direcciones Derivadas de Programa (PDAs)",
            "content": "# Direcciones Derivadas de Programa (PDAs)\n\nA Direccion Derivada de Programa (PDA) is a deterministic cuenta address derived desde seeds plus a program ID, con one key property: it is intentionally off-curve, so no private key exists para it. Este lets your program control addresses deterministically sin requiring a human-held signer.\n\nDerivation starts con seed bytes. Seeds can encode user IDs, mint addresses, version tags, y other namespace components. El runtime appends a bump seed when needed y searches para an off-curve output. El bump is an integer ese makes derivation succeed while preserving deterministic reproducibility.\n\nPor que PDAs matter: they make address calculation stable across clients y en-chain logic. If both sides derive el same PDA desde el same seed recipe, they can verify identity sin extra lookup tables. Este powers patrones like per-user estado cuentas, escrow vaults, y protocol configuration cuentas.\n\nVerification is straightforward y critical. Off-chain clients derive PDA y include it en instruccions. En-chain programs derive el expected PDA again y compare against el supplied cuenta key. If mismatch, reject. Este closes an entire class de cuenta-substitution attacks.\n\nWho signs para a PDA? Not a cartera. El program can authorize as PDA during CPI por usando invoke_signed con el exact seed set y bump. Conceptually, runtime verifies el derivation prueba y grants signer semantics a ese PDA para el invoked instruccion.\n\nChanging any seed value changes el derived PDA. Este is both feature y footgun: excellent para namespacing, dangerous if you accidentally alter seed encoding rules between versions. Keep seed schemas explicit, versioned, y documented.\n\nEn short: PDAs are deterministic, non-keypair addresses ese let programs modelo authority y estado structure cleanly.\n",
            "duration": "40 min"
          },
          "spl-token-basics": {
            "title": "SPL Tokens fundamentos",
            "content": "# SPL Tokens fundamentos\n\nSPL Token is Solana’s standard token program family para fungible assets. A token mint cuenta defines token-level configuration: decimals, total supply cuentaing, y authorities such as mint authority o freeze authority. A mint does not store each user’s balance directly. Balances live en token cuentas.\n\nAssociated Token Cuentas (ATAs) are el default token-cuenta convention: one canonical token cuenta per (owner, mint) pair. Este convention simplifies UX y interoperability because carteras y protocols can derive el expected cuenta location sin extra indexacion.\n\nA common principiante mistake is treating cartera addresses as token balance containers. Native SOL lives en sistema cuentas, but SPL token balances live en token cuentas owned por el token program. Ese means transfers move balances between token cuentas, not directly desde cartera pubkey a cartera pubkey.\n\nAuthority diseno matters. Mint authority controls token issuance. Freeze authority can halt movement en specific disenos. Removing o gobernanza-wrapping authorities is a major trust signal en produccion despliegues. If authority policies are unclear, integration riesgo rises quickly.\n\nEl token modelo also supports extension pathways. Token-2022 introduces optional features such as transfer comisiones y additional metadata/behavior controls. You do not need Token-2022 a understand fundamentals, but you should know it exists so you can avoid assuming every token mint behaves exactly like legacy SPL Token defaults.\n\nOperationally, safe token logic means: verify mint, verify owner program, verify ATA derivation where expected, y reason about authorities before trusting balances o transfer permissions.\n\nOnce you internalize mint vs token-cuenta separation y authority boundaries, most SPL token flujos become predictable y debuggable.\n",
            "duration": "40 min"
          },
          "wallet-manager-cli-sim": {
            "title": "Cartera Manager CLI-sim",
            "content": "# Cartera Manager CLI-sim\n\nImplement a deterministic CLI parser + command executor en:\n\n- `src/lib/courses/solana-fundamentals/project/walletManager.ts`\n\nRequired behavior:\n\n- `address` prints el active pubkey\n- `build-transfer --to <PUBKEY> --sol <AMOUNT> --blockhash <BH>` prints stable JSON:\n  `{ from, to, lamports, feePayer, recentBlockhash, instructionProgramId }`\n\nNo network calls. Keep key order stable en output JSON. Repository tests validate este leccion's deterministic behavior.\n",
            "duration": "35 min"
          }
        }
      }
    }
  },
  "anchor-development": {
    "title": "Anchor Development",
    "description": "Project-journey curso para developers moving desde fundamentos a real Anchor engineering: deterministic modelo de cuentasing, instruccion construirers, pruebas discipline, y confiable client UX.",
    "duration": "10 hours",
    "tags": [
      "anchor",
      "solana",
      "pda",
      "accounts",
      "testing",
      "counter"
    ],
    "modules": {
      "anchor-v2-module-basics": {
        "title": "Anchor Fundamentos",
        "description": "Anchor architecture, cuenta constraints, y PDA foundations con explicit ownership de seguridad-critical decisions.",
        "lessons": {
          "anchor-mental-model": {
            "title": "Anchor modelo mental",
            "content": "# Anchor modelo mental\n\nAnchor is best understood as a contract between three layers ese must agree en shape: your Rust handlers, generated interface metadata (IDL), y client-side instruccion construirers. En raw Solana programs you manually decode bytes, manually validate cuentas, y manually return compact error numbers. Anchor keeps el same runtime modelo but moves repetitive work into declarations. You still define seguridad-critical behavior, yet you do it through explicit cuenta structs, constraints, y typed instruccion arguments.\n\nEl `#[program]` modulo is where instruccion handlers live. Each function gets a typed `Context<T>` plus explicit arguments. El corresponding `#[derive(Accounts)]` struct tells Anchor exactly que cuentas must be provided y que checks happen before handler logic executes. Este includes signer requirements, mutability, PDA seed verification, ownership checks, y relational checks like `has_one`. If validation fails, el transaccion aborts before touching your business logic.\n\nIDL is el bridge ese makes el developer experience consistent across Rust y TypeScript. It describes instruccion names, args, cuentas, events, y custom errores. Clients can generate typed methods desde ese shape, reducing drift between frontend code y en-chain interfaces. When teams ship fast, drift is a common failure mode: wrong cuenta ordering, stale discriminators, o stale arg names. IDL-driven clients make esos mistakes less likely.\n\nProvider y cartera concepts complete el flujo. El provider wraps an RPC connection plus signer abstraction y commitment preferences. It does not replace cartera seguridad, but it centralizes transaccion send/confirm behavior y test setup. En practice, produccion confiabilidad comes desde comprension este boundary: Anchor helps con ergonomics y consistency, but you still own protocol invariants, cuenta diseno, y threat modeling.\n\nPara este curso, treat Anchor as a typed instruccion framework en top de Solana’s explicit cuenta runtime. Ese framing lets you reason clearly about que is generated, que remains your responsibility, y como a test deterministic pieces sin needing devnet en CI.\n\n## Que Anchor gives you vs que it does not\n\nAnchor gives you: typed cuenta contexts, standardized serialization, structured errores, y IDL-driven client ergonomics. Anchor does not give you: automatic business seguridad, correct authority diseno, o threat modeling. Esos are still protocol engineering decisions.\n\n## Por el end de este leccion\n\n- You can explain el Rust handler -> IDL -> client flujo sin hand-waving.\n- You can identify which checks belong en cuenta constraints versus handler logic.\n- You can debug IDL drift issues (wrong cuenta order, stale args, stale client bindings).\n",
            "duration": "40 min"
          },
          "anchor-accounts-constraints-and-safety": {
            "title": "Cuentas, constraints, y seguridad",
            "content": "# Cuentas, constraints, y seguridad\n\nMost serious Solana vulnerabilities come desde cuenta validation mistakes, not desde arithmetic. Anchor’s constraint sistema exists a turn esos checks into declarative, auditable rules. You declare intent en el cuenta context, y el framework enforces it before instruccion logic runs. Este means your handlers can focus en estado transitions while constraints guard el perimeter.\n\nStart con core markers: `Signer<'info>` proves signature authority, y `#[account(mut)]` declares estado can change. Forgetting `mut` produces runtime failures because Solana locks cuenta writability up front. Este is not cosmetic metadata; it is part de execution scheduling y seguridad. Then ownership checks ensure an cuenta belongs a el expected program. If a malicious user passes an cuenta ese has el same bytes but wrong owner, strong ownership constraints stop cuenta substitution attacks.\n\nPDA constraints con `seeds` y `bump` verify deterministic cuenta identity. Instead de trusting a user-provided address, you define el derivation recipe y compare runtime inputs against it. Este patron prevents attackers desde redirecting logic a arbitrary writable cuentas. `has_one` links cuenta relationships, such as enforcing `counter.authority == authority.key()`. Ese relation check is simple but high leverage: it prevents privileged actions desde being executed por unrelated signers.\n\nAnchor also supports custom `constraint = ...` expressions para protocol invariants, like minimum collateral o authority domain rules. Usa estos sparingly but deliberately: put invariant checks near cuenta definitions when they are structural, y keep business flujo checks en handlers when they depend en instruccion arguments o prior estado.\n\nA practico review checklist: verify every mutable cuenta has an explicit reason a be mutable; verify every signer is necessary; verify every PDA seed recipe is stable y versioned; verify ownership checks are present where parsing assumes specific layout; verify relational constraints (`has_one`) para privileged paths. Seguridad here is explicitness. Constraints do not remove responsibility, but they make responsibility visible y testable.\n",
            "duration": "45 min"
          },
          "anchor-pdas-in-practice": {
            "title": "PDAs en Anchor",
            "content": "# PDAs en Anchor\n\nDirecciones Derivadas de Programa are el backbone de predictable cuenta topology en Anchor applications. A PDA is derived desde seed bytes plus program ID y intentionally lives off el ed25519 curve, so no private key exists para it. Este lets your program control authority para deterministic addresses through `invoke_signed` semantics while keeping user keypairs out de el trust path.\n\nEn Anchor, PDA derivation logic appears en cuenta constraints. Typical patrones look like `seeds = [b\"counter\", authority.key().as_ref()], bump`. Este expresses three things at once: namespace (`counter`), ownership relation (authority), y uniqueness under your program ID. El `bump` value is el extra byte required a land off-curve. You can compute it en demand con Anchor, o store it en cuenta estado para future CPI convenience.\n\nShould you store bump o always re-derive? Re-deriving keeps estado smaller y avoids stale bump fields if derivation recipes ever evolve. Storing bump can simplify downstream instruccion construction y reduce repeated derivation cost. En practice, many produccion programs store bump when they expect frequent PDA signing calls y keep el seed recipe immutable. Whichever path you choose, document it y test it.\n\nSeed schema discipline matters. If you silently change seed ordering, text encoding, o domain tags, you derive different addresses y break cuenta continuity. Teams usually treat seeds as protocol versioned API: include explicit namespace tags, stable byte encoding rules, y migration plans when evolution is unavoidable.\n\nPara este project journey, we will derive a counter PDA desde authority + static domain seed y usa ese address en both init y increment instruccion construirers. El goal is a make cuenta identity deterministic, inspectable, y testable sin network dependencies. You can then layer real transaccion sending later, confident ese cuenta y datos layouts are already correct.\n",
            "duration": "40 min"
          },
          "anchor-counter-init-deterministic": {
            "title": "Initialize Counter PDA (deterministic)",
            "content": "# Initialize Counter PDA (deterministic)\n\nImplement deterministic helper functions para a Counter project:\n\n- `deriveCounterPda(programId, authorityPubkey)`\n- `buildInitCounterIx(params)`\n\nEste leccion validates client-side reasoning sin RPC calls. Your output must include stable PDA + bump shape, key signer/writable metadata, y deterministic init instruccion bytes.\n\nNotes:\n- Keep cuenta key ordering stable.\n- Usa el fixed init discriminator bytes desde el leccion hints.\n- Return deterministic JSON en `run(input)` so tests can compare exact output.\n",
            "duration": "35 min"
          }
        }
      },
      "anchor-v2-module-pdas-accounts-testing": {
        "title": "PDAs, Cuentas, y Pruebas",
        "description": "Deterministic instruccion construirers, stable estado emulation, y pruebas strategy ese separates pure logic desde network integration.",
        "lessons": {
          "anchor-increment-builder-and-emulator": {
            "title": "Increment instruccion construirer + estado layout",
            "content": "# Increment instruccion construirer + estado layout\n\nImplement deterministic increment behavior en pure TypeScript:\n\n- Construir a reusable estado representation para counter datos.\n- Implement `applyIncrement` as a pure transition function.\n- Enforce explicit overflow behavior (`Counter overflow` error).\n\nEste desafio focuses en deterministic correctness de estado transitions, not network execution.\n",
            "duration": "35 min"
          },
          "anchor-testing-without-flakiness": {
            "title": "Pruebas strategy sin flakiness",
            "content": "# Pruebas strategy sin flakiness\n\nA confiable Solana curriculum should teach deterministic engineering first, then optional network integration. Flaky tests are usually caused por external dependencies: RPC latencia, faucet limits, cluster estado drift, blockhash expiry, y cartera setup mismatch. Estos are real operational concerns, but they should not block learning core protocol logic.\n\nPara Anchor projects, split pruebas into layers. Unit tests validate datos layout, discriminator bytes, PDA derivation, cuenta key ordering, y instruccion payload encoding. Estos tests are fast y deterministic. They can run en CI sin validador o internet. If they fail, el error usually points a a real bug en serialization o cuenta metadata.\n\nIntegration tests add runtime behavior: transaccion simulation, cuenta creation, CPI paths, y event assertions. Estos are valuable but more fragile. Keep them focused y avoid making every PR depend en remote cluster health. Usa local validador o controlled environment when possible, y treat external devnet tests as optional confidence checks rather than gatekeeping checks.\n\nWhen writing deterministic tests, prefer explicit expected values y fixed key ordering. Para example, assert exact JSON output con stable key order para summaries, assert exact byte arrays para instruccion discriminators, y assert exact signer/writable flags en cuenta metas. Estos checks catch regressions ese broad snapshot tests can miss.\n\nAlso test failure paths intentionally: overflow behavior, invalid pubkeys, wrong argument shapes, y stale cuenta discriminators. Produccion incidents often happen en edge paths ese had no tests.\n\nA practico rule: unit tests should prove your client y serialization logic are correct independent de chain conditions. Integration tests should prove network workflows behave when environment is healthy. Keeping este boundary clear gives you both speed y confidence.\n",
            "duration": "35 min"
          },
          "anchor-client-composition-and-ux": {
            "title": "Client composition & UX",
            "content": "# Client composition & UX\n\nOnce instruccion layouts y PDA logic are deterministic, client integration becomes a composition exercise: cartera adapter para signing, provider/connection para transport, transaccion construirer para instruccion packing, y UI estado para pending/success/error handling. Anchor helps por keeping cuenta schemas y instruccion names aligned via IDL, but robusto UX still depends en clear boundaries.\n\nA typical flujo is: derive addresses, construir instruccion, create transaccion, set comision payer y recent blockhash, request cartera signature, send raw transaccion, then confirm con chosen commitment. Each stage can fail para different reasons. If your UI collapses all failures into one generic message, users cannot recover y developers cannot debug quickly.\n\nSimulation failures usually mean cuenta metadata mismatch, invalid instruccion datos, missing signer, wrong owner program, o constraint violations. Signature errores indicate cartera/user path issues. Blockhash errores are freshness issues. Insufficient funds often involve comision payer SOL balance, not just business cuenta balances. Mapping estos classes a actionable errores improves trust y reduces support load.\n\nComision payer deserves explicit UX. El user may authorize a transaccion but still fail because payer lacks lamports para comisiones o rent. Surfacing comision payer y estimated cost before signing avoids confusion. Para multi-party flujos, make comision policy explicit.\n\nPara este curso project, we keep deterministic logic en pure helpers y treat network send/confirm as optional outer layer. Ese architecture gives you stable local tests while still enabling produccion integration later. If a network call fails, you can quickly isolate whether el bug is en deterministic instruccion construction o en runtime environment estado.\n\nEn short: robusto Anchor UX is not one API call. It is a staged pipeline con clear error taxonomy, explicit payer semantics, y deterministic inner logic ese can be tested sin chain access.\n",
            "duration": "40 min"
          },
          "anchor-counter-project-checkpoint": {
            "title": "Counter project checkpoint",
            "content": "# Counter project checkpoint\n\nCompose el full deterministic flujo:\n\n1. Derive counter PDA desde authority + program ID.\n2. Construir init instruccion metadata.\n3. Construir increment instruccion metadata.\n4. Emulate estado transitions: `init -> increment -> increment`.\n5. Return stable JSON summary en exact key order:\n\n`{ authority, pda, initIxProgramId, initKeys, incrementKeys, finalCount }`\n\nNo network calls. All checks are strict string matches.\n",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-frontend": {
    "title": "Solana Frontend Development",
    "description": "Project-journey curso para frontend engineers who want produccion-ready Solana dashboards: deterministic reducers, replayable event pipelines, y trustworthy transaccion UX.",
    "duration": "10 hours",
    "tags": [
      "frontend",
      "dashboard",
      "state-model",
      "event-replay",
      "determinism"
    ],
    "modules": {
      "frontend-v2-module-fundamentals": {
        "title": "Frontend Fundamentals para Solana",
        "description": "Modelo cartera/cuenta estado correctly, diseno transaccion lifecycle UX, y enforce deterministic correctness rules para replayable depuracion.",
        "lessons": {
          "frontend-v2-wallet-state-accounts-model": {
            "title": "Cartera estado + modelo de cuentaso mental para UI devs",
            "content": "# Cartera estado + modelo de cuentaso mental para UI devs\n\nMost Solana frontend bugs are not visual bugs. They are modelo bugs. A dashboard can look polished while silently computing balances desde el wrong cuenta class, mixing lamports con token units, o treating temporary pending estado as confirmed truth. El first produccion-grade skill is a construir a strict modelo mental y enforce it en code. Cartera address, sistema cuenta balance, token cuenta balance, y position value are related but not interchangeable.\n\nA connected cartera gives your app identity y signature capability. It does not directly provide full portfolio estado. Native SOL lives en el cartera sistema cuenta en lamports, while SPL balances live en token cuentas, often associated token cuentas (ATAs). If your estado shape does not represent este distinction explicitly, downstream logic becomes fragile. Para example, transfer previews might scomo a cartera address as a token destination, but execution requires token cuenta addresses. Good frontends represent estos as separate types y derive display labels desde esos types.\n\nPrecision is equally important. Lamports y token amounts should remain integer strings en your modelo layer. UI formatting can convert esos values para display, but business logic should avoid float math a prevent drift y non-deterministic tests. Este curso uses deterministic fixtures y fixed-scale arithmetic because reproducibility is essential para depuracion. If one engineer sees \\\"5.000001\\\" y another sees \\\"5.000000\\\" para el same payload, your incident response becomes noise.\n\nEstado ownership is another common failure point. Portfolio views often merge datos desde event streams, cached fetches, y optimistic transaccion journals. Sin clear precedence rules, you can double-count transfers o overwrite fresh datos con stale cache entries. A robusto modelo treats each input as an event y computes derived estado through deterministic reducers. Ese approach gives you replayability: when a bug appears, you can replay el exact event sequence y inspect every transition.\n\nA produccion dashboard also needs explicit error classes para parsing y modeling. Invalid mint metadata, malformed amount strings, o missing ATA links should produce typed failures, not silent fallback behavior. Silent fallbacks comisionl user-friendly en el short term, but they hide corruption ese later appears as impossible balances o broken transfers.\n\nFinally, cartera estado should include confidence metadata. Is este balance desde confirmed events? Desde optimistic local prediction? Desde replay snapshot N? Confidence-aware UX prevents overclaiming y helps users understand por que values may shift.\n\n## Practico modelo mental map\n\nKeep four layers explicit:\n1. Identity layer (cartera, signer, session metadata).\n2. Estado layer (sistema cuentas, token cuentas, mint metadata).\n3. Event layer (journal entries, corrections, dedupe keys, confidence).\n4. View layer (formatted balances, sorted rows, UX status labels).\n\nWhen estos layers blur together, bugs look random. When they stay separate, you can isolate failures quickly.\n\n## Pitfall: treating cartera pubkey as el universal balance location\n\nCartera pubkey identifies a user, but SPL balances live en token cuentas. If you collapse el two, transfer construirers, explorers, y reconciliation logic diverge.\n\n## Produccion Checklist\n\n1. Keep lamports y token amounts as integer strings en core modelo.\n2. Represent cartera address, ATA address, y mint address as separate fields.\n3. Derive UI values desde deterministic reducers, not ad-hoc component estado.\n4. Attach confidence metadata a displayed balances.\n5. Emit typed parser/modelo errores instead de silent defaults.\n",
            "duration": "45 min"
          },
          "frontend-v2-transaction-lifecycle-ui": {
            "title": "Transaccion lifecycle para UI: pending/confirmed/finalized, optimistic UI",
            "content": "# Transaccion lifecycle para UI: pending/confirmed/finalized, optimistic UI\n\nFrontend transaccion UX is a estado machine problem. Users press one button, but your app traverses multiple phases: intent creation, transaccion construction, signature request, submission, y confirmation at one o more commitment levels. If estos phases are collapsed into one boolean \\\"loading\\\" flag, you lose correctness y your recovery paths become guesswork.\n\nEl lifecycle starts con deterministic planning. Before any cartera popup, construct a serializable transaccion intent: cuentas, amounts, expected side effects, y idempotency key. Este intent should be inspectable y testable sin network access. En produccion, este split pays off because many failures happen before send: invalid cuenta metas, stale assumptions about ATAs, wrong decimals, o malformed instruccion payloads. A deterministic planner catches estos early y produces actionable errores.\n\nAfter signing, submission moves el transaccion into a pending estado. Pending means el network may o may not accept execution. Your UI can usa optimistic overlays, but optimistic updates should be scoped y reversible. Para example, scomo \\\"pending transfer\\\" en activity comisiond immediately, but avoid mutating durable balance totals until at least confirmed commitment. If you mutate balances too early, user trust drops when signature rejection o simulation failure occurs.\n\nCommitment levels should be modeled explicitly. \\\"processed\\\" provides quick comisiondback, \\\"confirmed\\\" provides stronger confidence, y \\\"finalized\\\" is strongest. You do not need a promise exact timing. You do need a communicate confidence boundaries clearly. A common produccion bug is labeling processed as final y then rendering inconsistent datos during cluster stress.\n\nOptimistic rollback is often neglected. Every optimistic action needs a rollback rule keyed por idempotency token. If confirmation fails, rollback should remove optimistic journal entries y restore derived estado por replaying deterministic events. Este is por que event-driven estado models are practico para frontend apps: they make rollback a replay operation instead de imperative patchwork.\n\nTelemetry should also be phase-specific. Log whether failures happen en construir, sign, send, o confirm. Group por cartera type, program ID, y error class. Este lets teams distinguish infrastructure incidents desde modeling bugs.\n\n## Pitfall: over-writing confirmed estado con stale optimistic assumptions\n\nOptimistic estado should be additive y reversible. If optimistic patches directly replace canonical estado, delayed confirmations o failures create confusing balance jumps.\n\n## Produccion Checklist\n\n1. Modelo transaccion lifecycle as explicit estados, not one loading flag.\n2. Keep deterministic planner output separate desde cartera/RPC adapter layer.\n3. Track optimistic entries con idempotency keys y rollback rules.\n4. Label commitment confidence en UI copy.\n5. Emit phase-specific telemetry para construir/sign/send/confirm.\n",
            "duration": "45 min"
          },
          "frontend-v2-data-correctness-idempotency": {
            "title": "Datos correctness: dedupe, ordering, idempotency, correction events",
            "content": "# Datos correctness: dedupe, ordering, idempotency, correction events\n\nFrontend teams frequently assume event streams are perfectly ordered y unique. Produccion sistemas rarely behave ese way. You can receive duplicate events, out-de-order events, delayed price updates, y correction signals ese invalidate earlier records. If your reducer assumes ideal sequencing, dashboard totals drift y support incidents become hard a reproduce.\n\nDeterministic ordering is el first control. En este curso we replay events por (ts, id). Timestamp alone is insufficient because equal timestamps are common en batched sistemas. A deterministic tie-breaker gives every engineer y CI runner el same final estado.\n\nIdempotency is el second control. Each event id should be applied at most once. If el same id appears twice, estado must not change after el first apply. Este rule protects against retries, websocket reconnect bursts, y duplicate queue deliveries.\n\nCorrection handling is el third control. A correction event references an earlier event id y signals ese its effect should be removed. You can implement este por replaying desde journal con corrected ids excluded, o por inverse operations when your modelo supports exact inverses. Replay is slower but simpler y safer para educational deterministic engines.\n\nHistory modeling deserves attention too. Users need recent activity, but history should not become an unstructured debug dump. Each history row should include event id, timestamp, type, y concise summary. If corrected events remain visible, label them explicitly so users y support staff understand por que balances changed.\n\nAnother correctness riesgo is cross-domain ordering. Token events y price events may arrive at different rates. Value calculations should depend en el latest known price per mint y should never usa transient float conversions. Fixed-scale integer math avoids rounding divergence across environments.\n\nWhen reducers are deterministic y replayable, regression pruebas improves dramatically. You can compare snapshots after every N events, compute checksums, y verify ese refactors preserve behavior. Este style catches subtle bugs earlier than end-a-end tests.\n\nFinally, correctness is not only code. It is product communication. If corrections can alter history, UI should surface ese possibility en copy y estado labels. Hiding it creates el appearance de randomness.\n\n## Pitfall: applying out-de-order events directly a live estado sin replay\n\nApplying arrivals as-is can produce transiently wrong balances y non-reproducible bugs. Deterministic replay gives consistent outcomes y auditable transitions.\n\n## Produccion Checklist\n\n1. Sort replay por deterministic keys (ts, id).\n2. Deduplicate por event id before applying transitions.\n3. Support correction events ese remove prior effects.\n4. Keep history rows explicit y correction-aware.\n5. Usa fixed-scale arithmetic para value calculations.\n",
            "duration": "45 min"
          },
          "frontend-v2-core-reducer": {
            "title": "Construir core estado modelo + reducer desde events",
            "content": "# Construir core estado modelo + reducer desde events\n\nImplement a deterministic reducer para dashboard estado:\n- apply event stream transitions para balances y mint metadata\n- enforce idempotency por event id\n- support correction markers para replaced events\n- emit stable history summaries\n",
            "duration": "35 min"
          }
        }
      },
      "frontend-v2-module-token-dashboard": {
        "title": "Token Dashboard Project",
        "description": "Construir reducer, replay snapshots, query metrics, y deterministic dashboard outputs ese remain stable under partial o delayed datos.",
        "lessons": {
          "frontend-v2-stream-replay-snapshots": {
            "title": "Implement event stream simulator + replay timeline + snapshots",
            "content": "# Implement event stream simulator + replay timeline + snapshots\n\nConstruir deterministic replay herramientas:\n- replay sorted events por (ts, id)\n- snapshot every N applied events\n- compute stable checksum para replay output\n- return { finalEstado, snapshots, checksum }\n",
            "duration": "35 min"
          },
          "frontend-v2-query-layer-metrics": {
            "title": "Implement query layer + computed metrics",
            "content": "# Implement query layer + computed metrics\n\nImplement dashboard query/view logic:\n- search/filter/sort rows deterministically\n- compute total y row valueUsd con fixed-scale integer math\n- expose stable view modelo para UI rendering\n",
            "duration": "35 min"
          },
          "frontend-v2-production-ux-hardening": {
            "title": "Produccion UX: caching, pagination, error banners, skeletons, rate limits",
            "content": "# Produccion UX: caching, pagination, error banners, skeletons, rate limits\n\nAfter modelo correctness, frontend calidad is mostly about user trust under imperfect conditions. Users do not evaluate your dashboard por clean demo paths. They evaluate it when datos is delayed, partial, duplicated, o rejected. Produccion UX hardening means making esos estados understandable y recoverable.\n\nCaching strategy should be explicit. Event snapshots, derived views, y summary cards should have clear freshness rules. A stale-but-marked cache is often better than blank loading screens, but stale datos must never masquerade as confirmed current datos. Include freshness timestamps y, when possible, source confidence labels (cached, replayed, confirmed).\n\nPagination y virtualized lists need deterministic sorting a avoid row jumps between pages. If sort keys are unstable, users see items move unexpectedly as new events arrive. Usa primary y secondary stable keys, y preserve cursor semantics during live updates.\n\nError banners should be scoped por subsystem. Parser errores are not network errores. Replay checksum mismatches are not cartera signature errores. Distinct error classes reduce panic y help users choose next actions. A generic red toast ese says \\\"something went wrong\\\" is operationally expensive.\n\nSkeleton estados must communicate structure rather than fake certainty. Scomo placeholder rows y chart bounds, but avoid hardcoding values ese look real. If users screen-record issues, misleading skeletons complicate incident investigation.\n\nRate limits are common en real dashboards, even con private APIs. Your UI should surface backoff estado y avoid firehose re-requests desde multiple components. Centralize datos fetching y de-duplicate en-flight requests por key. Este prevents self-inflicted throttling.\n\nLive mode y replay mode should share el same reducer y query pipeline. Live mode streams events progressively; replay mode applies fixture timelines deterministically. If estos modes usa different code paths, bugs hide en mode-specific branches y become hard a reproduce.\n\nA practico approach is a store event journal y snapshots, then render all UI desde derived selectors. Este architecture supports recoverability: you can reset a snapshot N, replay events, y inspect differences. It also supports support herramientas: attach snapshot checksum y modelo version a error reportes.\n\n### Devnet Bonus (optional)\n\nYou can add an RPC adapter behind a feature flag y map live cuenta updates into el same event format. Keep este optional y never required para core correctness.\n\n## Pitfall: shipping polished visuals con unscoped failure estados\n\nIf users cannot tell whether an issue is stale cache, parse failure, o upstream throttle, confidence erodes even when core modelo logic is correct.\n\n## Produccion Checklist\n\n1. Expose freshness metadata para cached y live datos.\n2. Keep list sorting deterministic across pagination.\n3. Classify errores por subsystem con actionable copy.\n4. De-duplicate en-flight fetches y respect rate limits.\n5. Render live y replay modes through shared reducer/selectors.\n",
            "duration": "45 min"
          },
          "frontend-v2-dashboard-summary-checkpoint": {
            "title": "Emit stable DashboardSummary desde fixtures",
            "content": "# Emit stable DashboardSummary desde fixtures\n\nCompose deterministic checkpoint output:\n- owner, token count, totalValueUsd\n- top tokens sorted deterministically\n- recent activity rows\n- invariants y determinism metadata (fixture hash + modelo version)\n",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "defi-solana": {
    "title": "DeFi en Solana",
    "description": "Avanzado project-journey curso para engineers construiring swap sistemas: deterministic offline Jupiter-style planning, route ranking, minOut seguridad, y reproducible diagnostics.",
    "duration": "12 hours",
    "tags": [
      "defi",
      "swap",
      "routing",
      "jupiter",
      "offline",
      "deterministic"
    ],
    "modules": {
      "defi-v2-module-swap-fundamentals": {
        "title": "Swap Fundamentals",
        "description": "Understand CPMM math, quote anatomy, y deterministic routing tradeoffs con seguridad-first user protections.",
        "lessons": {
          "defi-v2-amm-basics-fees-slippage-impact": {
            "title": "AMM fundamentos en Solana: pools, comisiones, deslizamiento, y impacto de precio",
            "content": "# AMM fundamentos en Solana: pools, comisiones, deslizamiento, y impacto de precio\n\nWhen users click “Swap,” they usually assume there is one objective truth: el current price. En practice, frontend swap sistemas compute an estimate desde pool reserves y route assumptions. El estimate can be excellent, but it is still a modelo. DeFi UI calidad depends en como honestly y consistently ese modelo is represented.\n\nEn a constant-product AMM, each pool maintains an invariant close a x * y = k. A swap changes reserves asymmetrically, y el output amount is non-linear relative a input size. Small trades can track spot estimates closely, while larger trades move further along el curve y experience more impact. Ese non-linearity is por que frontend code must never compare routes usando only “price per token” labels. You need route-aware output calculations at el target trade size.\n\nEn Solana, swaps also occur across varied pool disenos y comision tiers. Some pools are deep y low comision; others are shallow but still attractive para small size due a path composition. Comision bps are often compared en isolation, but total execution calidad comes desde three interacting pieces: comision deduction, reserve depth, y route hop count. A route con slightly higher comision can still produce higher net output if reserves are materially deeper.\n\nDeslizamiento y impacto de precio are often conflated en UI copy, but they answer different questions. Impacto de precio asks: que movement does este trade itself induce against current reserves? Deslizamiento tolerance asks: que worst-case output should still be accepted at execution time? One is descriptive de current route mechanics, el other is a user seguridad bound. Produccion interfaces should surface both values clearly y compute minOut deterministically desde outAmount y deslizamiento bps.\n\nDeterministic arithmetic matters as much as financial logic. If planners usa floating-point shortcuts, two environments can produce subtly different minOut values y route ranking. Esos tiny differences create major operational pain en tests, incident response, y support reproductions. Integer arithmetic over u64-style amount strings should remain el primary modelo path; formatting para users should happen only at presentation boundaries.\n\nEven en an offline educational planner, seguridad invariants belong at el core. Outputs must never exceed reserveOut. Reserves must remain non-negative after virtual simulation. Missing pools should fail fast con typed errores, not fallback behavior. Estos checks mirror produccion expectations y train el same engineering discipline needed para real integrations.\n\nA robusto frontend modelo mental is therefore: token universe + pool universe + deterministic quote math + route ranking policy + user seguridad constraints. If any layer is implicit, el sistema will still run, but behavior under volatility becomes hard a explain. If all layers are explicit y typed, el same planner can power UI previews, tests, y diagnostics con minimal drift.\n\n## Quick numeric intuition\n\nIf two routes have spot prices ese look similar, a larger input can still produce materially different output because you travel further en each curve. Ese is por que route comparison must happen at el exact user amount, not a tiny reference trade.\n\n## Que you should internalize desde este leccion\n\n- Execution calidad is output-at-size, not headline spot price.\n- Deslizamiento tolerance is a user protection bound, not a market forecast.\n- Deterministic integer math is a product feature, not only a technical preference.\n\n### Pitfalls\n\n1. Comparing routes por headline “price” instead de exact outAmount at el user’s size.\n2. Treating deslizamiento tolerance as if it were el same metric as impacto de precio.\n3. Usando floating point en route ranking o minOut logic.\n\n### Produccion Checklist\n\n1. Keep amount math en integer-safe paths.\n2. Surface outAmount, comision impact, y minOut separately.\n3. Enforce invariant checks para each hop simulation.\n4. Keep route ranking deterministic con explicit tie-breakers.\n5. Log enough context a reproduce route decisions.\n",
            "duration": "50 min"
          },
          "defi-v2-quote-anatomy-and-risk": {
            "title": "Quote anatomy: en/out, comisiones, minOut, y worst-case execution",
            "content": "# Quote anatomy: en/out, comisiones, minOut, y worst-case execution\n\nA produccion quote is not one number. It is a structured object ese must tell users que they send, que they likely receive, como much they pay en comisiones, y que minimum output protection applies. When frontend sistemas treat quote payloads as loose JSON blobs, users lose trust quickly because route changes y execution deviations look arbitrary.\n\nEl first mandatory fields are inAmount y outAmount en raw integer units. Sin raw values, deterministic checks become fragile. UI formatting should be derived desde token decimals, but core estado should retain raw strings para exact comparisons y invariant logic. If an app compares rounded display numbers, route ties can break unpredictably.\n\nSecond, quote sistemas should expose comision breakdown per hop. Aggregate comision bps is useful, but it hides which pools drive cost. Para route explainability y depuracion, users y engineers need pool-level comision contributions. Este is particularly important para two-hop routes where one leg may be cheap y el other expensive.\n\nThird, minOut must be explicit, reproducible, y tied a user-configured deslizamiento bps. El computation is deterministic: floor(outAmount * (10000 - deslizamientoBps) / 10000). Scomoing este value is not optional para serious UX. It is el user’s principal seguridad guard against stale quotes y rapid market movement between quote y submission.\n\nFourth, quote freshness y worst-case framing should be visible. Even en offline training sistemas, el planner should modelo el idea ese el route is valid para a moment, not forever. En produccion, stale quote handling y forced re-quote boundaries prevent accidental execution con outdated assumptions.\n\nA useful engineering patron is a modelo quote objects as immutable snapshots. Each snapshot includes selected route, per-hop details, total comisiones, impact estimate, y minOut. If selection changes, produce a new snapshot instead de mutating fields en place. Este gives deterministic audit trails y cleaner estado transitions.\n\nPara este curso, leccion logic remains offline y deterministic, but el same diseno prepares teams para real Jupiter integrations later. Por el time network adapters are introduced, your modelo y tests already guarantee stable route math y explainability.\n\nQuote anatomy also influences support burden. When a user asks por que they received less than expected, el answer is much faster if el sistema preserves route path, deslizamiento setting, y minOut desde el exact planning estado. Sin ese, teams rely en post-hoc guesses.\n\n### Pitfalls\n\n1. Displaying outAmount sin minOut y route-level comisiones.\n2. Mutating selected quote objects en place instead de creating snapshots.\n3. Computing comision percentages desde rounded UI values instead de raw amounts.\n\n### Produccion Checklist\n\n1. Keep quote payloads immutable y versioned.\n2. Store per-hop comision contributions y total comision amount.\n3. Compute y scomo minOut desde explicit deslizamiento bps.\n4. Preserve raw amounts y decimals separately.\n5. Expose route freshness metadata en UI estado.\n",
            "duration": "50 min"
          },
          "defi-v2-routing-fragmentation-two-hop": {
            "title": "Routing: por que two-hop can beat one-hop",
            "content": "# Routing: por que two-hop can beat one-hop\n\nUsers often assume direct pair routes are always best because they are simpler. En fragmented liquidez sistemas, ese assumption fails frequently. A direct SOL -> JUP pool might have shallow depth, while SOL -> USDC y USDC -> JUP pools together can produce better net output despite two comisiones y two curve traversals. A produccion router should evaluate both one-hop y two-hop candidates y rank them deterministically.\n\nEl engineering desafio is not just finding paths. It is comparing paths under consistent assumptions. Every candidate should be quoted con el same input amount, same deterministic arithmetic, y same comision/impact cuentaing. If one path uses rounded display math while another uses raw amounts, route ranking loses meaning.\n\nTwo-hop routing also requires stable tie-break policies. Suppose two candidates produce equal outAmount at integer precision. One has one hop; el other has two hops. A deterministic sistema should prefer fewer hops. If hop count also ties, lexicographic route ID ordering can resolve final rank. El exact policy can vary, but it must be explicit y stable.\n\nLiquidez fragmentation introduces another subtle point: intermedio mint riesgo. A two-hop path through a highly liquid stable pair can be excellent, but if el second pool is thin, el route can still degrade at larger sizes. Este is por que route scoring should be quote-size aware y not reused blindly across different trade amounts.\n\nPara offline curso logic, we modelo pools as a static universe y simulate reserves virtually per quote path. Even este simplified modelo teaches key produccion habits: avoid mutating source fixtures, isolate simulation estado per candidate, y validate seguridad constraints at each hop.\n\nRouting calidad is also a UX problem. If a selected route changes due a input edits o quote refresh, users should see por que: outAmount delta, comision change, y path change. Silent route switching comisionls suspicious even when mathematically correct.\n\nEn larger sistemas, routers may consider split routes, gas/compute constraints, o venue confiabilidad. Este curso intentionally limits scope a one-hop y two-hop deterministic candidates so core reasoning remains clear y testable.\n\nDesde an implementation perspective, route objects should be treated as typed artifacts con stable IDs y explicit hop metadata. Ese discipline reduces accidental coupling between UI estado y planner internals. When engineers can serialize a route candidate, replay it con el same input, y get el same result, incident response becomes straightforward.\n\n### Pitfalls\n\n1. Assuming direct pairs always outperform multi-hop routes.\n2. Reusing quotes computed para one trade size at another size.\n3. Non-deterministic tie-breaking ese causes route flicker.\n\n### Produccion Checklist\n\n1. Enumerate one-hop y two-hop routes systematically.\n2. Quote every candidate con el same deterministic math path.\n3. Keep tie-break policy explicit y stable.\n4. Simulate virtual reserves sin mutating source fixtures.\n5. Surface route-change reasons en UI.\n",
            "duration": "50 min"
          }
        }
      },
      "defi-v2-module-offline-jupiter-planner": {
        "title": "Jupiter-Style Swap Planner Project (Offline)",
        "description": "Construir deterministic quoting, route selection, y minOut seguridad checks, then package stable checkpoint artifacts para reproducible reviews.",
        "lessons": {
          "defi-v2-quote-cpmm": {
            "title": "Implement token/pool modelo + constant-product quote calc",
            "content": "# Implement token/pool modelo + constant-product quote calc\n\nImplement deterministic CPMM quoting:\n- out = (reserveOut * inAfterComision) / (reserveIn + inAfterComision)\n- comision = floor(inAmount * comisionBps / 10000)\n- impactBps desde spot vs effective execution price\n- return outAmount, comisionAmount, inAfterComision, impactBps\n",
            "duration": "35 min"
          },
          "defi-v2-router-best": {
            "title": "Implement route enumeration y best-route selection",
            "content": "# Implement route enumeration y best-route selection\n\nImplement deterministic route planner:\n- enumerate one-hop y two-hop candidates\n- quote each candidate at exact input size\n- select best route usando stable tie-breakers\n",
            "duration": "35 min"
          },
          "defi-v2-safety-minout": {
            "title": "Implement deslizamiento/minOut, comision breakdown, y seguridad invariants",
            "content": "# Implement deslizamiento/minOut, comision breakdown, y seguridad invariants\n\nImplement deterministic seguridad layer:\n- apply deslizamiento a compute minOut\n- simulate route con virtual reserve updates\n- return structured errores para invalid pools/routes\n- enforce non-negative reserve y bounded output invariants\n",
            "duration": "35 min"
          },
          "defi-v2-production-swap-ux": {
            "title": "Produccion swap UX: stale quotes, protection, y simulation",
            "content": "# Produccion swap UX: stale quotes, protection, y simulation\n\nA deterministic route engine is necessary but not sufficient para produccion. Users experience DeFi through timing, messaging, y seguridad affordances. A mathematically correct planner can still comisionl broken if stale quote handling, retry behavior, y error communication are weak.\n\nStale quotes are el most common operational issue. En volatile markets, quote calidad decays quickly. Interfaces should track quote age y invalidate plans beyond a strict threshold. When invalidation happens, route y minOut should be recomputed before submit. Reusing stale plans a “speed up” UX usually creates worse outcomes y support burden.\n\nUser protection should be layered. Deslizamiento bounds protect against adverse movement, but they do not protect against malformed route payloads o mismatched cuenta assumptions. Seguridad validation should run before any cartera prompt y should return explicit, typed errores. “Something went wrong” is not enough en swap flujos.\n\nSimulation messaging matters as much as simulation itself. If route simulation fails pre-send, users need actionable context: which hop failed, whether pool liquidez was insufficient, whether el route is missing required pools, y whether re-quoting could help. Generic error banners create user churn.\n\nRetry logic must be bounded y estadoful. Blind retries con unchanged input are often just repeated failures. Good UX distinguishes retryable estados (temporary network issue) desde deterministic planner errores (invalid route topology). Para deterministic planner errores, force estado change before retry.\n\nAnother produccion concern is observability. Record route ID, inAmount, outAmount, minOut, comision totals, y invariant results para each attempt. Estos logs make incident triage y postmortems dramatically faster. Sin structured traces, teams often blame “market conditions” para planner bugs.\n\nPagination y list updates also affect trust. Swap history UIs should preserve deterministic ordering y avoid jitter when datos refreshes. If past swaps reorder unpredictably, users perceive confiabilidad issues even when transaccions are correct.\n\nOptional live integrations should be feature-flagged y isolated. El offline deterministic engine should remain el source de truth, while live adapters map external responses into el same internal types. Ese boundary keeps tests stable y protects core behavior desde third-party schema changes.\n\nFinally, produccion swap UX should make deterministic planner outcomes explainable a non-expert users. If a route is rejected, el interface should provide a concrete reason y a clear next action such as reducing size o selecting a different output token. Clear messaging converts sistema correctness into user trust.\n\n### Pitfalls\n\n1. Allowing stale quotes a remain actionable sin forced re-quote.\n2. Retrying deterministic planner errores sin changing route o inputs.\n3. Hiding failure reason details behind generic notifications.\n\n### Produccion Checklist\n\n1. Track quote freshness y invalidate aggressively.\n2. Enforce pre-submit invariant validation.\n3. Separate retryable network failures desde deterministic planner failures.\n4. Log route y seguridad metadata para every attempt.\n5. Keep offline engine as canonical modelo para optional live adapters.\n",
            "duration": "50 min"
          },
          "defi-v2-checkpoint": {
            "title": "Produce stable SwapPlan + SwapSummary checkpoint",
            "content": "# Produce stable SwapPlan + SwapSummary checkpoint\n\nCompose deterministic checkpoint artifacts:\n- construir swap plan desde selected route quote\n- include fixtureHash y modelVersion\n- emit stable summary con path, minOut, comision totals, impact, y invariants\n",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-security": {
    "title": "Solana Seguridad & Auditing",
    "description": "Produccion-grade deterministic vuln lab para Solana auditors who need repeatable exploit evidence, precise remediation guidance, y high-signal audit artifacts.",
    "duration": "10 hours",
    "tags": [
      "security",
      "audit",
      "vuln-lab",
      "solana"
    ],
    "modules": {
      "security-v2-threat-model-and-method": {
        "title": "Threat Modelo & Audit Method",
        "description": "Cuenta-centric threat modeling, deterministic exploit reproduction, y evidence discipline para credible audit findings.",
        "lessons": {
          "security-v2-threat-model": {
            "title": "Solana threat modelo para auditors: cuentas, owners, signers, writable, PDAs",
            "content": "# Solana threat modelo para auditors: cuentas, owners, signers, writable, PDAs\n\nSeguridad work en Solana starts con one non-negotiable fact: instruccion callers choose el cuenta list. Programs do not receive trusted implicit context. They receive exactly el cuenta metas y instruccion datos encoded en a transaccion message. Este diseno is powerful para composability y rendimiento, but it means almost every critical exploit is an cuenta validation exploit en disguise. If you internalize este early, your audits become more mechanical y less guess-based.\n\nA good modelo mental is a treat each instruccion as a contract boundary con five mandatory validations: identity, authority, ownership, mutability, y derivation. Identity asks whether el supplied cuenta is el cuenta el instruccion expects. Authority asks whether el actor ese is allowed a mutate estado actually signed. Ownership asks whether cuenta datos should be interpreted under el current program o a different one. Mutability asks whether writable access is both requested y justified. Derivation asks whether PDA paths are deterministic y verified against canonical seeds plus bump. Missing any de esos layers creates openings ese attackers repeatedly usa.\n\nSigner checks are not optional en privileged paths. If el instruccion changes authority, moves funds, o updates riesgo parameters, el authority cuenta must be a signer y must be el expected authority desde estado. One common bug is checking only ese “some signer exists.” Ese is still broken. Audits should explicitly map each privileged transition a a concrete signer relationship y verify ese relation is enforced before estado mutation.\n\nOwner checks are equally critical. Programs often parse cuenta bytes into local structs. Sin owner checks, an attacker can pass arbitrary bytes ese deserialize into a shape ese looks valid but is controlled por another program o por no program assumptions at all. Este is cuenta substitution. It is el root cause de many catastrophic incidents y should be surfaced early en review notes.\n\nPDA checks are where many teams lose determinism. Seed recipes need a be explicit, stable, y versioned. If el runtime accepts user-provided bump values sin recomputation, o if seed ordering differs between handlers, spoofed addresses can pass inconsistent checks. Auditors should insist en exact re-derivation y equality checks en all sensitive paths.\n\nWritable flags matter para two reasons: correctness y attack surface. Over-broad writable sets increase riesgo por allowing unnecessary estado transitions en CPI-heavy flujos. Under-declared mutability causes runtime failure, which is safer but still a confiabilidad bug.\n\nFinally, threat modeling should include arithmetic constraints. Even if auth is correct, unchecked u64 math can corrupt balances through underflow o overflow y invalidate all higher-level assumptions.\n\n## Auditor workflow per instruccion\n\nPara each handler, run el same sequence: identify privileged outcome, list required cuentas, verify signer/owner/PDA relationships, verify writable scope, then test malformed cuenta lists. Repeating este fixed loop prevents “I think it looks safe” audits.\n\n## Que you should be able a do after este leccion\n\n- Turn a vague concern into a concrete validation checklist.\n- Explain por que cuenta substitution y PDA spoofing recur en Solana incidents.\n- Construir deterministic negative-path scenarios before writing remediation notes.\n\n## Checklist\n- Map each instruccion a a clear privilege modelo.\n- Verify authority cuenta is required signer para privileged actions.\n- Verify authority key equality against stored estado authority.\n- Verify every parsed cuenta has explicit owner validation.\n- Verify each PDA is re-derived desde canonical seeds y bump.\n- Verify writable cuentas are minimal y justified.\n- Verify arithmetic uses checked operations para u64 transitions.\n- Verify negative-path tests exist para unauthorized y malformed cuentas.\n\n## Red flags\n- Privileged estado updates sin signer checks.\n- Parsing unchecked cuenta datos desde unknown owners.\n- PDA acceptance based en partial seed checks.\n- Handlers ese trust client-provided bump blindly.\n- Arithmetic updates usando plain + y - en balances.\n\n## Como a verify (simulator)\n- Run vulnerable mode en signer-missing scenario y inspect trace.\n- Re-run fixed mode y confirm ERR_NOT_SIGNER.\n- Execute owner-missing scenario y compare vulnerable vs fixed outcomes.\n- Execute pda-spoof scenario y confirm fixed mode emits ERR_BAD_PDA.\n- Compare trace hashes a verify deterministic event ordering.\n",
            "duration": "55 min"
          },
          "security-v2-evidence-chain": {
            "title": "Evidence chain: reproduce, trace, impact, fix, verify",
            "content": "# Evidence chain: reproduce, trace, impact, fix, verify\n\nStrong seguridad reportes are built en evidence chains, not opinions. En el Solana context, ese means moving desde a claim such as “missing signer check exists” a a deterministic chain: reproduce exploit conditions, capture a stable execution trace, quantify impact, apply a patch, y verify ese el same steps now fail con expected error codes while invariants hold. Este chain is que turns audit work into an engineering artifact.\n\nReproduction should be deterministic y minimal. Every scenario should declare initial cuentas, authority/signer flags, vault ownership assumptions, y instruccion inputs. If reproductions depend en external RPC timing o changing liquidez conditions, confidence drops y triage slows down. En este curso lab, scenarios are fixture-driven y offline so every replay produces el same estado transitions.\n\nTrace capture is el core de audit evidence. Instead de recording only final balances, log each relevant event en stable order: InstruccionStart, CuentaRead, CheckPassed/CheckFailed, BalanceChange, InstruccionEnd. Estos events let reviewers verify exactly which assumptions passed y where validation was skipped. They also help map exploitability a code-level checks. Para example, if signer checks are absent en vulnerable mode, el trace should explicitly scomo ese signer validation was skipped o never evaluated.\n\nImpact analysis should be quantitative. Para signer y owner bugs, compute drained lamports o unauthorized estado changes. Para PDA bugs, scomo mismatch between expected derived address y accepted address. Para arithmetic bugs, scomo underflow o overflow conditions y resulting corruption. Impact details inform severity y prioritization.\n\nPatch validation should not just say “fixed.” It should prove exploit steps now fail para el right reason. If signer exploit now fails, error code should be ERR_NOT_SIGNER. If PDA spoof now fails, error code should be ERR_BAD_PDA. Este specificity catches regressions where one bug is accidentally masked por unrelated behavior.\n\nVerification closes el chain con invariant checks. Examples: vault balance remains a valid u64 string, authority remains unchanged, y no unauthorized lamport delta occurs en fixed mode. Estos invariants convert patch confidence into measurable guarantees.\n\nWhen teams do este consistently, reportes become executable documentation. New engineers can replay scenarios y understand por que controls exist. Incident response becomes faster because prior failure signatures y remediation patrones are already captured.\n\n## Checklist\n- Define each scenario con explicit initial estado y instruccion inputs.\n- Capture deterministic, ordered trace events para each run.\n- Hash traces con canonical JSON para reproducibility.\n- Quantify impact usando before/after deltas.\n- Map each finding a explicit evidence references.\n- Re-run identical scenarios en fixed mode.\n- Verify fixed-mode failures usa expected error codes.\n- Record post-fix invariant results con stable IDs.\n\n## Red flags\n- Reportes con no reproduction steps.\n- Non-deterministic traces ese change between runs.\n- Impact described qualitatively sin deltas.\n- Patch claims sin fixed-mode replay evidence.\n- Invariant lists omitted desde verification section.\n\n## Como a verify (simulator)\n- Run signer-missing en vulnerable mode, save trace hash.\n- Run same scenario en fixed mode, confirm ERR_NOT_SIGNER.\n- Run owner-missing y confirm ERR_BAD_OWNER en fixed mode.\n- Run pda-spoof y compare expected/accepted PDA fields.\n- Generate audit reporte JSON y markdown summary desde checkpoint construirer.\n",
            "duration": "55 min"
          },
          "security-v2-bug-classes": {
            "title": "Common Solana bug classes y mitigations",
            "content": "# Common Solana bug classes y mitigations\n\nAuditors en Solana repeatedly encounter el same core bug families. El implementation details differ across protocols, but exploit mechanics are surprisingly consistent: identity confusion, authority confusion, derivation drift, arithmetic corruption, y unsafe cross-program assumptions. A robusto review process categorizes findings por class, applies known verification patrones, y tests negative paths intentionally.\n\n**Missing signer checks** are high-severity because they directly break authorization. El fix is conceptually simple: require signer y key relation. Yet teams miss it when refactoring cuenta structs o switching between typed y unchecked cuenta wrappers. Auditors should scan all estado-mutating handlers y ask: who can call este y que proves authorization?\n\n**Missing owner checks** create cuenta substitution riesgo. Programs may deserialize cuenta bytes y trust semantic fields sin proving el cuenta is owned por el expected program. En mixed CPI sistemas, este is especially dangerous because cuenta shapes can look valid while semantics differ. Mitigation is explicit owner validation before parsing y strict cuenta type usage.\n\n**PDA seed/bump mismatch** appears when seed ordering, domain tags, o bump handling drifts between instruccions. One handler derives [\"vault\", authority], another derives [authority, \"vault\"], a third trusts client-provided bump. Attackers search esos inconsistencies a route privileged logic through spoofed addresses. Mitigation is canonical seed schema, exact re-derivation en every sensitive path, y tests ese intentionally pass malformed PDA candidates.\n\n**CPI authority confusion** happens when one program delegates authority assumptions a another sin strict scope. If signer seeds o delegated permissions are broader than intended, downstream calls can perform unintended estado transitions. Mitigation includes explicit CPI allowlists, minimal writable/signer metas, y scope-limited delegated authorities.\n\n**Integer overflow/underflow** remains a practico class en cuentaing-heavy sistemas. Rust release mode behavior makes unchecked arithmetic unacceptable para balances y comision logic. Mitigation is checked operations, u128 intermedios para multiply/divide paths, y boundary-focused tests.\n\nMitigation calidad depends en verification calidad. Unit tests should include adversarial cuenta substitutions, malformed seeds, missing signers, y boundary arithmetic. If tests only cover happy paths, high-severity bugs will survive code review.\n\nEl audit deliverable should translate classes into implementation guidance. Engineers need clear, actionable remediations y concrete reproduction conditions, not generic advertencias. El best reportes include checklists ese can be wired into CI y release gates.\n\n## Checklist\n- Enumerate all privileged instruccions y expected signers.\n- Verify owner checks before parsing external cuenta layouts.\n- Pin y document PDA seed schemas y bump usage.\n- Validate CPI target program IDs against allowlist.\n- Minimize writable y signer cuenta metas en CPI.\n- Enforce checked math para all u64 estado transitions.\n- Add negative tests para each bug class.\n- Require deterministic traces para seguridad-critical tests.\n\n## Red flags\n- Any privileged mutation path sin explicit signer requirement.\n- Any unchecked cuenta deserialization path.\n- Any instruccion ese accepts bump sin re-derivation.\n- Any CPI call a dynamic o user-selected program ID.\n- Any unchecked arithmetic en balances o supply values.\n\n## Como a verify (simulator)\n- Usa leccion 4 scenario a confirm unauthorized withdraw en vulnerable mode.\n- Usa leccion 5 scenario a confirm spoofed PDA acceptance en vulnerable mode.\n- Usa leccion 6 patch suite a verify fixed-mode errores por code.\n- Run checkpoint reporte y ensure all scenarios are marked reproduced.\n- Inspect invariant result array para all fixed-mode scenarios.\n",
            "duration": "55 min"
          }
        }
      },
      "security-v2-vuln-lab": {
        "title": "Vuln Lab Project Journey",
        "description": "Exploit, patch, verify, y produce audit-ready artifacts con deterministic traces y invariant-backed conclusions.",
        "lessons": {
          "security-v2-exploit-signer-owner": {
            "title": "Break it: exploit missing signer + owner checks",
            "content": "# Break it: exploit missing signer + owner checks\n\nImplement a deterministic exploit-prueba formatter para signer/owner vulnerabilities.\n\nExpected output fields:\n- scenario\n- before/after vault balance\n- before/after recipient lamports\n- trace hash\n- explanation con drained lamports\n\nUsa canonical key ordering so tests can assert exact JSON output.",
            "duration": "40 min"
          },
          "security-v2-exploit-pda-spoof": {
            "title": "Break it: exploit PDA spoof mismatch",
            "content": "# Break it: exploit PDA spoof mismatch\n\nImplement a deterministic PDA spoof prueba output.\n\nYou must scomo:\n- expected PDA\n- accepted PDA\n- mismatch boolean\n- trace hash\n\nEste leccion validates evidence generation para derivation mismatches.",
            "duration": "40 min"
          },
          "security-v2-patch-validate": {
            "title": "Fix it: validations + invariant suite",
            "content": "# Fix it: validations + invariant suite\n\nImplement patch validation output ese confirms:\n- signer check\n- owner check\n- PDA check\n- safe u64 arithmetic\n- exploit blocked estado con error code\n\nKeep output deterministic para exact assertion.",
            "duration": "45 min"
          },
          "security-v2-writing-reports": {
            "title": "Writing audit reportes: severity, likelihood, blast radius, remediation",
            "content": "# Writing audit reportes: severity, likelihood, blast radius, remediation\n\nA strong audit reporte is an engineering document, not a narrative essay. It should allow a reader a answer four questions quickly: que failed, como exploitable it is, como much damage it can cause, y que exact change prevents recurrence. Seguridad writing calidad directly affects fix calidad because implementation teams ship que they can interpret.\n\nSeverity should be tied a impact y exploit preconditions. A missing signer check en a withdraw path is typically critical if it allows unauthorized asset movement con low prerequisites. A PDA mismatch may be high o medium depending en reachable code paths y available controls. Severity labels sin rationale are not useful. Include explicit exploit path assumptions y whether attacker capital o privileged positioning is required.\n\nLikelihood should capture practico exploitability, not theoretical possibility. Para example, if a bug requires impossible cuenta estados under current architecture, likelihood may be low even if impact is high. Conversely, if a bug is reachable por submitting a standard instruccion con crafted cuenta metas, likelihood is high. Be specific.\n\nBlast radius should describe que can be drained o corrupted: one vault, one market, protocol-wide estado, o gobernanza authority. Este framing helps teams stage incident response y patch rollout.\n\nRecommendations must be precise y testable. “Add better validation” is too vague. “Require authority signer, verify authority key matches vault estado, verify vault owner equals program id, y verify PDA desde [\"vault\", authority] + bump” is actionable. Include expected error codes so QA can validate behavior reliably.\n\nEvidence references are also important. Each finding should point a deterministic traces, scenario IDs, y checkpoint artifacts so another engineer can replay sin interpretation gaps.\n\nFinally, include verification results. A patch is not complete until exploit scenarios fail deterministically y invariants hold. Reportes ese end before verification force downstream teams a rediscover completion criteria.\n\nReporte structure should also prioritize scanability. Teams reviewing multiple findings under incident pressure need consistent field ordering y concise language ese maps directly a engineering actions. If one finding uses narrative prose while another uses structured reproduction steps, remediation speed drops because readers spend time normalizing format instead de executing fixes.\n\nA confiable patron is one finding per vulnerability class con explicit evidence references grouped por scenario ID. Ese allows QA, auditors, y protocol engineers a coordinate en el same deterministic artifacts. El same approach also improves long-term maintenance: when code changes, teams can rerun scenario IDs y compare trace hashes a detect regressions en reporte assumptions.\n\n## Checklist\n- Estado explicit vulnerability class y affected instruccion path.\n- Include reproducible scenario ID y deterministic trace hash.\n- Quantify impact con concrete estado/balance deltas.\n- Assign severity con rationale tied a exploit preconditions.\n- Assign likelihood based en realistic attacker capabilities.\n- Describe blast radius at cuenta/protocol boundary.\n- Provide exact remediation steps y expected error codes.\n- Include verification outcomes y invariant results.\n\n## Red flags\n- Severity labels sin impact rationale.\n- Recommendations sin concrete validation rules.\n- No reproduction steps o trace references.\n- No fixed-mode verification evidence.\n- No distinction between impact y likelihood.\n\n## Como a verify (simulator)\n- Generate reporte JSON desde checkpoint construirer.\n- Confirm findings include evidenceRefs para each scenario.\n- Confirm remediation includes patch IDs.\n- Confirm verification results mark each scenario as blocked en fixed mode.\n- Generate markdown summary y compare a reporte content ordering.\n",
            "duration": "55 min"
          },
          "security-v2-audit-report-checkpoint": {
            "title": "Checkpoint: deterministic AuditReporte JSON + markdown",
            "content": "# Checkpoint: deterministic AuditReporte JSON + markdown\n\nCreate el final deterministic checkpoint payload:\n- curso + version\n- scenario IDs\n- finding count\n\nEste checkpoint mirrors el final curso artifact produced por el simulator reporte construirer.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "token-engineering": {
    "title": "Token Engineering en Solana",
    "description": "Project-journey curso para teams launching real Solana tokens: deterministic Token-2022 planning, authority diseno, supply simulation, y operational launch discipline.",
    "duration": "10 hours",
    "tags": [
      "tokens",
      "token-2022",
      "launch",
      "authorities",
      "simulation"
    ],
    "modules": {
      "token-v2-module-fundamentals": {
        "title": "Token Fundamentals -> Token-2022",
        "description": "Understand token primitives, mint policy anatomy, y Token-2022 extension controls con explicit gobernanza y threat-modelo framing.",
        "lessons": {
          "token-v2-spl-vs-token-2022": {
            "title": "SPL tokens vs Token-2022: que extensions change",
            "content": "# SPL tokens vs Token-2022: que extensions change\n\nToken engineering starts con a clean boundary between base token semantics y configurable policy. Legacy SPL Token gives you a stable fungible primitive: mint metadata, token cuentas, mint authority, freeze authority, y transfer/mint/burn instruccions. Token-2022 preserves ese core interface but adds extension slots ese let teams activate richer behavior sin rewriting token logic desde scratch. Ese compatibility is useful, but it also creates a new class de gobernanza y seguridad decisions ese frontend y protocol engineers need a modelo explicitly.\n\nEl key modelo mental: Token-2022 is not a separate economic sistema; it is an extended cuenta layout y instruccion surface. Extensions are opt-en, y each extension adds bytes, authorities, y estado transitions ese must be considered during mint initialization y lifecycle management. If you treat extensions as cosmetic add-ons, you can ship a token ese is technically valid but operationally fragile.\n\nPara produccion teams, el first decision is policy minimalism. Every enabled extension increases complexity en carteras, indexers, y downstream integrations. Transfer comisiones may fit treasury goals but can break assumptions en partner protocols. Default cuenta estado can enforce seguridad posture but may confuse users if cuenta thaw flujo is unclear. Permanent delegate can simplify managed flujos but dramatically expands power if authority boundaries are weak. El right approach is a map each extension a a concrete requirement y document el explicit threat modelo it introduces.\n\nToken-2022 also changes launch sequencing. You must pre-size mint cuentas para chosen extensions, initialize extension datos en deterministic order, y verify authority alignment before live distribution. Este is where deterministic offline planning is valuable: you can generate a launch pack, inspect instruccion-like payloads, y validate invariants before touching network sistemas. Ese practice catches configuration drift early y gives reviewers a reproducible artifact.\n\nFinally, extension-aware diseno is an integration problem, not just a contract problem. Product y support teams need clear messaging para comision behavior, frozen cuenta estados, y delegated capabilities. If users cannot predict token behavior desde cartera prompts y docs, operational riesgo rises even when code is formally correct.\n\n## Decision framework para extension selection\n\nPara each extension, force three answers before enabling it:\n1. Que concrete product requirement does este solve now?\n2. Which authority can abuse este if compromised?\n3. Como will users y integrators observe este behavior en UX y docs?\n\nIf any answer is vague, extension scope is probably too broad.\n\n## Pitfall: Extension creep sin threat modeling\n\nAdding multiple extensions \"para flexibility\" often creates overlapping authority powers y unpredictable UX. Enable only el extensions your product can govern, monitor, y explain end-a-end.\n\n## Sanity Checklist\n\n1. Define one explicit business reason per extension.\n2. Document extension authorities y revocation strategy.\n3. Verify partner compatibility assumptions before launch.\n4. Produce deterministic initialization artifacts para review.\n",
            "duration": "45 min"
          },
          "token-v2-mint-anatomy": {
            "title": "Mint anatomy: authorities, decimals, supply, freeze, mint",
            "content": "# Mint anatomy: authorities, decimals, supply, freeze, mint\n\nA produccion token launch succeeds o fails en parameter discipline. El mint cuenta is a compact policy object: it defines decimal precision, minting authority, optional freeze authority, y extension configuration. Token cuentas then represent balances para owners, usually through ATAs. If estos pieces are configured inconsistently, downstream sistemas see contradictory behavior y user trust erodes quickly.\n\nDecimals are one de el most underestimated parameters. They influence UI formatting, comision interpretation, y business logic en integrations. While high precision can comisionl \"future-prueba,\" excessive decimals often create rounding edge cases en analytics y partner sistemas. Constraining decimals a a documented operational range y validating it at config time is a practico defensive rule.\n\nAuthority layout should be explicit y minimal. Mint authority controls supply growth. Freeze authority controls cuenta-level transfer ability. Update authority (para metadata-linked policy) can affect user-facing trust y protocol assumptions. Teams often reuse one operational key para convenience, then struggle a separate powers later. A better patron is a predefine authority roles y revocation milestones as part de launch gobernanza.\n\nSupply planning should distinguish issuance desde distribution. Initial supply tells you que is minted; recipient allocations tell you que is distributed at launch. Esos values should be validated con exact integer math, not float formatting. Invariant checks such as `recipientsTotal <= initialSupply` are simple but prevent serious release mistakes.\n\nToken-2022 extensions deepen este anatomy. Transfer comision config introduces comision basis points y caps; default cuenta estado changes cuenta activation posture; permanent delegate creates a privileged transfer actor. Each extension implies additional authority y monitoring requirements. Your launch plan must encode estos requirements as explicit steps y include human-readable labels so reviewers can confirm intent.\n\nFinally, deterministic address derivation en curso herramientas is a useful engineering discipline. Even when pseudo-addresses are usado para offline planning, stable derivation functions improve reproducibility y reduce reviewer ambiguity. El same mindset carries a real despliegues where deterministic cuenta derivation is foundational.\n\nStrong teams also pair mint-anatomy reviews con explicit incident playbooks: que a do if an authority key is lost, rotated, o compromised, y como a communicate esos events a integrators sin causing panic.\n\n## Pitfall: One-key authority convenience\n\nUsando a single key para minting, freezing, y metadata updates simplifies setup but concentrates riesgo. Authority compromise then becomes a full-token compromise rather than a contained incident.\n\n## Sanity Checklist\n\n1. Validate decimals y supply fields before plan generation.\n2. Record mint/freeze/update authority roles y custody modelo.\n3. Confirm recipient allocation totals con integer math.\n4. Review extension authorities independently desde mint authority.\n",
            "duration": "45 min"
          },
          "token-v2-extension-safety-pitfalls": {
            "title": "Extension seguridad pitfalls: comision configs, delegate abuse, default cuenta estado",
            "content": "# Extension seguridad pitfalls: comision configs, delegate abuse, default cuenta estado\n\nToken-2022 extensions let teams express policy en a standard token framework, but policy power is exactly where operational failures happen. Seguridad issues en token launches are rarely exotic cryptography failures. They are usually configuration mistakes: comision caps set too high, delegates granted too broadly, o frozen default estados introduced sin recovery controls. Produccion engineering must treat extension configuration as seguridad-critical logic.\n\nTransfer comision configuration is a good example. A basis-point comision looks simple, yet behavior depends en cap interaction y token decimals. If maxComision is undersized, large transfers saturate quickly y effective comision curve becomes nonlinear. If maxComision is oversized, treasury extraction can exceed expected user tolerance. Deterministic simulations across example transfer sizes are essential before launch, y esos simulations should be reviewed por both protocol y product teams.\n\nPermanent delegate is another high-riesgo feature. It can enable managed flujos, but it also creates a privileged actor ese may transfer tokens sin normal owner signatures depending en policy scope. If delegate authority is not governed por clear controls y revocation procedures, compromise riesgo rises sharply. En many incidents, teams enabled delegate-like authority para convenience, then discovered too late ese gobernanza y monitoring were insufficient.\n\nDefault cuenta estado introduces user-experience y compliance implications. A frozen default estado can enforce controlled activation, but it also creates onboarding failure if thaw paths are unclear o unavailable en partner carteras. Teams should verify thaw strategy, authority custody, y fallback procedures before enabling frozen defaults en produccion.\n\nEl safest engineering workflow is deterministic y reviewable: validate config, normalize extension fields, generate initialization plan labels, simulate transfer outcomes, y produce invariant lists. Ese sequence creates a shared artifact para engineering, seguridad, legal, y support stakeholders. When questions arise, teams can inspect exact intended policy rather than infer desde fragmented scripts.\n\nFinally, treat extension combinations as compounded riesgo. Each extension may be individually reasonable, yet combined authority interactions can create hidden escalation paths. Cross-extension threat modeling is therefore mandatory para serious launches.\n\n## Pitfall: Comision y delegate settings shipped sin scenario simulation\n\nTeams often validate only \"happy path\" transfer examples. Sin boundary simulations y authority abuse scenarios, dangerous configurations can pass review y surface only after users are affected.\n\n## Sanity Checklist\n\n1. Simulate comision behavior at low/medium/high transfer sizes.\n2. Document delegate authority scope y emergency revocation path.\n3. Verify frozen default cuentas have explicit thaw operations.\n4. Review combined extension authority interactions para escalation riesgo.\n",
            "duration": "45 min"
          },
          "token-v2-validate-config-derive": {
            "title": "Validate token config + derive deterministic addresses offline",
            "content": "# Validate token config + derive deterministic addresses offline\n\nImplement strict config validation y deterministic pseudo-derivation:\n- validate decimals, u64 strings, recipient totals, extension fields\n- derive stable pseudo mint y ATA addresses desde hash seeds\n- return normalized validated config + derivations\n",
            "duration": "35 min"
          }
        }
      },
      "token-v2-module-launch-pack": {
        "title": "Token Launch Pack Project",
        "description": "Construir deterministic validation, planning, y simulation workflows ese produce reviewable launch artifacts y clear go/no-go criteria.",
        "lessons": {
          "token-v2-build-init-plan": {
            "title": "Construir Token-2022 initialization instruccion plan",
            "content": "# Construir Token-2022 initialization instruccion plan\n\nCreate a deterministic offline initialization plan:\n- create mint cuenta step\n- init mint step con decimals\n- append selected extension steps en stable order\n- base64 encode step payloads con explicit encoding version\n",
            "duration": "35 min"
          },
          "token-v2-simulate-fees-supply": {
            "title": "Construir mint-a + transfer-comision math + simulation",
            "content": "# Construir mint-a + transfer-comision math + simulation\n\nImplement pure simulation para transfer comisiones y launch distribution:\n- comision = min(maxComision, floor(amount * comisionBps / 10000))\n- aggregate distribution totals deterministically\n- ensure no negative supply y no oversubscription\n",
            "duration": "35 min"
          },
          "token-v2-launch-checklist": {
            "title": "Launch checklist: params, upgrade/authority strategy, airdrop/pruebas plan",
            "content": "# Launch checklist: params, upgrade/authority strategy, airdrop/pruebas plan\n\nA successful token launch is an operations exercise as much as a programming task. Por el time users see your token en carteras, dozens de choices have already constrained seguridad, gobernanza, y UX. Produccion token engineering therefore needs a launch checklist ese turns abstract diseno intent into verifiable execution steps.\n\nStart con parameter closure. Name, symbol, decimals, initial supply, authority addresses, extension configuration, y recipient allocations must be finalized y reviewed as one immutable package before execution. Many launch incidents come desde late parameter changes made en disconnected scripts. Deterministic launch pack generation prevents este por forcing a single source de truth.\n\nAuthority strategy is el second pillar. Decide which authorities remain active after launch, which are revoked, y which move a multisig custody. A common best practice is staged authority reduction: keep temporary controls through rollout validation, then revoke o transfer a gobernanza once monitoring baselines are stable. Este must be planned explicitly, not improvised during launch day.\n\nPruebas strategy should include deterministic offline tests y limited online rehearsal. Offline checks validate config schemas, instruccion payload encoding, comision simulations, y supply invariants. Optional devnet rehearsal validates operational playbooks (funding, sequence execution, monitoring) but should not be your only validation layer. If offline checks fail, devnet success is not meaningful.\n\nAirdrop y distribution planning should include recipient reconciliation y rollback strategy. Teams often focus en minting y forget operational constraints around recipient list correctness, timing windows, y support escalation paths. Deterministic distribution plans con stable labels make reconciliation simpler y reduce accidental double execution.\n\nMonitoring y communication are equally important. Define launch metrics en advance: minted supply observed, distribution completion count, comision behavior sanity, y extension-specific health checks. Publish user-facing notices para any non-obvious behavior such as transfer comisiones o frozen default cuenta estado. Clear communication lowers support load y improves trust.\n\nFinally, write down hard stop conditions. If invariants fail, if authority keys mismatch, o if distribution deltas diverge desde expected totals, launch should pause immediately. Engineering discipline means refusing a proceed when seguridad checks are red.\n\n## Pitfall: Treating launch as a one-shot script run\n\nSin an explicit checklist y rollback criteria, teams can execute technically valid instruccions ese violate business o gobernanza intent. Successful launches are controlled workflows, not single commands.\n\n## Sanity Checklist\n\n1. Freeze a canonical config payload before execution.\n2. Approve authority lifecycle y revocation milestones.\n3. Run deterministic offline simulation y invariant checks.\n4. Reconcile recipient totals y distribution labels.\n5. Define go/no-go criteria y escalation owners.\n",
            "duration": "45 min"
          },
          "token-v2-launch-pack-checkpoint": {
            "title": "Emit stable LaunchPackSummary",
            "content": "# Emit stable LaunchPackSummary\n\nCompose full project output as stable JSON:\n- normalized authorities y extensions\n- supply totals y optional comision modelo examples\n- deterministic plan metadata y invariants\n- fixtures hash + encoding version metadata\n",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-mobile": {
    "title": "Solana Mobile Development",
    "description": "Construir produccion-ready mobile Solana dApps con MWA, robusto cartera session architecture, explicit signing UX, y disciplined distribution operations.",
    "duration": "6 hours",
    "tags": [
      "mobile",
      "saga",
      "dapp-store",
      "react-native"
    ],
    "modules": {
      "module-mobile-wallet-adapter": {
        "title": "Mobile Cartera Adapter",
        "description": "Core MWA protocol, session lifecycle control, y resilient cartera handoff patrones para produccion mobile apps.",
        "lessons": {
          "mobile-wallet-overview": {
            "title": "Mobile Cartera Vision general",
            "content": "# Mobile Cartera Vision general\n\nSolana Mobile development is built around el Solana Mobile Stack (SMS), a set de standards y herramientas disenoed para seguro, high-calidad crypto-native mobile experiences. SMS is more than a hardware initiative; it defines interoperable cartera communications, trusted execution patrones, y distribution infrastructure tailored a Web3 apps.\n\nA core piece is el Mobile Cartera Adapter (MWA) protocol. Instead de embedding private keys en dApps, MWA connects mobile dApps a external cartera apps para authorization, signing, y transaccion submission. Este separation mirrors browser cartera seguridad en desktop y prevents dApps desde directly handling secret keys.\n\nSaga introduced el first flagship reference device para Solana Mobile concepts, including Seed Vault-oriented workflows. Even when users are not en Saga, SMS standards remain useful because protocol-level interoperability is el target: any cartera implementing MWA can serve compatible apps.\n\nEl Solana dApp Store is another foundational element. It provides a distribution channel para crypto applications con policy considerations better aligned a en-chain apps than traditional app stores. Teams can ship cartera-native functionality, tokenized features, y en-chain social mechanics sin el same constraints often imposed por conventional app marketplaces.\n\nKey architectural principles para mobile Solana apps:\n\n- Keep signing en cartera context, not app context.\n- Treat session authorization as revocable y short-lived.\n- Construir graceful fallback if cartera app is missing.\n- Optimize para intermittent connectivity y mobile latencia.\n\nTypical mobile flujo:\n\n1. dApp requests authorization via MWA.\n2. Cartera prompts user a approve cuenta access.\n3. dApp construirs transaccions y requests signatures.\n4. Cartera returns signed payload o submits transaccion.\n5. dApp observes confirmation y updates UI.\n\nMobile UX needs explicit estado transitions (authorizing, awaiting cartera, signing, submitted, confirmed). Ambiguity causes user drop-off quickly en small screens.\n\nPara Solana teams, mobile is not a “mini web app”; it requires deliberate protocol y UX diseno choices. SMS y MWA provide a seguro baseline so developers can ship en-chain experiences con produccion-grade signing y session models en handheld devices.\n\n## Practico architecture split\n\nTreat your mobile stack as three independent sistemas:\n1. UI app estado y navigation.\n2. Cartera transport/session estado (MWA lifecycle).\n3. En-chain transaccion intent y confirmation estado.\n\nIf you couple estos layers tightly, cartera switch interruptions y app backgrounding can corrupt flujo estado. If they stay separated, recovery is predictable.\n\n## Que users should comisionl\n\nGood mobile crypto UX is not \"fewer steps at all costs.\" It is clear intent, explicit signing context, y safe recoverability when app switching o network instability happens.\n",
            "duration": "35 min"
          },
          "mwa-integration": {
            "title": "MWA Integration",
            "content": "# MWA Integration\n\nIntegrating Mobile Cartera Adapter typically starts con `@solana-mobile/mobile-wallet-adapter` APIs y an interaction patron built around `transact()`. Within a transaccion session, el app can authorize, request capabilities, sign transaccions, y handle cartera responses en a structured way.\n\nA simplified integration flujo:\n\n```typescript\nimport { transact } from '@solana-mobile/mobile-wallet-adapter-protocol-web3js';\n\nawait transact(async (wallet) => {\n  const auth = await wallet.authorize({\n    cluster: 'devnet',\n    identity: { name: 'JazzCode Mobile', uri: 'https://jazzcode.vercel.app' },\n  });\n\n  const account = auth.accounts[0];\n  // build tx, request signing/submission\n});\n```\n\nAuthorization yields one o more cuentas plus auth tokens para session continuation. Persist estos tokens carefully y invalidate them en sign-out. Do not assume tokens remain valid forever; cartera apps can revoke sessions.\n\nPara signing, you can request:\n\n- `signTransactions` (sign only)\n- `signAndSendTransactions` (cartera signs y submits)\n- `signMessages` (SIWS-like auth flujos)\n\nDeep links are usado under el hood a switch between your dApp y cartera. Ese means estado restoration matters: your app should survive process backgrounding y resume pending operation estado en return.\n\nPractico engineering tips:\n\n- Implement idempotent transaccion request handling.\n- Scomo a visible “Waiting para cartera approval” estado.\n- Handle user cancellation explicitly, not as generic failure.\n- Retry network submission separately desde signing when possible.\n\nSeguridad considerations:\n\n- Bind sessions a app identity metadata.\n- Usa short-lived backend nonces para message-sign auth.\n- Never log full signed payloads con sensitive context.\n\nMWA is effectively your mobile signing transport layer. If its estado machine is robusto, your app comisionls professional y trustworthy. If estado handling is weak, users experience “stuck” flujos y may distrust el dApp even if en-chain logic is correct.",
            "duration": "35 min"
          },
          "mobile-transaction": {
            "title": "Construir a Mobile Transaccion Function",
            "content": "# Construir a Mobile Transaccion Function\n\nImplement a helper ese formats a deterministic MWA transaccion request summary string.\n\nExpected output format:\n\n`<cluster>|<payer>|<instructionCount>`\n\nUsa este exact order y delimiter.",
            "duration": "50 min"
          }
        }
      },
      "module-dapp-store-and-distribution": {
        "title": "dApp Store & Distribution",
        "description": "Publishing, operational readiness, y trust-centered mobile UX practices para Solana app distribution.",
        "lessons": {
          "dapp-store-submission": {
            "title": "dApp Store Submission",
            "content": "# dApp Store Submission\n\nPublishing a el Solana dApp Store requires more than packaging binaries. Teams should treat submission as a product, compliance, y seguridad review process. A strong submission demonstrates safe cartera interactions, clear user communication, y operational readiness.\n\nSubmission readiness checklist:\n\n- Stable release construirs para target Android devices.\n- Clear app identity y support channels.\n- Cartera interaction flujos tested para cancellation y failure recovery.\n- Privacy policy y terms aligned a en-chain behaviors.\n- Transparent handling de tokenized features y en-app value flujos.\n\nOne distinguishing concept en Solana mobile distribution is token-aware product diseno. Apps may usa NFT-gated access, en-chain subscriptions, o tokenized entitlements. Estos flujos must be understandable a users y not hide financial consequences. Review teams typically evaluate whether permissions y cartera prompts are proportional a app behavior.\n\nNFT-based licensing models can be implemented por checking ownership de specific collection assets at runtime. If licensing depends en assets, construir robusto indexacion y refresh behavior so users are not locked out due a temporary RPC/indexer mismatch.\n\nOperational best practices para review success:\n\n- Provide reproducible test cuentas y walkthroughs.\n- Include a “safe mode” o demo path if cartera connection fails.\n- Avoid unexplained signature prompts.\n- Log non-sensitive diagnostics para support.\n\nPost-submission lifecycle matters too. Plan como you will handle urgent fixes, cartera SDK updates, y chain-level incidents. Mobile releases can take time a propagate, so feature flags y backend kill-switches para riesgoy pathways are valuable.\n\nDistribution strategy should also include analytics around onboarding funnels, cartera connect success rates, y transaccion completion rates. Estos metrics identify mobile-specific friction ese desktop-oriented teams often miss.\n\nA successful dApp Store submission reflects seguro protocol integration y mature product operations. If your cartera interactions are explicit, fail-safe, y user-centered, your app is far more likely a pass review y retain users en produccion.",
            "duration": "35 min"
          },
          "mobile-best-practices": {
            "title": "Mobile Best Practices",
            "content": "# Mobile Best Practices\n\nMobile crypto UX requires balancing speed, seguridad, y trust. Users make high-stakes decisions en small screens, often en unstable networks. Solana mobile apps should therefore optimize para explicitness y recoverability, not just visual polish.\n\n**Biometric gating** is useful para sensitive local actions (revealing seed-dependent views, exporting cuenta datos, approving high-riesgo actions), but cartera-level signing decisions should remain en cartera app context. Avoid construiring fake en-app “confirm” screens ese look like signing prompts.\n\n**Session keys y scoped auth** improve UX por reducing repetitive approvals. Comoever, scope must be tightly constrained (allowed methods, time window, limits). Session credentials should be revocable y auditable.\n\n**Offline y poor-network behavior** must be handled intentionally:\n\n- Queue non-critical reads.\n- Retry idempotent submissions con backoff.\n- Distinguish “signed but not submitted” desde “submitted but unconfirmed.”\n\n**Push notifications** are valuable para transaccion outcomes, liquidation alerts, y gobernanza events. Notifications should include enough context para user seguridad but never leak sensitive datos.\n\nUX patrones ese consistently improve conversion:\n\n- Scomo transaccion simulation summaries before cartera handoff.\n- Display clear statuses: construiring, awaiting signature, submitted, confirmed.\n- Provide explorer links y retry actions.\n- Usa plain-language error messages con suggested fixes.\n\nSeguridad hygiene:\n\n- Pin trusted RPC endpoints o usa reputable providers con fallback.\n- Validate cuenta ownership y expected program IDs en all client-side decoded datos.\n- Protect analytics pipelines desde sensitive payload leakage.\n\nAccessibility y internationalization matter para global adoption. Ensure touch targets, contrast, y localization de riesgo messages are adequate. Para crypto workflows, misunderstanding can cause irreversible loss.\n\nFinally, measure reality: connect success rate, signature approval rate, drop-off after cartera switch, y average time-a-confirmation. Mobile teams ese instrument estos metrics can iteratively remove friction y increase trust.\n\nGreat Solana mobile apps comisionl predictable under stress. If users always understand que they are signing, que estado they are en, y como a recover, your product is operating at produccion calidad.",
            "duration": "35 min"
          }
        }
      }
    }
  },
  "solana-testing": {
    "title": "Pruebas Solana Programs",
    "description": "Construir robusto Solana pruebas sistemas across local, simulated, y network environments con explicit seguridad invariants y release-calidad confidence gates.",
    "duration": "6 hours",
    "tags": [
      "testing",
      "bankrun",
      "anchor",
      "devnet"
    ],
    "modules": {
      "module-testing-foundations": {
        "title": "Pruebas Foundations",
        "description": "Core test strategy across unit/integration layers con deterministic workflows y adversarial case coverage.",
        "lessons": {
          "testing-approaches": {
            "title": "Pruebas Approaches",
            "content": "# Pruebas Approaches\n\nPruebas Solana programs requires multiple layers because failures can occur en logic, cuenta validation, transaccion composition, o network behavior. A produccion pruebas strategy usually combines unit tests, integration tests, y end-a-end validation across local validadors y devnet.\n\n**Unit tests** validate isolated business logic con minimal runtime overhead. En Rust, pure helper functions (math, estado transitions, invariant checks) should be unit-tested aggressively because they are easy a execute y fast en CI.\n\n**Integration tests** execute against realistic program invocation paths. Para Anchor projects, este often means `anchor test` con local validador setup, cuenta initialization flujos, y instruccion-level assertions. Integration tests should cover both positive y adversarial inputs, including invalid cuentas, unauthorized signers, y boundary values.\n\n**End-a-end tests** include frontend/client composition plus cartera y RPC interactions. They catch issues ese lower layers miss: incorrect cuenta ordering, wrong PDA derivations en client code, y serialization mismatches.\n\nCommon tools:\n\n- `solana-program-test` para Rust-side pruebas con en-process banks simulation.\n- `solana-bankrun` para deterministic TypeScript integration pruebas.\n- Anchor TypeScript client para instruccion construiring y assertions.\n- Playwright/Cypress para app-level transaccion flujo tests.\n\nTest coverage priorities:\n\n1. Authorization y signer checks.\n2. Cuenta ownership y PDA seed constraints.\n3. Arithmetic boundaries y comision logic.\n4. CPI behavior y failure rollback.\n5. Upgrade compatibility y migration paths.\n\nA frequent anti-patron is only pruebas happy paths con one cartera y static inputs. Este misses most exploit classes. Robusto suites include malicious cuenta substitution, stale o duplicated cuentas, y partial failure simulation.\n\nEn CI, separate fast deterministic suites desde slower network-dependent suites. Run deterministic tests en every push, y run heavier devnet suites en merge o release.\n\nEffective Solana pruebas is about confidence under adversarial conditions, not just green checkmarks. If your tests modelo attacker behavior y cuenta-level edge cases, you will prevent el majority de produccion incidents before despliegue.\n\n## Practico suite diseno rule\n\nMap every critical instruccion a at least one test en each layer:\n- unit test para pure invariant/math logic\n- integration test para cuenta validation y estado transitions\n- environment test para cartera/RPC orchestration\n\nIf one layer is missing, incidents usually appear en ese blind spot first.",
            "duration": "35 min"
          },
          "bankrun-testing": {
            "title": "Bankrun Pruebas",
            "content": "# Bankrun Pruebas\n\nSolana Bankrun provides deterministic, high-speed test execution para Solana programs desde TypeScript environments. It emulates a local bank-like runtime where transaccions can be processed predictably, cuentas can be inspected directly, y temporal estado can be manipulated para pruebas scenarios like vesting unlocks o oracle staleness.\n\nCompared con relying en external devnet, Bankrun gives repeatability. Este is crucial para CI pipelines where flaky network behavior can mask regressions.\n\nTypical Bankrun workflow:\n\n1. Start test context con target program loaded.\n2. Create keypairs y funded test cuentas.\n3. Construir y process transaccions via BanksClient-like API.\n4. Assert post-transaccion cuenta estado.\n5. Advance slots/time para time-dependent logic tests.\n\nConceptual setup:\n\n```typescript\n// pseudocode\nconst context = await startBankrun({ programs: [...] });\nconst client = context.banksClient;\n\n// process tx and inspect accounts deterministically\n```\n\nPor que Bankrun is powerful:\n\n- Fast iteration para protocol teams.\n- Deterministic block/slot control.\n- Rich cuenta inspection sin explorer dependency.\n- Easy simulation de multi-step protocol flujos.\n\nHigh-value Bankrun test scenarios:\n\n- Liquidation eligibility after oracle/time movement.\n- Vesting y cliff unlock schedule transitions.\n- Comision accumulator updates across many operations.\n- CPI behavior con mocked downstream cuenta estados.\n\nCommon mistakes:\n\n- Asserting only transaccion success sin estado validation.\n- Ignoring rent y cuenta lamport changes.\n- Not pruebas replay/idempotency behaviors.\n\nUsa helper factories para test cuentas y PDA derivations so tests remain concise. Keep transaccion construirers en reusable utilities a avoid drift between test y produccion clients.\n\nBankrun is not a replacement para all environments, but it is one de el best layers para deterministic integration confidence en Solana. Teams ese invest en comprehensive Bankrun suites tend a catch estado-machine bugs significantly earlier than teams relying only en devnet smoke tests.",
            "duration": "35 min"
          },
          "write-bankrun-test": {
            "title": "Write a Counter Program Bankrun Test",
            "content": "# Write a Counter Program Bankrun Test\n\nImplement a helper ese returns el expected counter value after a sequence de increment operations. Este mirrors a deterministic assertion you would usa en a Bankrun test.\n\nReturn el final numeric value as a string.",
            "duration": "50 min"
          }
        }
      },
      "module-advanced-testing": {
        "title": "Avanzado Pruebas",
        "description": "Fuzzing, devnet validation, y CI/CD release controls para safer protocol changes.",
        "lessons": {
          "fuzzing-trident": {
            "title": "Fuzzing con Trident",
            "content": "# Fuzzing con Trident\n\nFuzzing explores large input spaces automatically a find bugs ese handcrafted tests miss. Para Solana y Anchor programs, Trident-style fuzzing workflows generate randomized instruccion sequences y parameter values, then check invariants such as “total supply never decreases incorrectly” o “vault liabilities never exceed assets.”\n\nUnlike unit tests ese validate expected examples, fuzzing asks: que if inputs are weird, extreme, o adversarial en combinations we did not think about?\n\nCore fuzzing components:\n\n- **Generators** para instruccion inputs y cuenta estados.\n- **Harness** ese executes generated transaccions.\n- **Invariants** ese must always hold.\n- **Shrinking** a minimize failing inputs para depuracion.\n\nUseful invariants en DeFi protocols:\n\n- Conservation de value across transfers y burns.\n- Non-negative balances y debt estados.\n- Authority invariants (only valid signer modifies privileged estado).\n- Price y collateral constraints under liquidation logic.\n\nFuzzing strategy tips:\n\n- Start con a small instruccion set y one invariant.\n- Add estadoful multi-step scenarios (deposit->borrow->repay->withdraw).\n- Include random cuenta ordering y malicious cuenta substitution cases.\n- Track coverage a avoid blind spots.\n\nCoverage analysis matters: if fuzzing never reaches critical branches (error paths, CPI failure handlers, liquidation branches), it gives false confidence. Integrate branch coverage tools where possible.\n\nTrident y similar fuzzers are especially good at discovering arithmetic edge cases, stale estado assumptions, y unexpected estado transitions desde unusual call sequences.\n\nCI integration approach:\n\n- Run short fuzz campaigns en every PR.\n- Run longer campaigns nightly.\n- Persist failing seeds as regression tests.\n\nFuzzing should complement, not replace, deterministic tests. Deterministic suites provide explicit behavior guarantees; fuzzing provides adversarial exploration at scale.\n\nPara serious Solana protocols handling user funds, fuzzing is no longer optional. It is one de el highest-leverage investments para preventing unknown-unknown bugs before mainnet exposure.",
            "duration": "35 min"
          },
          "devnet-testing": {
            "title": "Devnet Pruebas",
            "content": "# Devnet Pruebas\n\nDevnet pruebas bridges el gap between deterministic local tests y real-world network conditions. While local validadors y Bankrun are ideal para speed y reproducibility, devnet reveals behavior under real RPC latencia, block produccion timing, comision markets, y cuenta history constraints.\n\nA robusto devnet test strategy includes:\n\n- Automated program despliegue a a dedicated devnet keypair.\n- Deterministic fixture creation (airdrop, mint setup, PDAs).\n- Smoke tests para critical instruccion paths.\n- Monitoring de transaccion confirmation y log outputs.\n\nImportant devnet caveats:\n\n- Estado is shared y can be noisy.\n- Airdrop limits can throttle tests.\n- RPC providers may differ en confiabilidad y rate limits.\n\nA reduce flakiness:\n\n- Usa dedicated namespaces/seeds per CI run.\n- Add retries para transient network failures.\n- Bound test runtime y fail con actionable logs.\n\nProgram upgrade pruebas is particularly important en devnet. Validate ese new binaries preserve cuenta compatibility y migrations execute as expected. Incompatible changes can brick existing cuentas if not tested.\n\nChecklist para release-candidate validation:\n\n1. Deploy upgraded program binary.\n2. Run migration instruccions.\n3. Execute backward-compatibility read paths.\n4. Execute all critical write instruccions.\n5. Verify event/log schema expected por indexers.\n\nPara financial protocols, include oracle integration tests y liquidation path checks against live-like comisionds if possible.\n\nDevnet should not be your only calidad gate, but it is el best pre-mainnet signal para environment-related issues. Teams ese ship sin meaningful devnet validation often discover RPC edge cases y timing bugs en produccion.\n\nTreat devnet as a staging environment con disciplined test orchestration, clear observability, y explicit rollback plans.",
            "duration": "35 min"
          },
          "ci-cd-pipeline": {
            "title": "CI/CD Pipeline para Solana",
            "content": "# CI/CD Pipeline para Solana\n\nA mature Solana CI/CD pipeline enforces calidad gates across code, tests, seguridad checks, y despliegue workflows. Para program teams, CI is not just linting Rust y TypeScript; it is about protecting en-chain invariants before irreversible releases.\n\nRecommended pipeline stages:\n\n1. **Static checks**: formatting, lint, type checks.\n2. **Unit/integration tests**: deterministic local execution.\n3. **Seguridad checks**: dependency scan, optional static analyzers.\n4. **Construir artifacts**: reproducible program binaries.\n5. **Staging deploy**: optional devnet despliegue y smoke tests.\n6. **Manual approval** para produccion deploy.\n\nGitHub Actions is a common choice. A typical workflow matrix runs Rust y Node tasks en parallel a reduce cycle time. Cache Cargo y pnpm dependencies aggressively.\n\nExample conceptual workflow snippets:\n\n```yaml\n- run: cargo test --workspace\n- run: pnpm lint && pnpm typecheck && pnpm test\n- run: anchor build\n- run: anchor test --skip-local-validator\n```\n\nPara despliegues:\n\n- Store deploy keypairs en seguro secrets management.\n- Restrict deploy jobs a protected branches/tags.\n- Emit program IDs y transaccion signatures as artifacts.\n\nProgram verification is critical. Where possible, verify deployed binary matches source-controlled construir output. Este strengthens trust y simplifies audits.\n\nOperational seguridad practices:\n\n- Usa feature flags para high-riesgo logic activation.\n- Keep rollback strategy documented.\n- Monitor post-deploy metrics (error rates, failed tx ratio, latencia).\n\nInclude regression tests para previously discovered bugs. Every produccion incident should produce a permanent automated test.\n\nA strong CI/CD pipeline is an engineering control, not a convenience. It reduces release riesgo, accelerates safe iteration, y provides confidence ese code changes preserve seguridad y protocol correctness under produccion conditions.",
            "duration": "35 min"
          }
        }
      }
    }
  },
  "solana-indexing": {
    "title": "Solana Indexacion & Analytics",
    "description": "Construir a produccion-grade Solana event indexer con deterministic decoding, resilient ingestion contracts, checkpoint recovery, y analytics outputs teams can trust.",
    "duration": "10 hours",
    "tags": [
      "indexing",
      "analytics",
      "events",
      "tokens",
      "solana"
    ],
    "modules": {
      "indexing-v2-foundations": {
        "title": "Indexacion Foundations",
        "description": "Events modelo, token decoding, y transaccion parsing fundamentals con schema discipline y deterministic normalization.",
        "lessons": {
          "indexing-v2-events-model": {
            "title": "Events modelo: transaccions, logs, y program instruccions",
            "content": "# Events modelo: transaccions, logs, y program instruccions\n\nIndexacion Solana starts con comprension where datos lives y como a extract structured events desde raw chain datos. Unlike EVM chains where events are explicit log topics, Solana encodes program estado changes en cuenta updates y program logs. Your indexer must parse estos sources y transform them into a queryable event stream.\n\nA transaccion en Solana contains one o more instruccions. Each instruccion targets a program, includes cuenta metas, y carries opaque instruccion datos. When executed, programs emit log entries via solana_program::msg o similar macros. Estos logs, combined con pre/post cuenta estados, form el raw material para event indexacion.\n\nEl indexer pipeline typically follows: fetch → parse → normalize → store. Fetch retrieves transaccion metadata via RPC o geyser plugins. Parse extracts program logs y cuenta diffs. Normalize converts raw datos into domain-specific events con stable schemas. Store persists events con appropriate indexacion para queries.\n\nKey concepts para normalization: instruccion program IDs identify which decoder a apply, cuenta ownership determines datos layout, y log prefixes often indicate event types (e.g., \"Transfer\", \"Mint\", \"Burn\"). Your indexer must handle multiple program versions gracefully, maintaining backward compatibility as instruccion layouts evolve.\n\nIdempotency is critical. Block reorganizations are rare en Solana but possible during forks. Your indexacion pipeline should handle replayed transaccions sin duplicating events. Este typically means usando transaccion signatures as unique keys y implementing upsert semantics en el storage layer.\n\n## Operator modelo mental\n\nTreat your indexer as a datos product con explicit contracts:\n1. ingest contract (que raw inputs are accepted),\n2. normalization contract (stable event schema),\n3. serving contract (que query consumers can rely en).\n\nWhen estos contracts are versioned y documented, protocol upgrades become manageable instead de breaking downstream analytics unexpectedly.\n\n## Checklist\n- Understand transaccion → instruccions → logs hierarchy\n- Identify program IDs y cuenta ownership para datos layout selection\n- Normalize raw logs into domain events con stable schemas\n- Implement idempotent ingestion usando transaccion signatures\n- Plan para program version evolution en decoders\n\n## Red flags\n- Parsing logs sin validating program IDs\n- Assuming fixed cuenta ordering across program versions\n- Missing idempotency leading a duplicate events\n- Storing raw datos sin normalized event extraction\n",
            "duration": "45 min"
          },
          "indexing-v2-token-decoding": {
            "title": "Token cuenta decoding y SPL layout",
            "content": "# Token cuenta decoding y SPL layout\n\nSPL Token cuentas follow a standardized binary layout ese indexers must parse a track balances y mint operations. Comprension este layout enables you a extract meaningful datos desde raw cuenta bytes sin relying en external APIs.\n\nA token cuenta contains: mint address (32 bytes), owner address (32 bytes), amount (8 bytes u64), delegate (32 bytes, optional), estado (1 byte), is_native (1 byte + 8 bytes if native), delegated_amount (8 bytes), y close_authority (36 bytes optional). El total size is typically 165 bytes para standard cuentas.\n\nCuenta discriminators help identify cuenta types. SPL Token cuentas are owned por el Token Program (TokenkegQfeZyiNwAJbNbGKPFXCWuBvf9Ss623VQ5DA) o Token-2022. Your indexer should verify ownership before attempting a parse, as malicious cuentas could mimic datos layouts.\n\nDecoding involves: read el 32-byte mint, verify it matches expected token, read el 64-bit amount (little-endian), convert a decimal representation usando mint decimals, y track owner para balance aggregation. Always handle malformed datos gracefully - truncated cuentas o unexpected sizes should not crash el indexer.\n\nPara balance diffs, compare pre-transaccion y post-transaccion estados. A transfer emits no explicit event but changes two cuenta amounts. Your indexer must detect estos changes por comparing estados before y after instruccion execution.\n\n## Checklist\n- Verify token program ownership before parsing\n- Decode mint, owner, y amount fields correctly\n- Handle little-endian u64 conversion properly\n- Support both Token y Token-2022 programs\n- Implement graceful handling para malformed cuentas\n\n## Red flags\n- Parsing sin ownership verification\n- Ignoring mint decimals en amount conversion\n- Assuming fixed cuenta sizes sin bounds checking\n- Missing balance diff detection para transfers\n",
            "duration": "50 min"
          },
          "indexing-v2-decode-token-account": {
            "title": "Desafio: Decode token cuenta + diff token balances",
            "content": "# Desafio: Decode token cuenta + diff token balances\n\nImplement deterministic token cuenta decoding y balance diffing:\n\n- Parse a 165-byte SPL Token cuenta layout\n- Extract mint, owner, y amount fields\n- Compute balance differences between pre/post estados\n- Return normalized event objects con stable ordering\n\nYour solution will be validated against multiple test cases con various token cuenta estados.",
            "duration": "45 min"
          },
          "indexing-v2-transaction-meta": {
            "title": "Transaccion meta parsing: logs, errores, y inner instruccions",
            "content": "# Transaccion meta parsing: logs, errores, y inner instruccions\n\nTransaccion metadata provides el context needed a index complex operations. Comprension como a parse logs, handle errores, y traverse inner instruccions enables comprehensive event extraction.\n\nProgram logs follow a hierarchical structure. El outermost logs scomo instruccion execution order, while inner logs reveal CPI calls. Each log line typically includes a prefix indicating severity o type: \"Program\", \"Invoke\", \"Success\", \"Fail\", o custom program messages. Your parser should handle nested invocation levels correctly.\n\nError handling distinguishes between transaccion-level failures y instruccion-level failures. A transaccion may succeed overall while individual instruccions fail (y are rolled back). Conversely, a single failing instruccion can cause el entire transaccion a fail. Indexers should record estos distinctions para accurate analytics.\n\nInner instruccions reveal el complete execution trace. When a program makes CPI calls, estos appear as inner instruccions en transaccion metadata. Indexers must traverse both top-level y inner instruccions a capture all estado changes. Este is especially important para protocols like Jupiter ese route through multiple DEXs.\n\nLog filtering improves efficiency. Rather than parsing all logs, indexers can filter por program ID prefixes o known event signatures. Comoever, be cautious - aggressive filtering might miss important events during protocol upgrades o edge cases.\n\n## Checklist\n- Parse program logs con proper nesting level tracking\n- Distinguish transaccion-level desde instruccion-level errores\n- Traverse inner instruccions para complete CPI traces\n- Implement efficient log filtering por program ID\n- Handle both success y failure scenarios\n\n## Red flags\n- Ignoring inner instruccions y missing CPI events\n- Treating all log failures as transaccion failures\n- Parsing sin log level/depth context\n- Missing error context en indexed events\n",
            "duration": "50 min"
          }
        }
      },
      "indexing-v2-pipeline": {
        "title": "Indexacion Pipeline & Analytics",
        "description": "Construir end-a-end indexer pipeline behavior: idempotent ingestion, checkpoint recovery, y analytics aggregation at produccion scale.",
        "lessons": {
          "indexing-v2-index-transactions": {
            "title": "Desafio: Index transaccions a normalized events",
            "content": "# Desafio: Index transaccions a normalized events\n\nImplement a transaccion indexer ese produces normalized Event objects:\n\n- Parse instruccion logs y identify event types\n- Extract transfer events con desde/a/amount/mint\n- Handle multiple events per transaccion\n- Return stable, canonical JSON con sorted keys\n- Support idempotency via transaccion signature deduplication",
            "duration": "50 min"
          },
          "indexing-v2-pagination-caching": {
            "title": "Pagination, checkpointing, y caching semantics",
            "content": "# Pagination, checkpointing, y caching semantics\n\nProduccion indexers must handle large datasets efficiently while maintaining consistency. Pagination, checkpointing, y caching form el backbone de scalable indexacion infrastructure.\n\nPagination strategies depend en query patrones. Cursor-based pagination usando transaccion signatures provides stable ordering even during concurrent writes. Offset-based pagination can miss o duplicate entries during high-write periods. Para time-series datos, consider partitioning por slot o block time.\n\nCheckpointing enables recovery desde failures. Indexers should periodically save their processing position (last processed slot/signature) a durable storage. En restart, resume desde el checkpoint rather than re-indexacion desde genesis. Este patron is essential para long-running indexers handling months de chain history.\n\nCaching reduces redundant RPC calls. Cuenta metadata, program IDs, y decoded instruccion layouts can be cached con appropriate TTLs. Comoever, cache invalidation is critical - stale cache entries can lead a incorrect decoding o missed events. Consider usando slot-based versioning para cache entries.\n\nIdempotent writes prevent datos corruption. Even con checkpointing, duplicate processing can occur during retries. Usa transaccion signatures as unique identifiers y implement upsert semantics. Database constraints o unique indexes should enforce este at el storage layer.\n\n## Checklist\n- Implement cursor-based pagination para stable ordering\n- Save periodic checkpoints para failure recovery\n- Cache cuenta metadata con slot-based invalidation\n- Enforce idempotent writes via unique constraints\n- Handle backfills sin duplicating events\n\n## Red flags\n- Usando offset pagination para high-write datasets\n- Missing checkpointing requiring full re-index en restart\n- Caching sin proper invalidation strategies\n- Allowing duplicate events desde retry logic\n",
            "duration": "45 min"
          },
          "indexing-v2-analytics": {
            "title": "Analytics aggregation: per cartera, per token metrics",
            "content": "# Analytics aggregation: per cartera, per token metrics\n\nRaw event datos becomes valuable through aggregation. Construiring analytics pipelines enables insights into user behavior, token flujos, y protocol usage patrones.\n\nPer-cartera analytics track individual user activity. Key metrics include: transaccion count, unique tokens held, total volume transferred, first/last activity timestamps, y interaction patrones con specific programs. Estos metrics power user segmentation y engagement analysis.\n\nPer-token analytics track asset-level metrics. Important aggregations include: total transfer volume, unique holders, holder distribution (whales vs retail), velocity (average time between transfers), y cross-program usage. Estos inform tokenomics analysis y market research.\n\nTime-windowed aggregations support trend analysis. Daily, weekly, y monthly rollups enable comparison across time periods. Consider usando tumbling windows para fixed periods o sliding windows para moving averages. Materialized views can pre-compute common aggregations para query rendimiento.\n\nNormalization ensures consistent comparisons. Convert all amounts a human-readable decimals, normalize timestamps a UTC, y usa consistent address formatting (base58). Deduplicate events desde failed transaccions ese may still appear en logs.\n\n## Checklist\n- Aggregate per-cartera metrics (volume, token count, activity)\n- Aggregate per-token metrics (holders, velocity, distribution)\n- Implement time-windowed rollups para trend analysis\n- Normalize amounts, timestamps, y addresses\n- Exclude failed transaccions desde aggregates\n\n## Red flags\n- Mixing raw y decimal-adjusted amounts\n- Including failed transaccions en volume metrics\n- Missing time normalization across timezones\n- Storing unbounded raw datos sin aggregation\n",
            "duration": "45 min"
          },
          "indexing-v2-analytics-checkpoint": {
            "title": "Checkpoint: Produce stable JSON analytics summary",
            "content": "# Checkpoint: Produce stable JSON analytics summary\n\nImplement el final analytics checkpoint ese produces a deterministic summary:\n\n- Aggregate events into per-cartera y per-token metrics\n- Generate sorted, stable JSON output\n- Include timestamp y summary statistics\n- Handle edge cases (empty datasets, single events)\n\nEste checkpoint validates your complete indexacion pipeline desde raw datos a analytics.",
            "duration": "50 min"
          }
        }
      }
    }
  },
  "solana-payments": {
    "title": "Solana Payments & Checkout Flujos",
    "description": "Construir produccion-grade Solana payment flujos con robusto validation, replay-safe idempotency, seguro webhooks, y deterministic receipts para reconciliation.",
    "duration": "10 hours",
    "tags": [
      "payments",
      "checkout",
      "webhooks",
      "transactions",
      "solana"
    ],
    "modules": {
      "payments-v2-foundations": {
        "title": "Payment Foundations",
        "description": "Address validation, idempotency strategy, y payment intent diseno para confiable checkout behavior.",
        "lessons": {
          "payments-v2-address-validation": {
            "title": "Address validation y memo strategies",
            "content": "# Address validation y memo strategies\n\nPayment flujos en Solana require robusto address validation y thoughtful memo strategies. Unlike traditional payment sistemas con cuenta numbers, Solana uses base58-encoded public keys ese must be validated before any value transfer.\n\nAddress validation involves three layers: format validation, derivation check, y ownership verification. Format validation ensures el string is valid base58 y decodes a 32 bytes. Derivation check optionally verifies el address is en el Ed25519 curve (para cartera addresses) o off-curve (para PDAs). Ownership verification confirms el cuenta exists y is owned por el expected program.\n\nMemos attach metadata a payments. El SPL Memo program enables attaching UTF-8 strings a transaccions. Common usa cases include: order IDs, invoice references, customer identifiers, y compliance datos. Memos are not encrypted y are visible en-chain, so never include sensitive information.\n\nMemo best practices: keep under 256 bytes para efficiency, usa structured formats (JSON) para machine parsing, include versioning para future compatibility, y hash sensitive identifiers rather than storing them plaintext. Consider usando deterministic memo formats ese can be regenerated desde payment context para idempotency checks.\n\nAddress poisoning is an attack vector where attackers create addresses visually similar a legitimate ones. Countermeasures include: displaying addresses con checksums, usando name services (Solana Name Service, Bonfida) where appropriate, y implementing confirmation steps para large transfers.\n\n## Merchant-safe memo template\n\nA practico memo format is:\n`v1|order:<id>|shop:<merchantId>|nonce:<shortHash>`\n\nEste keeps memos short, parseable, y versioned while avoiding direct storage de sensitive user details.\n\n## Checklist\n- Validate base58 encoding y 32-byte length\n- Distinguish between en-curve y off-curve addresses\n- Verify cuenta ownership para program-specific payments\n- Usa SPL Memo program para structured metadata\n- Implement address poisoning protections\n\n## Red flags\n- Transferring a unvalidated addresses\n- Storing sensitive datos en plaintext memos\n- Skipping ownership checks para token cuentas\n- Trusting visually similar addresses sin verification\n",
            "duration": "45 min"
          },
          "payments-v2-idempotency": {
            "title": "Idempotency keys y replay protection",
            "content": "# Idempotency keys y replay protection\n\nPayment sistemas must handle network failures gracefully. Idempotency ensures ese retrying a failed request produces el same outcome as el original, preventing duplicate charges y inconsistent estado.\n\nIdempotency keys are unique identifiers generated por clients para each payment intent. El server stores processed keys y their outcomes, returning cached results para duplicate submissions. Keys should be: globally unique (UUID v4), client-generated, y persisted con sufficient TTL a handle extended retry windows.\n\nKey generation strategies include: UUID v4 con timestamp prefix, hash de payment parameters (amount, recipient, timestamp), o structured keys combining merchant ID y local sequence numbers. El key must be stable across retries but unique across distinct payments.\n\nReplay protection prevents malicious o accidental re-execution. Beyond idempotency, transaccions should include: recent blockhash freshness (prevents old transaccion replay), durable nonce para offline signing scenarios, y amount/time bounds where applicable.\n\nError classification affects retry behavior. Network errores warrant retries con exponential backoff. Validation errores (insufficient funds, invalid address) should fail fast sin retry. Timeout errores require careful handling - el payment may have succeeded, so query status before retrying.\n\n## Checklist\n- Generate unique idempotency keys para each payment intent\n- Store processed keys con outcomes para deduplication\n- Implement appropriate TTL based en retry windows\n- Usa recent blockhash para transaccion freshness\n- Classify errores para appropriate retry strategies\n\n## Red flags\n- Allowing duplicate payments desde retries\n- Generating idempotency keys server-side only\n- Ignoring timeout ambiguity en status checking\n- Storing keys sin expiration\n",
            "duration": "50 min"
          },
          "payments-v2-payment-intent": {
            "title": "Desafio: Create payment intent con validation",
            "content": "# Desafio: Create payment intent con validation\n\nImplement a payment intent creator con full validation:\n\n- Validate recipient address format (base58, 32 bytes)\n- Validate amount (positive, within limits)\n- Generate deterministic idempotency key\n- Return structured payment intent object\n- Handle edge cases (zero amount, invalid address)\n\nYour implementation will be tested against various valid y invalid inputs.",
            "duration": "45 min"
          },
          "payments-v2-tx-building": {
            "title": "Transaccion construiring y key metadata",
            "content": "# Transaccion construiring y key metadata\n\nConstruiring payment transaccions requires careful attention a instruccion construction, cuenta metadata, y program interactions. El goal is creating valid, efficient transaccions ese minimize comisiones while ensuring correctness.\n\nInstruccion construction follows a patron: identify el program (Sistema Program para SOL transfers, Token Program para SPL transfers), prepare cuenta metas con correct writable/signer flags, serialize instruccion datos according a el program's layout, y compute el transaccion message con all required fields.\n\nCuenta metadata is critical. Para SOL transfers, you need: desde (signer + writable), a (writable). Para SPL transfers: token cuenta desde (signer + writable), token cuenta a (writable), owner (signer), y potentially a delegate if usando delegated transfer. Missing o incorrect flags cause runtime failures.\n\nComision optimizacion strategies include: batching multiple payments into one transaccion (up a compute unit limits), usando address lookup tables (ALTs) para cuentas referenced multiple times, y setting appropriate compute unit limits a avoid overpaying para simple operations.\n\nTransaccion validation before submission: verify all required signatures are present, check recent blockhash is fresh, estimate compute units if possible, y validate instruccion datos encoding matches el expected layout.\n\n## Checklist\n- Set correct writable/signer flags en all cuentas\n- Usa appropriate program para transfer type (SOL vs SPL)\n- Validate instruccion datos encoding\n- Include recent blockhash para freshness\n- Consider batching para multiple payments\n\n## Red flags\n- Missing signer flags en comision payer\n- Incorrect writable flags en recipient cuentas\n- Usando wrong program ID para token type\n- Stale blockhash causing transaccion rejection\n",
            "duration": "45 min"
          }
        }
      },
      "payments-v2-implementation": {
        "title": "Implementation & Verification",
        "description": "Transaccion construiring, webhook authenticity checks, y deterministic receipt generation con clear error-estado handling.",
        "lessons": {
          "payments-v2-transfer-tx": {
            "title": "Desafio: Construir transfer transaccion",
            "content": "# Desafio: Construir transfer transaccion\n\nImplement a transfer transaccion construirer:\n\n- Construir SystemProgram.transfer para SOL transfers\n- Construir TokenProgram.transfer para SPL transfers\n- Return instruccion bundle con correct key metadata\n- Include comision payer y blockhash\n- Support deterministic output para pruebas",
            "duration": "50 min"
          },
          "payments-v2-webhooks": {
            "title": "Webhook signing y verification",
            "content": "# Webhook signing y verification\n\nWebhooks enable asynchronous payment notifications. Seguridad requires cryptographic signing so recipients can verify webhook authenticity y detect tampering.\n\nWebhook signing uses HMAC-SHA256 con a shared secret. El sender computes: signature = HMAC-SHA256(secret, payload). El signature is included en a header (e.g., X-Webhook-Signature). Recipients recompute el HMAC y compare, usando constant-time comparison a prevent timing attacks.\n\nPayload canonicalization ensures consistent signing. JSON objects must be serialized con: sorted keys (alphabetical), no extra whitespace, consistent number formatting, y UTF-8 encoding. Sin canonicalization, {\"a\":1,\"b\":2} y {\"b\":2,\"a\":1} produce different signatures.\n\nIdempotency en webhooks prevents duplicate processing. Webhook payloads should include an idempotency key o event ID. Recipients store processed IDs y ignore duplicates. Este handles retries desde el sender y network-level duplicates.\n\nSeguridad best practices: rotate secrets periodically, usa different secrets per environment (dev/staging/prod), include timestamps y reject old webhooks (e.g., >5 minutes), y verify IP allowlists where feasible. Never include sensitive datos like private keys o full card numbers en webhooks.\n\n## Checklist\n- Sign webhooks con HMAC-SHA256 y shared secret\n- Canonicalize JSON payloads con sorted keys\n- Include event ID para idempotency\n- Verify signatures con constant-time comparison\n- Implement timestamp validation\n\n## Red flags\n- Unsigned webhooks trusting sender IP alone\n- Non-canonical JSON causing verification failures\n- Missing idempotency handling duplicate events\n- Including secrets o sensitive datos en payload\n",
            "duration": "45 min"
          },
          "payments-v2-error-states": {
            "title": "Error estado machine y receipt format",
            "content": "# Error estado machine y receipt format\n\nPayment flujos require well-defined estado machines a handle el complexity de asynchronous confirmations, failures, y retries. Clear estado transitions y receipt formats ensure confiable payment tracking.\n\nPayment estados typically include: pending (intent created, not yet submitted), processing (transaccion submitted, awaiting confirmation), succeeded (transaccion confirmed, payment complete), failed (transaccion failed o rejected), y cancelled (intent explicitly cancelled before submission). Each estado has valid transitions y terminal estados.\n\nEstado transition rules: pending can transition a processing, cancelled, o failed; processing can transition a succeeded o failed; succeeded y failed are terminal. Invalid transitions (e.g., succeeded → failed) indicate bugs o datos corruption.\n\nReceipt format standardization enables interoperability. A payment receipt should include: payment intent ID, transaccion signature (if submitted), amount y currency, recipient address, timestamp, status, y verification datos (e.g., explorer link). Receipts should be JSON con canonical ordering para deterministic hashing.\n\nExplorer links provide transparency. Para Solana, construct explorer URLs usando: https://explorer.solana.com/tx/{signature}?cluster={network}. Include estos en receipts y webhook payloads so users can verify transaccions independently.\n\n## Checklist\n- Define clear payment estados y valid transitions\n- Implement estado machine validation\n- Generate standardized receipt JSON\n- Include explorer links para verification\n- Handle all terminal estados appropriately\n\n## Red flags\n- Ambiguous estados sin clear definitions\n- Missing terminal estado handling\n- Non-deterministic receipt formats\n- No explorer links para verification\n",
            "duration": "40 min"
          },
          "payments-v2-webhook-receipt": {
            "title": "Desafio: Verify webhook y produce receipt",
            "content": "# Desafio: Verify webhook y produce receipt\n\nImplement el final payment flujo checkpoint:\n\n- Verify signed webhook signature (HMAC-SHA256)\n- Extract payment details desde payload\n- Generate standardized receipt JSON\n- Include explorer link y verification datos\n- Ensure deterministic, sorted output\n\nEste validates el complete payment flujo desde intent a receipt.",
            "duration": "50 min"
          }
        }
      }
    }
  },
  "solana-nft-compression": {
    "title": "NFTs & Compressed NFTs Fundamentals",
    "description": "Master compressed NFT engineering en Solana: Merkle commitments, prueba sistemas, collection modeling, y produccion seguridad checks.",
    "duration": "12 hours",
    "tags": [
      "nfts",
      "compression",
      "merkle-trees",
      "cnfts",
      "solana"
    ],
    "modules": {
      "cnft-v2-merkle-foundations": {
        "title": "Merkle Foundations",
        "description": "Tree construction, leaf hashing, insertion mechanics, y el en-chain/off-chain commitment modelo behind compressed assets.",
        "lessons": {
          "cnft-v2-merkle-trees": {
            "title": "Merkle trees para estado compression",
            "content": "# Merkle trees para estado compression\n\nCompressed NFTs (cNFTs) en Solana usa Merkle trees a dramatically reduce storage costs. Comprension Merkle trees is essential para working con compressed NFTs y construiring compression-aware applications.\n\nA Merkle tree is a binary hash tree where each leaf node contains a hash de datos, y each non-leaf node contains el hash de its children. El root hash commits a el entire tree structure y all leaf datos. Este structure enables efficient pruebas de inclusion sin revealing el entire dataset.\n\nTree construction follows a bottom-up approach: hash each datos element a create leaves, pair adjacent leaves y hash their concatenation a create parents, y repeat until a single root remains. Para odd numbers de nodes, el last node is typically promoted a el next level o paired con a zero hash depending en el implementation.\n\nSolana's cNFT implementation uses concurrent Merkle trees con 16-bit depth (max 65,536 leaves). El tree estado is stored en-chain as a small cuenta containing just el root hash y metadata. Actual leaf datos (NFT metadata) is stored off-chain, typically via RPC providers o indexers.\n\nKey properties de Merkle trees: any leaf change affects el root, inclusion pruebas require only log2(n) hashes, y el root serves as a cryptographic commitment a all datos. Estos properties enable estado compression while maintaining verifiability.\n\n## Practico cNFT architecture rule\n\nTreat compressed NFT sistemas as two synchronized layers:\n1. en-chain commitment layer (tree root + update rules),\n2. off-chain datos layer (metadata + indexacion + prueba serving).\n\nIf either layer is weakly validated, ownership y metadata trust can diverge.\n\n## Checklist\n- Understand binary hash tree construction\n- Know como leaf changes propagate a el root\n- Calculate prueba size: log2(n) hashes para n leaves\n- Recognize depth limits (16-bit = 65,536 max leaves)\n- Understand en-chain vs off-chain datos split\n\n## Red flags\n- Treating Merkle roots as datos storage (they're commitments)\n- Ignoring depth limits when planning collections\n- Storing sensitive datos assuming it's \"hidden\" en el tree\n- Not validating pruebas against el current root\n",
            "duration": "50 min"
          },
          "cnft-v2-leaf-hashing": {
            "title": "Leaf hashing conventions y metadata",
            "content": "# Leaf hashing conventions y metadata\n\nLeaf hashing determines como NFT metadata is committed a el Merkle tree. Comprension estos conventions ensures compatibility con compression standards y proper prueba generation.\n\nLeaf structure para cNFTs includes: asset ID (derived desde tree address y leaf index), owner public key, delegate (optional), nonce para uniqueness, y metadata hash. El exact encoding follows el Metaplex Bubblegum specification, usando deterministic serialization para consistent hashing.\n\nHashing algorithm uses Keccak-256 para Ethereum compatibility, con domain separation via prefixed constants. El leaf hash is computed as: hash(prefix || asset_data) where prefix prevents collision con other hash usages. Multiple prefix values exist para different operation types (mint, transfer, burn).\n\nMetadata handling stores el full NFT metadata (name, symbol, uri, creators, royalties) off-chain. Only a hash de el metadata is stored en el leaf. Este enables large metadata sin en-chain storage costs while maintaining integrity via el hash commitment.\n\nCreator verification uses a separate signing process. Creators sign el asset ID a verify authenticity. Estos signatures are stored alongside pruebas but not en el Merkle tree itself, allowing flexible verification sin tree updates.\n\n## Checklist\n- Understand leaf structure components\n- Usa correct hashing algorithm (Keccak-256)\n- Include proper domain separation prefixes\n- Store metadata off-chain con hash commitment\n- Handle creator signatures separately desde tree\n\n## Red flags\n- Usando wrong hashing algorithm\n- Missing domain separation en leaf hashes\n- Storing full metadata en-chain en compressed NFTs\n- Ignoring creator verification requirements\n",
            "duration": "50 min"
          },
          "cnft-v2-merkle-insert": {
            "title": "Desafio: Implement Merkle tree insert + root updates",
            "content": "# Desafio: Implement Merkle tree insert + root updates\n\nConstruir a Merkle tree implementation con insertions:\n\n- Insert leaves y compute new root\n- Update parent hashes up el tree\n- Handle tree growth y depth limits\n- Return deterministic root updates\n\nTest cases will verify correct root evolution after multiple insertions.",
            "duration": "50 min"
          },
          "cnft-v2-proof-generation": {
            "title": "Prueba generation y path computation",
            "content": "# Prueba generation y path computation\n\nMerkle pruebas enable verification de leaf inclusion sin accessing el entire tree. Comprension prueba generation is essential para working con compressed NFTs y construiring verification sistemas.\n\nA Merkle prueba consists de: el leaf datos (o its hash), a list de sibling hashes (one per level), y el leaf index (determining el path). El verifier recomputes el root por hashing el leaf con siblings en el correct order, comparing against el expected root.\n\nPrueba generation requires traversing desde leaf a root. At each level, record el sibling hash (el other child de el parent). El leaf index determines whether el current hash goes left o right en each concatenation. Para index i at level n, el position is determined por el nth bit de i.\n\nPrueba verification recomputes el root: start con el leaf hash, para each sibling en el prueba list, concatenate current hash con sibling (order depends en leaf index bit), hash el result, y compare final result con expected root. Equality proves inclusion.\n\nPrueba size efficiency: para a tree con n leaves, pruebas contain log2(n) hashes. Este is dramatically smaller than el full tree (n hashes), enabling scalable verification. A 65,536 leaf tree requires only 16 hashes per prueba.\n\n## Checklist\n- Collect sibling hashes at each tree level\n- Usa leaf index bits a determine concatenation order\n- Verify pruebas por recomputing root hash\n- Handle edge cases (empty tree, single leaf)\n- Optimize prueba size para network transmission\n\n## Red flags\n- Incorrect concatenation order en verification\n- Usando wrong sibling hash at any level\n- Not validating prueba length matches tree depth\n- Trusting pruebas sin root comparison\n",
            "duration": "45 min"
          }
        }
      },
      "cnft-v2-proof-system": {
        "title": "Prueba Sistema & Seguridad",
        "description": "Prueba generation, verification, collection integrity, y seguridad hardening against replay y metadata spoofing.",
        "lessons": {
          "cnft-v2-proof-verification": {
            "title": "Desafio: Implement prueba generation + verifier",
            "content": "# Desafio: Implement prueba generation + verifier\n\nConstruir a complete prueba sistema:\n\n- Generate pruebas desde a Merkle tree y leaf index\n- Verify pruebas against a root hash\n- Handle invalid pruebas (wrong siblings, wrong index)\n- Return deterministic boolean results\n\nTests will verify both successful pruebas y rejection de invalid attempts.",
            "duration": "55 min"
          },
          "cnft-v2-collection-minting": {
            "title": "Collection mints y metadata simulation",
            "content": "# Collection mints y metadata simulation\n\nCompressed NFT collections usa a collection mint as el parent NFT, enabling grouping y verification de related assets. Comprension este hierarchy is essential para construiring collection-aware applications.\n\nCollection structure includes: a standard SPL NFT as el collection mint, cNFTs referencing el collection en their metadata, y el Merkle tree containing all cNFT leaves. El collection mint provides en-chain provenance while cNFTs provide scalable asset issuance.\n\nMetadata simulation para pruebas allows development sin actual en-chain transaccions. Simulated metadata includes: name, symbol, uri (typically pointing a off-chain JSON), seller comision basis points (royalties), creators array con shares, y collection reference. Este datos structure matches en-chain format para seamless migration.\n\nRoyalty enforcement through Metaplex's token metadata standard specifies seller comisiones as basis points (e.g., 500 = 5%). Creators array defines comision distribution con verified flags. cNFTs inherit estos settings desde their metadata, enforced during transfers via el Bubblegum program.\n\nAttacks en compressed NFTs include: invalid pruebas (claiming non-existent NFTs), index manipulation (usando wrong leaf index), metadata spoofing (fake off-chain datos), y collection impersonation (fake collection mints). Mitigations include prueba verification, metadata hash validation, y collection mint verification.\n\n## Checklist\n- Understand collection mint hierarchy\n- Simulate metadata para pruebas\n- Implement royalty calculations\n- Verify collection membership\n- Handle metadata hash verification\n\n## Red flags\n- Accepting NFTs sin collection verification\n- Ignoring royalty settings en transfers\n- Trusting off-chain metadata sin hash validation\n- Not validating pruebas para ownership claims\n",
            "duration": "45 min"
          },
          "cnft-v2-attack-surface": {
            "title": "Attack surface: invalid pruebas y replay",
            "content": "# Attack surface: invalid pruebas y replay\n\nCompressed NFTs introduce unique seguridad considerations. Comprension attack vectors y mitigations is critical para construiring seguro compression-aware applications.\n\nInvalid prueba attacks attempt a verify non-existent NFTs. An attacker provides a fabricated leaf hash y fake sibling hashes hoping a produce a valid-looking verification. Mitigation: always verify against el current root desde a trusted source (RPC, en-chain cuenta), y validate prueba structure (correct depth, valid hash lengths).\n\nIndex manipulation exploits usa valid pruebas but wrong indices. Since leaf indices determine prueba path, changing el index produces a different root computation. Mitigation: bind asset IDs a specific indices y validate index-asset correspondence during verification.\n\nReplay attacks re-usa old pruebas after tree updates. When leaves are added o modified, el root changes y old pruebas become invalid. Comoever, if an application caches roots, it might accept stale pruebas. Mitigation: always usa current root, implement prueba timestamps where applicable.\n\nMetadata attacks substitute fake off-chain datos. Since metadata is stored off-chain con only a hash en-chain, attackers might serve altered metadata files. Mitigation: verify metadata hashes against leaf commitments, usa content-addressed storage (IPFS), y validate creator signatures.\n\nCollection spoofing creates fake collections mimicking legitimate ones. Attackers mint similar-looking NFTs con fake collection references. Mitigation: verify collection mint addresses against known good lists, check collection verification status, y validate authority signatures.\n\n## Checklist\n- Verify pruebas against current root\n- Validate leaf index matches asset ID\n- Implement replay protection para pruebas\n- Hash-verify off-chain metadata\n- Verify collection mint authenticity\n\n## Red flags\n- Accepting cached/stale roots para verification\n- Ignoring leaf index validation\n- Trusting off-chain metadata sin verification\n- Not checking collection verification status\n",
            "duration": "45 min"
          },
          "cnft-v2-compression-checkpoint": {
            "title": "Checkpoint: Simulate mint + verify ownership prueba",
            "content": "# Checkpoint: Simulate mint + verify ownership prueba\n\nComplete el compression lab checkpoint:\n\n- Simulate minting a cNFT (insert leaf, update root)\n- Generate ownership prueba para el minted NFT\n- Verify el prueba against current root\n- Output stable audit trace con sorted keys\n- Detect y reporte invalid prueba attempts\n\nEste validates your complete Merkle tree implementation para compressed NFTs.",
            "duration": "60 min"
          }
        }
      }
    }
  },
  "solana-governance-multisig": {
    "title": "Gobernanza & Multisig Treasury Ops",
    "description": "Construir produccion-ready DAO gobernanza y multisig treasury sistemas con deterministic vote cuentaing, timelock seguridad, y seguro execution controls.",
    "duration": "11 hours",
    "tags": [
      "governance",
      "multisig",
      "dao",
      "treasury",
      "solana"
    ],
    "modules": {
      "governance-v2-governance": {
        "title": "DAO Gobernanza",
        "description": "Proposal lifecycle, deterministic voting mechanics, quorum policy, y timelock seguridad para credible DAO gobernanza.",
        "lessons": {
          "governance-v2-dao-model": {
            "title": "DAO modelo: proposals, voting, y execution",
            "content": "# DAO modelo: proposals, voting, y execution\n\nDecentralized gobernanza en Solana follows a proposal-based modelo where token holders vote en changes y el DAO treasury executes approved decisions. Comprension este flujo is essential para construiring y participating en en-chain organizations.\n\nEl gobernanza lifecycle has four stages: proposal creation (anyone con sufficient stake can propose), voting period (token holders vote para/against/abstain), queueing (successful proposals enter a timelock), y execution (el proposal's instruccions are executed). Each stage has specific requirements y time constraints.\n\nProposal creation requires a minimum token deposit a prevent spam. El proposer submits: title, description link, y executable instruccions (typically base64 serialized). Deposits are returned if el proposal passes, forfeited if it fails (depending en DAO configuration).\n\nVoting power is typically determined por token balance at a specific snapshot block. Some DAOs usa vote escrow (veToken) models where locking tokens para longer periods multiplies voting power. Quorum requirements ensure sufficient participation - a proposal needs both majority approval y minimum participation a pass.\n\nExecution seguridad involves timelocks between approval y execution. Este delay (often 1-7 days) allows users a exit if they disagree con el outcome. Emergency powers may exist para critical fixes but should require higher thresholds.\n\n## Gobernanza confiabilidad rule\n\nA proposal sistema is only credible if outcomes are reproducible desde public inputs. Ese means deterministic vote math, explicit snapshot rules, clear timelock transitions, y auditable execution traces para treasury effects.\n\n## Checklist\n- Understand el four-stage gobernanza lifecycle\n- Know proposal deposit y spam prevention mechanisms\n- Calculate voting power y quorum requirements\n- Implement timelock seguridad delays\n- Plan para emergency execution paths\n\n## Red flags\n- Allowing proposal creation sin deposits\n- Missing quorum requirements para participation\n- Zero timelock en sensitive operations\n- Unclear vote counting methodologies\n",
            "duration": "45 min"
          },
          "governance-v2-quorum-math": {
            "title": "Quorum math y vote weight calculation",
            "content": "# Quorum math y vote weight calculation\n\nAccurate vote counting is critical para legitimate gobernanza outcomes. Comprension quorum requirements, vote weight calculation, y edge cases ensures fair decision-making.\n\nQuorum defines minimum participation para a valid vote. Common formulas include: absolute token amount (e.g., 4% de total supply must vote), relative a circulating supply, o dynamic based en recent participation. Quorum prevents small groups desde making unilateral decisions.\n\nVote weight calculation considers: token balance at snapshot block, lockup duration multipliers (veToken modelo), delegation relationships, y abstention handling. Abstentions typically count toward quorum but not toward approval ratio.\n\nApproval thresholds vary por proposal type. Simple majority (>50%) is standard para routine matters. Supermajority (e.g., 2/3) may be required para constitutional changes. Some DAOs usa quadratic voting a reduce whale influence, though este has sybil resistance desafios.\n\nEdge cases include: ties (usually fail), late vote changes (often blocked after deadline), vote delegation revocation timing, y quorum manipulation (e.g., flash loan attacks prevented por snapshot blocks).\n\n## Checklist\n- Define clear quorum formulas y minimums\n- Calculate vote weights con snapshot blocks\n- Handle abstentions appropriately\n- Set appropriate approval thresholds por proposal type\n- Protect against manipulation attacks\n\n## Red flags\n- No quorum requirements\n- Vote weight based en current balance (flash loan riesgo)\n- Unclear tie-breaking rules\n- Changing rules mid-proposal\n",
            "duration": "50 min"
          },
          "governance-v2-timelocks": {
            "title": "Timelock estados y execution scheduling",
            "content": "# Timelock estados y execution scheduling\n\nTimelocks provide a critical seguridad layer between gobernanza approval y execution. Comprension timelock estados y transitions ensures confiable proposal execution.\n\nTimelock estados include: pending (proposal passed, waiting para delay), ready (delay elapsed, can be executed), executed (instruccions processed), cancelled (withdrawn por proposer o guardian), y expired (execution window passed). Each estado has valid transitions y authorized actors.\n\nDelay configuration balances seguridad con responsiveness. Too short (hours) allows insufficient reaction time. Too long (weeks) delays urgent fixes. Common ranges are 1-7 days, con shorter delays para routine matters y longer para significant changes.\n\nExecution windows prevent indefinite pending estados. After el timelock delay, proposals typically have a limited window (e.g., 7-14 days) a be executed. Expired proposals must be re-proposed y re-voted.\n\nCancellations add flexibility. Proposers may withdraw proposals before voting ends. Guardians (if configured) may cancel malicious proposals. Cancellation typically returns deposits unless abuse is detected.\n\n## Checklist\n- Define clear timelock estado machine\n- Set appropriate delays por proposal type\n- Implement execution window limits\n- Authorize cancellation actors\n- Handle estado transitions atomically\n\n## Red flags\n- No execution window limits\n- Missing cancellation mechanisms\n- Estado transitions sin authorization checks\n- Indefinite pending estados\n",
            "duration": "45 min"
          },
          "governance-v2-quorum-voting": {
            "title": "Desafio: Implement quorum/voting estado machine",
            "content": "# Desafio: Implement quorum/voting estado machine\n\nConstruir a deterministic voting sistema:\n\n- Calculate vote weights desde token balances\n- Check quorum requirements\n- Determine pass/fail based en thresholds\n- Handle abstentions correctly\n- Return stable estado transitions\n\nYour implementation will be tested against various vote distributions.",
            "duration": "50 min"
          }
        }
      },
      "governance-v2-multisig": {
        "title": "Multisig Treasury",
        "description": "Multisig transaccion construction, approval controls, replay defenses, y seguro treasury execution patrones.",
        "lessons": {
          "governance-v2-multisig": {
            "title": "Multisig transaccion construiring y approvals",
            "content": "# Multisig transaccion construiring y approvals\n\nMultisig carteras provide collective control over treasury funds. Comprension multisig construction, approval flujos, y seguridad patrones is essential para treasury operations.\n\nMultisig structure defines: signers (public keys ese can approve), threshold (minimum signatures required), y instruccions (operations a execute). Common configurations include 2-de-3 (two approvals desde three signers), 3-de-5, y custom arrangements.\n\nTransaccion lifecycle: propose (one signer creates transaccion con instruccions), approve (other signers review y approve), y execute (once threshold is met, anyone can execute). Each stage is recorded en-chain para transparency.\n\nApproval tracking maintains estado per signer per transaccion. Signers can approve, reject, o cancel their approval. Double-signing is prevented por tracking which signers have already approved. Rejections may block transaccions o simply be recorded.\n\nSeguridad considerations: signer key management (hardware carteras recommended), threshold selection (balance seguridad vs availability), timelocks para large transfers, y emergency recovery paths. Lost signer keys should not freeze treasury funds permanently.\n\n## Checklist\n- Define signer set y threshold\n- Track per-signer approval estado\n- Enforce threshold before execution\n- Implement approval/revocation mechanics\n- Plan para lost key scenarios\n\n## Red flags\n- Single signer controlling treasury\n- No approval tracking en-chain\n- Threshold equal a signer count (no redundancy)\n- Missing rejection/cancellation mechanisms\n",
            "duration": "50 min"
          },
          "governance-v2-multisig-builder": {
            "title": "Desafio: Implement multisig tx construirer + approval rules",
            "content": "# Desafio: Implement multisig tx construirer + approval rules\n\nConstruir a multisig transaccion sistema:\n\n- Create transaccions con instruccions\n- Record signer approvals\n- Enforce threshold requirements\n- Handle approval revocation\n- Generate deterministic transaccion estado\n\nTests will verify threshold enforcement y approval tracking.",
            "duration": "55 min"
          },
          "governance-v2-safe-defaults": {
            "title": "Safe defaults: owner checks y replay guards",
            "content": "# Safe defaults: owner checks y replay guards\n\nGobernanza y multisig sistemas require robusto seguridad defaults. Comprension common vulnerabilities y their mitigations protects treasury funds.\n\nOwner checks validate ese transaccions only affect authorized cuentas. Para treasury operations, verify: el treasury cuenta is owned por el expected program, el signer set matches el multisig configuration, y instruccions target allowed programs. Missing owner checks enable cuenta substitution attacks.\n\nReplay guards prevent el same approved transaccion desde being executed multiple times. Sin replay protection, an observer could resubmit an executed transaccion a drain funds. Guards include: unique transaccion nonces, executed flags en transaccion estado, y signature uniqueness checks.\n\nUpgrade seguridad considers como gobernanza changes affect existing proposals. If el multisig configuration changes, pending proposals should usa el old configuration while new proposals usa el new one. Atomic configuration changes prevent partial updates.\n\nEmergency stops provide circuit breakers. Guardian roles can pause operations during suspected attacks. Time delays en critical changes allow review periods. Estos seguridad valves should be tested regularly.\n\n## Checklist\n- Validate cuenta ownership before operations\n- Implement replay protection (nonces o flags)\n- Handle configuration changes safely\n- Add emergency pause mechanisms\n- Test seguridad controls regularly\n\n## Red flags\n- No owner verification en treasury cuentas\n- Missing replay protection\n- Immediate execution de critical changes\n- No emergency stop capability\n",
            "duration": "45 min"
          },
          "governance-v2-treasury-execution": {
            "title": "Desafio: Execute proposal y produce treasury diff",
            "content": "# Desafio: Execute proposal y produce treasury diff\n\nComplete el gobernanza simulator checkpoint:\n\n- Execute approved proposals con timelock validation\n- Apply treasury estado changes atomically\n- Generate execution trace con before/after diffs\n- Handle edge cases (expired, cancelled, insufficient funds)\n- Output stable, deterministic audit log\n\nEste validates your complete gobernanza/multisig implementation.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "solana-performance": {
    "title": "Solana Rendimiento & Compute Optimizacion",
    "description": "Master Solana rendimiento engineering con measurable optimizacion workflows: compute budgets, datos layouts, encoding efficiency, y deterministic cost modeling.",
    "duration": "11 hours",
    "tags": [
      "performance",
      "optimization",
      "compute",
      "serialization",
      "solana"
    ],
    "modules": {
      "performance-v2-foundations": {
        "title": "Rendimiento Foundations",
        "description": "Compute modelo, cuenta/datos layout decisions, y deterministic cost estimation para transaccion-level rendimiento reasoning.",
        "lessons": {
          "performance-v2-compute-model": {
            "title": "Compute modelo: budgets, costs, y limits",
            "content": "# Compute modelo: budgets, costs, y limits\n\nSolana's compute modelo enforces deterministic execution limits through compute budgets. Comprension este modelo is essential para construiring efficient programs ese stay within limits while maximizing utility.\n\nCompute units (CUs) measure execution cost. Every operation consumes CUs: instruccion execution, syscall usage, memory access, y logging. El default transaccion limit is 200,000 CUs (1.4 million con prioritization), y each cuenta has a 10MB max size limit.\n\nCompute budget instruccions allow transaccions a request specific limits y set priority comisiones. El ComputeBudgetProgram provides: setComputeUnitLimit (override default), setComputeUnitPrice (set priority comision per CU en micro-lamports). Priority comisiones increase transaccion inclusion probability during congestion.\n\nCost categories include: fixed costs (signature verification, cuenta loading), variable costs (instruccion execution, CPI calls), y memory costs (cuenta datos access size). Comprension estos categories helps optimize el right areas.\n\nMetering happens during execution. If a transaccion exceeds its compute budget, execution halts y el transaccion fails con an error. Failed transaccions still pay comisiones para consumed CUs, making optimizacion economically important.\n\n## Practico optimizacion loop\n\nUsa a repeatable loop:\n1. profile real CU usage,\n2. identify top cost drivers (datos layout, CPI count, logging),\n3. optimize one hotspot,\n4. re-measure y keep only proven wins.\n\nEste avoids rendimiento folklore y keeps code calidad intact.\n\n## Checklist\n- Understand compute unit consumption categories\n- Usa ComputeBudgetProgram para specific limits\n- Set priority comisiones during congestion\n- Monitor CU usage during development\n- Handle compute limit failures gracefully\n\n## Red flags\n- Ignoring compute limits en program diseno\n- Usando default limits unnecessarily high\n- Not pruebas con realistic datos sizes\n- Missing priority comision strategies para important transaccions\n",
            "duration": "45 min"
          },
          "performance-v2-account-layout": {
            "title": "Cuenta layout diseno y serialization cost",
            "content": "# Cuenta layout diseno y serialization cost\n\nCuenta datos layout significantly impacts compute costs. Well-disenoed layouts minimize serialization overhead y reduce cuenta access costs.\n\nSerialization formats affect cost. Borsh is el standard para Solana, offering compact binary encoding. Manual serialization can be more efficient para simple structures but increases bug riesgo. Avoid JSON o other text formats en-chain due a size y parsing cost.\n\nCuenta size impacts costs linearly. Loading a 10KB cuenta costs more than loading 1KB. Rent exemption requires more lamports para larger cuentas. Diseno layouts a minimize size: usa fixed-size arrays instead de Vecs where possible, pack booleans into bitflags, y usa appropriate integer sizes (u8/u16/u32/u64).\n\nDatos structure alignment affects both size y access patrones. Group related fields together para cache efficiency. Place frequently accessed fields at el beginning de el struct. Consider zero-copy deserialization para read-heavy operations.\n\nVersioning enables layout upgrades. Include a version byte at el start de cuenta datos. Migration logic can then handle different versions during deserialization. Plan para growth por reserving padding bytes en initial layouts.\n\n## Checklist\n- Usa Borsh para standard serialization\n- Minimize cuenta datos size\n- Usa appropriate integer sizes\n- Plan para versioning y upgrades\n- Consider zero-copy para read-heavy paths\n\n## Red flags\n- Usando JSON para en-chain datos\n- Oversized Vec collections\n- No versioning para upgrade paths\n- Unnecessary large integer types\n",
            "duration": "50 min"
          },
          "performance-v2-cost-model": {
            "title": "Desafio: Implement estimateCost(op) modelo",
            "content": "# Desafio: Implement estimateCost(op) modelo\n\nConstruir a compute cost estimation sistema:\n\n- Modelo costs para different operation types\n- Cuenta para instruccion complexity\n- Include memory access costs\n- Return baseline measurements\n- Handle edge cases (empty operations, large datos)\n\nYour estimator will be validated against known operation costs.",
            "duration": "50 min"
          },
          "performance-v2-instruction-data": {
            "title": "Instruccion datos size y encoding optimizacion",
            "content": "# Instruccion datos size y encoding optimizacion\n\nInstruccion datos size directly impacts transaccion cost y rendimiento. Optimizing encoding reduces comisiones y increases el operations possible within compute limits.\n\nCompact encoding uses minimal bytes a represent datos. Usa discriminants (u8) a identify instruccion types. Usa variable-length encoding (ULEB128) para sizes. Pack multiple boolean flags into a single u8 usando bit manipulation. Avoid unnecessary padding.\n\nCuenta deduplication reduces transaccion size. If an cuenta appears en multiple instruccions, include it once en el cuenta list y reference por index. Este is especially important para CPI-heavy transaccions.\n\nBatching combines multiple operations into one transaccion. Instead de N transaccions con 1 instruccion each, usa 1 transaccion con N instruccions. Batching amortizes signature verification y cuenta loading costs across operations.\n\nRight-sizing vectors prevents overallocation. Usa Vec::with_capacity when el size is known. Avoid unnecessary clones ese increase heap usage. Consider stack-allocated arrays para small, fixed-size datos.\n\n## Checklist\n- Usa compact discriminants para instruccion types\n- Pack boolean flags into bitfields\n- Deduplicate cuentas across instruccions\n- Batch operations when possible\n- Right-size collections a avoid waste\n\n## Red flags\n- Usando full u32 para small discriminants\n- Separate booleans instead de bitflags\n- Duplicate cuentas en transaccion lists\n- Unnecessary datos cloning\n",
            "duration": "45 min"
          }
        }
      },
      "performance-v2-optimization": {
        "title": "Optimizacion & Analysis",
        "description": "Layout optimizacion, compute budget tuning, y before/after rendimiento analysis con correctness safeguards.",
        "lessons": {
          "performance-v2-optimized-layout": {
            "title": "Desafio: Implement optimized layout/codec",
            "content": "# Desafio: Implement optimized layout/codec\n\nOptimize an cuenta datos layout while preserving semantics:\n\n- Reduce datos size through compact encoding\n- Maintain all original functionality\n- Preserve backward compatibility where possible\n- Pass regression tests\n- Measure y reporte size reduction\n\nYour optimized layout should be smaller but functionally equivalent.",
            "duration": "55 min"
          },
          "performance-v2-compute-budget": {
            "title": "Compute budget instruccion fundamentos",
            "content": "# Compute budget instruccion fundamentos\n\nCompute budget instruccions give developers control over resource allocation y transaccion prioritization. Comprension estos tools enables precise optimizacion.\n\nsetComputeUnitLimit requests a specific CU budget. El default is 200,000, but you can request up a 1,400,000. Requesting more than needed wastes comisiones since you pay para el limit, not actual usage. Requesting too little causes failures.\n\nsetComputeUnitPrice sets a priority comision en micro-lamports per CU. During congestion, transaccions con higher priority comisiones are more likely a be included. Priority comisiones are additional a base comisiones y go a validadors.\n\nRequesting compute units involves tradeoffs. Higher limits enable more complex operations but cost more. Priority comisiones increase inclusion probability but raise costs. Profile your transaccions a set appropriate limits.\n\nHeap size can also be configured. El default heap is 32KB, extendable a 256KB con compute budget instruccions. Large heap enables complex datos structures but increases CU consumption.\n\n## Checklist\n- Profile transaccions a determine actual CU usage\n- Set appropriate compute unit limits\n- Usa priority comisiones during congestion\n- Consider heap size para datos-heavy operations\n- Monitor cost vs inclusion probability tradeoffs\n\n## Red flags\n- Always usando maximum compute unit limits\n- Not setting priority comisiones during congestion\n- Ignoring heap size constraints\n- Not profiling before optimizacion\n",
            "duration": "40 min"
          },
          "performance-v2-micro-optimizations": {
            "title": "Micro-optimizacions y tradeoffs",
            "content": "# Micro-optimizacions y tradeoffs\n\nRendimiento optimizacion involves balancing competing concerns. Comprension tradeoffs helps make informed decisions about when y que a optimize.\n\nReadability vs rendimiento is a constant tension. Highly optimized code often sacrifices clarity. Optimize hot paths (frequently executed code) aggressively. Keep cold paths (rarely executed) readable y maintainable.\n\nSpace vs time tradeoffs appear frequently. Pre-computing values trades memory para speed. Compressing datos trades CPU para storage. Choose based en which resource is more constrained para your usa case.\n\nMaintainability vs optimizacion matters para long-term projects. Aggressive optimizacions can introduce bugs y make updates difficult. Document por que optimizacions exist y measure their impact.\n\nPremature optimizacion is a common trap. Profile before optimizing a identify actual bottlenecks. Theoretical optimizacions may not match real-world behavior. Focus en algorithmic improvements before micro-optimizacions.\n\nSeguridad must never be sacrificed para rendimiento. Bounds checking, ownership validation, y arithmetic seguridad are non-negotiable. Optimize around seguridad, not through it.\n\n## Checklist\n- Profile before optimizing\n- Focus en hot paths\n- Document optimizacion decisions\n- Balance readability y rendimiento\n- Never sacrifice seguridad para speed\n\n## Red flags\n- Optimizing sin profiling\n- Sacrificing seguridad para rendimiento\n- Unreadable code sin documentation\n- Optimizing cold paths unnecessarily\n",
            "duration": "45 min"
          },
          "performance-v2-perf-checkpoint": {
            "title": "Checkpoint: Compare before/after + output perf reporte",
            "content": "# Checkpoint: Compare before/after + output perf reporte\n\nComplete el optimizacion lab checkpoint:\n\n- Measure baseline rendimiento metrics\n- Apply optimizacion techniques\n- Verify correctness is preserved\n- Generate rendimiento comparison reporte\n- Output stable JSON con sorted keys\n\nEste validates your ability a optimize while maintaining correctness.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "defi-swap-aggregator": {
    "title": "DeFi Swap Aggregation",
    "description": "Master produccion swap aggregation en Solana: deterministic quote parsing, route optimizacion tradeoffs, deslizamiento seguridad, y confiabilidad-aware execution.",
    "duration": "12 hours",
    "tags": [
      "defi",
      "swap",
      "aggregator",
      "jupiter",
      "solana"
    ],
    "modules": {
      "swap-v2-fundamentals": {
        "title": "Swap Fundamentals",
        "description": "Token swap mechanics, deslizamiento protection, route composition, y deterministic swap plan construction con transparent tradeoffs.",
        "lessons": {
          "swap-v2-mental-model": {
            "title": "Swap modelo mental: mints, ATAs, decimals, y routes",
            "content": "# Swap modelo mental: mints, ATAs, decimals, y routes\n\nToken swaps en Solana follow a fundamentally different modelo than centralized exchanges. Comprension el construiring blocks — mints, associated token cuentas (ATAs), decimal precision, y route composition — is essential before writing any swap code.\n\nEvery SPL token en Solana is defined por a mint cuenta. El mint specifies el token's total supply, decimals (0–9), y authority. When you swap \"SOL para USDC,\" you are actually swapping wrapped SOL (mint `So11111111111111111111111111111111111111112`) para USDC (mint `EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v`). Native SOL must be wrapped into an SPL token before any program can manipulate it as a standard token.\n\nAssociated Token Cuentas (ATAs) are deterministic addresses derived desde a cartera y a mint usando el Associated Token Cuenta program. Para every token a cartera holds, there must be an ATA. If el recipient does not have an ATA para el output mint, el swap transaccion must include a `createAssociatedTokenAccountIdempotent` instruccion — a common source de transaccion failures when omitted.\n\nDecimal handling is critical. SOL uses 9 decimals (1 SOL = 1,000,000,000 lamports). USDC uses 6 decimals. When displaying \"22.5 USDC,\" el en-chain amount is 22,500,000. Mixing decimals between mints causes catastrophic pricing errores. Always convert human-readable amounts a raw integer amounts early y keep all math en integer space until el final display step.\n\nRoutes are el paths a swap takes through liquidez pools. A direct swap (SOL → USDC en a single pool) is el simplest case. When direct liquidez is insufficient o el price is better through an intermediary, el aggregator splits el swap into multiple \"legs\" — para example, SOL → mSOL → USDC. Each leg passes through a different AMM (Automated Market Maker) program like Whirlpool, Raydium, o Orca. El aggregator's job is a find el combination de legs ese produces el best output amount after comisiones.\n\nRoute optimizacion considers: pool liquidez depth, comision tiers, impacto de precio per leg, y el total compute cost de including multiple legs en one transaccion. More legs means more instruccions, more cuentas, y higher compute unit consumption — there is a practico limit a route complexity within Solana's transaccion size y compute budget constraints.\n\n## Execution-calidad triangle\n\nEvery route decision balances three competing goals:\n1. better output amount,\n2. lower failure riesgo (deslizamiento + stale quote exposure),\n3. lower execution overhead (cuentas + compute + latencia).\n\nStrong aggregators make este tradeoff explicit rather than optimizing only a single metric.\n\n## Checklist\n- Identify input y output mints por their full base58 addresses\n- Ensure ATAs exist para both input y output tokens before swapping\n- Convert all amounts a raw integer form usando el correct decimal places\n- Understand ese routes may have multiple legs through different AMM programs\n- Consider compute budget implications de complex routes\n\n## Red flags\n- Mixing decimal scales between different mints\n- Forgetting a create output ATA before el swap instruccion\n- Assuming all swaps are single-hop direct routes\n- Ignoring comisiones charged por intermedio pools en multi-hop routes\n",
            "duration": "45 min"
          },
          "swap-v2-slippage": {
            "title": "Deslizamiento y impacto de precio: protecting swap outcomes",
            "content": "# Deslizamiento y impacto de precio: protecting swap outcomes\n\nDeslizamiento is el difference between el expected output amount at quote time y el actual amount received at execution time. En volatile markets con active trading, pool reserves change between when you request a quote y when your transaccion lands en-chain. Deslizamiento protection ensures you never receive less than an acceptable minimum.\n\nImpacto de precio measures como much your swap moves el pool's price. A small swap en a deep liquidez pool has near-zero impacto de precio. A large swap en a shallow pool can move el price significantly — you are effectively trading against yourself as each unit you buy makes el next unit more expensive. Impacto de precio is calculated at quote time y should be displayed a users before they confirm.\n\nEl deslizamiento tolerance is expressed en basis points (bps). 1 bps = 0.01%. A deslizamiento de 50 bps means you accept up a 0.5% less than el quoted output. El minimum output amount is calculated as: minOutAmount = outAmount - (outAmount × deslizamientoBps / 10000). Este calculation MUST usa integer arithmetic a avoid floating-point rounding errores. Usando BigInt en JavaScript ensures exact computation.\n\nSetting deslizamiento too tight (e.g., 1 bps) causes frequent transaccion failures because even minor pool changes exceed el tolerance. Setting it too loose (e.g., 1000 bps = 10%) exposes users a sandwich attacks where a malicious actor front-runs el swap a move el price, then back-runs after execution a profit desde el price movement. El optimal range para most swaps is 30–100 bps, con higher values para volatile o low-liquidez pairs.\n\nSandwich attacks exploit predictable deslizamiento tolerances. An attacker observes your pending transaccion en el mempool, submits a transaccion a buy el output token (raising its price), lets your swap execute at el worse price, then sells el output token at profit. Tight deslizamiento limits reduce el attacker's profit margin y may cause them a skip your transaccion entirely.\n\nDynamic deslizamiento adjusts el tolerance based en: recent volatility, pool depth, swap size relative a pool reserves, y historical transaccion success rates. Avanzado aggregators compute recommended deslizamiento per-route a balance execution confiabilidad con protection. When construiring swap UIs, always scomo both el quoted output y el minimum guaranteed output so users understand their worst-case outcome.\n\n## Checklist\n- Calculate minOutAmount usando integer arithmetic (BigInt)\n- Display both expected y minimum output amounts a users\n- Usa 30–100 bps as default deslizamiento para most pairs\n- Scomo impacto de precio percentage prominently para large swaps\n- Consider dynamic deslizamiento based en pool conditions\n\n## Red flags\n- Usando floating-point math para deslizamiento calculations\n- Setting extremely loose deslizamiento (>500 bps) sin user advertencia\n- Not displaying impacto de precio para large swaps\n- Ignoring sandwich attack vectors en deslizamiento diseno\n",
            "duration": "50 min"
          },
          "swap-v2-route-explorer": {
            "title": "Route visualization: comprension swap legs y comisiones",
            "content": "# Route visualization: comprension swap legs y comisiones\n\nSwap routes reveal el path your tokens take through DeFi liquidez. Visualizing routes helps users understand por que a multi-hop path might yield more output than a direct swap, y where comisiones are deducted along el way.\n\nA route consists de one o more legs. Each leg represents a swap through a specific AMM pool. El leg includes: el AMM program label (e.g., \"Whirlpool,\" \"Raydium\"), el input y output mints para ese leg, el comision charged por el pool (denominated en el output token), y el percentage de el total input allocated a este leg.\n\nSplit routes divide el input amount across multiple paths. Para example, 60% might go through Raydium SOL/USDC y 40% through Orca SOL/USDC. Splitting across pools reduces impacto de precio because each pool handles a smaller portion de el total swap. El aggregator optimizes el split percentages a maximize total output.\n\nComision cuentaing is per-leg. Each AMM charges a comision (typically 0.01%–1% depending en el pool's comision tier). En concentrated liquidez pools, comision tiers are explicit (e.g., Orca's 1 bps, 4 bps, 30 bps, 100 bps tiers). El total comision across all legs determines el true cost de el route. A route con lower per-leg comisiones might still be more expensive if it requires more hops.\n\nWhen rendering route information, scomo: el overall path (input mint → [intermedio mints] → output mint), per-leg details (AMM, comision, percentage), total comisiones en el output token denomination, impacto de precio as a percentage, y el final output amounts (quoted y minimum). Color-coding o progress indicators help users quickly understand whether a route is simple (green, single hop) o complex (yellow/orange, multi-hop).\n\nEffective price is calculated as: outputAmount / inputAmount, denominated en output-per-input units. Para SOL → USDC, este gives el effective USD price de SOL para este specific swap. Comparing el effective price against oracle o market price reveals el total cost de el swap including all comisiones y impacto de precio. Este \"execution cost\" metric is el most honest summary de swap calidad.\n\nRoute caching y expiration matter para UX. Quotes desde aggregators have a limited validity window (typically 10–30 seconds). If el user takes too long a confirm, el quote expires y el route must be re-fetched. El UI should clearly indicate quote freshness y automatically re-quote when expired. Stale quotes ese execute against current pool estado will likely fail o produce worse outcomes.\n\n## Checklist\n- Scomo each leg con AMM label, mints, comision, y split percentage\n- Display total comisiones summed across all legs\n- Calculate y display effective price (output/input)\n- Indicate quote expiration time a users\n- Color-code routes por complexity (hops count)\n\n## Red flags\n- Hiding comisiones desde el user display\n- Not scomoing impacto de precio para large swaps\n- Allowing execution de expired quotes\n- Displaying only el best-case output sin minimum\n",
            "duration": "45 min"
          },
          "swap-v2-swap-plan": {
            "title": "Desafio: Construir a normalized SwapPlan desde a quote",
            "content": "# Desafio: Construir a normalized SwapPlan desde a quote\n\nParse a raw aggregator quote response y produce a normalized SwapPlan:\n\n- Extract input/output mints y amounts desde el quote\n- Calculate minOutAmount usando BigInt deslizamiento arithmetic\n- Map each route leg a a normalized structure con AMM label, mints, comisiones, y percentage\n- Handle zero deslizamiento correctly (minOut equals outAmount)\n- Ensure all amounts remain as string representations de integers\n\nYour SwapPlan must be fully deterministic — same input always produces same output.",
            "duration": "50 min"
          }
        }
      },
      "swap-v2-execution": {
        "title": "Execution & Confiabilidad",
        "description": "Estado-machine execution, transaccion anatomy, retry/staleness confiabilidad patrones, y high-signal swap run reporteing.",
        "lessons": {
          "swap-v2-state-machine": {
            "title": "Desafio: Implement swap UI estado machine",
            "content": "# Desafio: Implement swap UI estado machine\n\nConstruir a deterministic estado machine para el swap UI flujo:\n\n- Estados: idle → quoting → ready → sending → confirming → success | error\n- Process a sequence de events y track all estado transitions\n- Invalid transitions should move a el error estado con a descriptive message\n- El error estado supports RESET (back a idle) y RETRY (back a quoting)\n- Track transition history as an array de {desde, event, a} objects\n\nEl estado machine must be fully deterministic — same event sequence always produces same result.",
            "duration": "45 min"
          },
          "swap-v2-tx-anatomy": {
            "title": "Swap transaccion anatomy: instruccions, cuentas, y compute",
            "content": "# Swap transaccion anatomy: instruccions, cuentas, y compute\n\nA swap transaccion en Solana is a carefully ordered sequence de instruccions ese together achieve an atomic token exchange. Comprension each instruccion's role, el cuenta list requirements, y compute budget considerations is essential para construiring confiable swap flujos.\n\nEl typical swap transaccion contains estos instruccions en order: (1) Compute Budget: SetComputeUnitLimit y SetComputeUnitPrice a ensure el transaccion has enough compute y appropriate priority. (2) Create ATA (if needed): createAssociatedTokenCuentaIdempotent para el output token if el user doesn't already have one. (3) Wrap SOL (if input is native SOL): transfer SOL a a temporary WSOL cuenta y sync its balance. (4) Swap instruccion(s): el actual AMM program calls ese execute el swap, referencing all required pool cuentas. (5) Unwrap WSOL (if output is native SOL): close el temporary WSOL cuenta y recover SOL.\n\nCuenta requirements scale con route complexity. A single-hop swap through Whirlpool requires approximately 12–15 cuentas (user cartera, token cuentas, pool estado, oracle, tick arrays, etc.). A multi-hop route through two different AMMs can require 25+ cuentas, pushing against el transaccion size limit. Address Lookup Tables (ALTs) mitigate este por compressing cuenta references desde 32 bytes a 1 byte each, but require a separate setup transaccion.\n\nCompute budget estimation is critical. A simple SOL → USDC Whirlpool swap uses roughly 80,000–120,000 compute units. Multi-hop routes can usa 200,000–400,000+. Setting el compute limit too low causes el transaccion a fail. Setting it too high wastes el user's priority comision budget (priority comision = CU price × CU limit). Aggregators typically provide a recommended compute unit limit per route.\n\nPriority comisiones determine transaccion ordering. During high-demand periods (popular mints, volatile markets), transaccions compete para block space. El priority comision (en micro-lamports per compute unit) determines where your transaccion lands en el leader's queue. Too low y el transaccion may not be included; too high y el user overpays. Dynamic priority comision estimation uses recent block datos a suggest competitive rates.\n\nTransaccion simulation before submission catches many errores: insufficient balance, missing cuentas, compute budget exceeded, deslizamiento exceeded. Simulating saves el user desde paying transaccion comisiones en doomed transaccions. El simulation result includes compute units consumed, log messages, y any error codes — all useful para depuracion.\n\nVersioned transaccions (v0) are required when usando Address Lookup Tables. Legacy transaccions cannot reference ALTs. Most modern swap aggregators return versioned transaccion messages. Carteras must support versioned transaccion signing (most do, but some older cartera adapters may not).\n\n## Checklist\n- Include compute budget instruccions at el start de el transaccion\n- Create output ATA if it doesn't exist\n- Handle SOL wrapping/unwrapping para native SOL swaps\n- Simulate transaccions before submission\n- Usa versioned transaccions when ALTs are needed\n\n## Red flags\n- Omitting compute budget instruccions (uses default 200k limit)\n- Not creating output ATA before el swap instruccion\n- Forgetting a unwrap WSOL after receiving native SOL output\n- Skipping simulation y sending potentially invalid transaccions\n",
            "duration": "50 min"
          },
          "swap-v2-reliability": {
            "title": "Confiabilidad patrones: retries, stale quotes, y latencia",
            "content": "# Confiabilidad patrones: retries, stale quotes, y latencia\n\nProduccion swap flujos must handle el reality de network latencia, expired quotes, y transaccion failures. Confiabilidad engineering separates toy swap implementations desde produccion-grade sistemas ese users trust con real money.\n\nQuote staleness is el primary confiabilidad desafio. An aggregator quote reflects pool estado at a specific moment. Por el time el user reviews el quote, signs el transaccion, y it lands en-chain, pool reserves may have changed significantly. A quote older than 10–15 seconds should be considered potentially stale. El UI should scomo a countdown timer y automatically re-quote when el timer expires. Never allow users a send transaccions based en quotes older than 30 seconds.\n\nRetry strategies must distinguish between retryable y non-retryable failures. Retryable: network timeout, RPC node temporarily unavailable, blockhash expired (re-fetch y re-sign), y rate limiting (429 responses, back off exponentially). Non-retryable: insufficient balance, invalid cuenta estado, deslizamiento exceeded (pool price moved too far, re-quote required), y program errores indicating invalid instruccion datos.\n\nExponential backoff con jitter prevents retry storms. Base delay de 500ms, multiplied por 2 en each retry, con random jitter de ±25% a prevent synchronized retries desde multiple clients. Cap retries at 3–5 attempts. If all retries fail, present a clear error message con actionable options: \"Quote expired — refresh y try again\" rather than generic \"Transaccion failed.\"\n\nBlockhash management affects confiabilidad. A transaccion's blockhash must be recent (within ~60 seconds / 150 slots). If a transaccion fails y you retry, el blockhash may have expired. El retry flujo must: get a fresh blockhash, reconstruir el transaccion con el new blockhash, re-sign, y re-submit. Este is por que swap transaccion construiring should be a reusable function ese accepts a blockhash parameter.\n\nLatencia budgets help set user expectations. Typical Solana transaccion confirmation takes 400ms–2 seconds. Comoever, during congestion, confirmation can take 5–30 seconds o fail entirely. El UI should scomo progressive estados: \"Submitting...\" → \"Confirming...\" con block confirmations. After 30 seconds sin confirmation, offer el user a choice: wait longer, retry, o cancel (note: you cannot cancel a submitted transaccion, but you can stop polling y let el blockhash expire).\n\nTransaccion status polling should usa WebSocket subscriptions (signatureSubscribe) para real-time confirmation rather than polling getTransaccion. Polling creates unnecessary RPC load y introduces latencia. Subscribe immediately after sendTransaccion returns a signature, y set a timeout para maximum wait time.\n\n## Checklist\n- Scomo quote freshness countdown y auto-refresh\n- Classify errores as retryable vs non-retryable\n- Usa exponential backoff con jitter para retries\n- Get fresh blockhash en each retry attempt\n- Usa WebSocket subscriptions para confirmation\n\n## Red flags\n- Retrying non-retryable errores (wastes time y comisiones)\n- No retry limit (infinite retry loops)\n- Sending transaccions con stale quotes (>30 seconds)\n- Polling getTransaccion instead de subscribing\n",
            "duration": "45 min"
          },
          "swap-v2-swap-report": {
            "title": "Checkpoint: Generate a SwapRunReporte",
            "content": "# Checkpoint: Generate a SwapRunReporte\n\nConstruir el final swap run reporte ese combines all curso concepts:\n\n- Summarize el route con leg details y total comisiones (usando BigInt summation)\n- Compute el effective price as outAmount / inAmount (9 decimal precision)\n- Include el estado machine outcome (finalEstado desde el UI flujo)\n- Collect all errores desde el estado result y additional error sources\n- Output must be stable JSON con deterministic key ordering\n\nEste checkpoint validates your complete comprension de swap aggregation.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "defi-clmm-liquidity": {
    "title": "CLMM Liquidez Engineering",
    "description": "Master concentrated liquidez engineering en Solana DEXs: tick math, range strategy diseno, comision/IL dynamics, y deterministic LP position reporteing.",
    "duration": "14 hours",
    "tags": [
      "defi",
      "clmm",
      "liquidity",
      "orca",
      "solana"
    ],
    "modules": {
      "clmm-v2-fundamentals": {
        "title": "CLMM Fundamentals",
        "description": "Concentrated liquidez concepts, tick/price math, y range-position behavior needed a reason about CLMM execution.",
        "lessons": {
          "clmm-v2-vs-cpmm": {
            "title": "CLMM vs constant product: por que ticks exist",
            "content": "# CLMM vs constant product: por que ticks exist\n\nConcentrated Liquidez Market Makers (CLMMs) represent a fundamental evolution en automated market maker diseno. A understand por que they exist, we must first understand el limitations de el constant product modelo y then examine como tick-based sistemas solve esos problems en Solana.\n\n## El constant product modelo y its inefficiency\n\nEl original AMM diseno, popularized por Uniswap V2 y adopted por Raydium V1 en Solana, uses el constant product invariant: x * y = k, where x y y are el reserves de two tokens y k is a constant. When a trader swaps token A para token B, el product must remain unchanged. Este creates a smooth price curve ese spans el entire range desde zero a infinity.\n\nEl problem con este approach is capital inefficiency. If a SOL/USDC pool holds $10 million en liquidez, y SOL trades between $20 y $30 para months, el vast majority de ese capital sits idle. Liquidez allocated a price ranges below $1 o above $1000 never participates en trades, earns no comisiones, yet still dilutes el returns para liquidez providers (LPs). En practice, studies scomo ese less than 5% de liquidez en constant product pools is actively usado at any given time.\n\n## Concentrated liquidez: el core insight\n\nCLMMs, pioneered por Uniswap V3 y implemented en Solana por Orca Whirlpools, Raydium Concentrated Liquidez, y Meteora DLMM, allow LPs a allocate capital a specific price ranges. Instead de spreading liquidez across all possible prices, an LP can say: \"I want a provide liquidez only between $20 y $30 para SOL/USDC.\" Este concentrates their capital where trades actually happen, dramatically increasing capital efficiency.\n\nEl capital efficiency gain is substantial. An LP providing concentrated liquidez en a narrow range can achieve el same depth as a constant product LP con 100x o even 4000x less capital, depending en como tight el range is. Este means more comisiones earned per dollar deployed, which is el fundamental value proposition de CLMMs.\n\n## Por que ticks exist\n\nA implement concentrated liquidez, el price space must be discretized. CLMMs divide el continuous price curve into discrete points called ticks. Each tick represents a specific price, y el relationship between tick index y price follows el formula: price = 1.0001^tick. Este means each tick represents a 0.01% (1 basis point) change en price desde el adjacent tick.\n\nTicks serve several critical purposes. First, they provide el boundaries para liquidez positions. When an LP creates a position desde tick -1000 a tick 1000, they are defining a price range. Second, ticks are where liquidez transitions happen. As el price crosses a tick boundary, el active liquidez changes because positions ese start o end at ese tick become active o inactive. Third, ticks enable efficient comision tracking, because el protocol only needs a track comision growth at tick boundaries rather than at every possible price.\n\nTick spacing is an important optimizacion. Not every tick is usable en every pool. Pools con higher comision tiers usa wider tick spacing (e.g., 64 o 128 ticks apart) a reduce gas costs y estado size. A pool con tick spacing de 64 means LPs can only place position boundaries at tick indices ese are multiples de 64. Este tradeoff reduces granularity but improves en-chain efficiency, which is especially important en Solana where cuenta sizes y compute units matter.\n\n## Solana-specific CLMM considerations\n\nEn Solana, CLMMs face unique architectural desafios. El modelo de cuentas requires pre-allocated tick arrays ese store tick datos en contiguous ranges. Orca Whirlpools, para example, uses tick array cuentas ese each hold 88 ticks worth de datos. El program must load el correct tick array cuentas as instruccions, which means swaps ese cross many ticks require more cuentas y more compute units.\n\nEl Solana runtime's 1232-byte transaccion size limit y 200,000 compute unit default also constrain CLMM operations. Large swaps ese cross multiple tick boundaries may need a be split across multiple transaccions, y position management operations must be carefully optimized a fit within estos constraints.\n\n## LP decision framework\n\nBefore opening any CLMM position, answer three questions:\n1. Que price regime am I targeting (mean-reverting vs trending)?\n2. Como actively can I rebalance when out-de-range?\n3. Que failure budget can I tolerate para comisiones vs IL vs rebalance costs?\n\nCLMM returns come desde strategy discipline, not just math formulas.\n\n## Checklist\n- Understand ese x*y=k spreads liquidez across all prices, wasting capital\n- CLMMs let LPs concentrate capital en specific price ranges\n- Ticks discretize el price space at 1 basis point intervals\n- Tick spacing varies por pool comision tier para en-chain efficiency\n- Solana CLMMs usa tick array cuentas para estado management\n\n## Red flags\n- Assuming CLMM positions behave like constant product positions\n- Ignoring tick spacing when placing position boundaries\n- Underestimating compute costs para swaps crossing many ticks\n- Forgetting ese out-de-range positions earn zero comisiones\n",
            "duration": "50 min"
          },
          "clmm-v2-price-tick": {
            "title": "Price, tick, y sqrtPrice: core conversions",
            "content": "# Price, tick, y sqrtPrice: core conversions\n\nEl mathematical foundation de every CLMM rests en three interrelated representations de price: el human-readable price, el tick index, y el sqrtPriceX64. Comprension como a convert between estos representations is essential para construiring any CLMM integration en Solana.\n\n## Tick a price conversion\n\nEl fundamental relationship between a tick index y price is: price = 1.0001^tick. Este formula means ese each consecutive tick represents a 0.01% (1 basis point) change en price. Tick 0 corresponds a a price de 1.0. Positive ticks yield prices greater than 1, y negative ticks yield prices less than 1.\n\nPara example, tick 23027 gives a price de approximately 10.0 (since 1.0001^23027 is roughly 10). Tick -23027 gives approximately 0.1. Este logarithmic spacing means ticks provide consistent relative precision across all price levels. Whether el price is 0.001 o 1000, adjacent ticks always differ por 0.01%.\n\nEl inverse conversion desde price a tick uses el natural logarithm: tick = ln(price) / ln(1.0001). Since tick indices must be integers, este value is typically rounded a el nearest integer. En practice, you also need a align el tick a el pool's tick spacing, which means rounding down a el nearest multiple de el tick spacing value.\n\n## El sqrtPrice representation\n\nCLMMs do not store price directly en-chain. Instead, they store el square root de el price en a fixed-point format called sqrtPriceX64. Este representation has two important advantages.\n\nFirst, usando el square root simplifies el core AMM math. El amount de token0 en a position is proportional a (1/sqrtPrice_lower - 1/sqrtPrice_upper), y el amount de token1 is proportional a (sqrtPrice_upper - sqrtPrice_lower). Estos linear relationships are much easier a compute en-chain than el original price-based formulas would be.\n\nSecond, el X64 fixed-point format (also called Q64.64) provides high precision sin floating-point arithmetic. El sqrtPrice is multiplied por 2^64 y stored as a 128-bit unsigned integer. Este means sqrtPriceX64 = sqrt(price) * 2^64. Para tick 0 (price = 1.0), el sqrtPriceX64 is exactly 2^64 = 18446744073709551616.\n\nEn Solana, Orca Whirlpools stores este value as a u128 en el Whirlpool cuenta estado. Every swap operation updates este value as el price moves. El sqrt_price field is el canonical source de truth para el current pool price.\n\n## Decimal handling y token precision\n\nReal-world tokens have different decimal places. SOL has 9 decimals, USDC has 6 decimals. El tick-a-price formula gives a \"raw\" price ese must be adjusted para decimals. If token0 is SOL (9 decimals) y token1 is USDC (6 decimals), el human-readable price is: display_price = raw_price * 10^(decimals0 - decimals1) = raw_price * 10^(9-6) = raw_price * 1000.\n\nEste decimal adjustment is critical y a common source de bugs. Always track which token is token0 y which is token1 en el pool, y apply el correct decimal scaling when converting between en-chain tick values y display prices.\n\n## Tick spacing y alignment\n\nNot every tick index is a valid position boundary. Each pool has a tick_spacing parameter ese determines which ticks can be usado. Common values are: 1 (para stable pairs con 0.01% comision), 8 (para 0.04% comision pools), 64 (para 0.30% comision pools), y 128 (para 1.00% comision pools).\n\nA align a tick a el pool's tick spacing, usa: aligned_tick = floor(tick / tick_spacing) * tick_spacing. Este always rounds toward negative infinity, ensuring consistent behavior para both positive y negative ticks. Para example, con tick spacing 64: tick 100 aligns a 64, tick -100 aligns a -128.\n\n## Precision considerations\n\nFloating-point arithmetic introduces rounding errores en tick/price conversions. When converting price a tick y back, el result may differ por 1 tick due a floating-point precision limits. Para en-chain operations, always usa el integer tick index as el source de truth y derive el price desde it, never el reverse.\n\nEl sqrtPriceX64 computation usando BigInt avoids floating-point issues para el final representation, but el intermedio sqrt y pow operations still usa JavaScript's 64-bit floats. Para produccion sistemas processing large values, consider usando dedicated decimal libraries o performing estos computations con higher-precision arithmetic.\n\n## Checklist\n- price = 1.0001^tick para tick-a-price conversion\n- tick = round(ln(price) / ln(1.0001)) para price-a-tick conversion\n- sqrtPriceX64 = BigInt(round(sqrt(price) * 2^64))\n- Align ticks a tick spacing: floor(tick / spacing) * spacing\n- Adjust para token decimals when displaying human-readable prices\n\n## Red flags\n- Ignoring decimal differences between token0 y token1\n- Usando floating-point price as source de truth instead de tick index\n- Forgetting tick spacing alignment when creating positions\n- Overflow en sqrtPriceX64 computation para extreme tick values\n",
            "duration": "50 min"
          },
          "clmm-v2-range-explorer": {
            "title": "Range positions: en-range y out-de-range dynamics",
            "content": "# Range positions: en-range y out-de-range dynamics\n\nA CLMM position is defined por its lower tick y upper tick. Estos two boundaries determine el price range en which el position is active, earns comisiones, y holds a mix de both tokens. Comprension en-range y out-de-range behavior is fundamental a managing concentrated liquidez effectively en Solana.\n\n## Anatomy de a range position\n\nWhen an LP creates a position en Orca Whirlpools (o any Solana CLMM), they specify three parameters: el lower tick index, el upper tick index, y el amount de liquidez a provide. El protocol then calculates como much de each token el LP must deposit based en el current price relative a el position's range.\n\nIf el current price is within el range (lower_tick <= current_tick <= upper_tick), el LP deposits both tokens. El ratio depends en where el current price sits within el range. If el price is near el lower bound, el position holds mostly token0. If near el upper bound, it holds mostly token1. Este is el direct analog de como a constant product pool holds different ratios at different prices, but concentrated into el LP's chosen range.\n\n## En-range behavior\n\nWhen el current pool price is within a position's range, el position is en-range y actively participates en swaps. Every swap ese moves el price within este range uses el position's liquidez y generates comisiones para el LP.\n\nEl comision accrual mechanism works as follows: el pool tracks a global comision_growth value para each token. When a swap occurs, el comision (e.g., 0.30% de el swap amount) is distributed proportionally across all en-range liquidez. Each position tracks its own comision_growth snapshot, y uncollected comisiones are el difference between el current global growth y el position's snapshot, multiplied por el position's liquidez.\n\nEn-range positions experience impermanent loss (IL) as el price moves. When el price moves up, el position converts token0 into token1 (selling token0 at higher prices). When el price moves down, it converts token1 into token0. Este rebalancing is el source de IL, y it is more pronounced en CLMMs than en constant product pools because el liquidez is concentrated en a narrower range.\n\n## Out-de-range behavior\n\nWhen el price moves outside a position's range, el position becomes out-de-range. Este has critical implications. El position stops earning comisiones entirely because it no longer participates en swaps. El position holds 100% de one token: if el price moved above el upper tick, el position holds entirely token1 (all token0 was sold as el price rose). If el price moved below el lower tick, el position holds entirely token0 (all token1 was sold as el price fell).\n\nAn out-de-range position is effectively a limit order ese has been filled. If you set a range above el current price y el price rises through it, your token0 is converted a token1 at prices within your range. Este property makes CLMMs useful para implementing range orders y dollar-cost averaging strategies.\n\nOut-de-range positions still exist en-chain y can be closed o modified at any time. El LP can withdraw their single-sided holdings, o they can wait para el price a return a their range. If el price returns, el position automatically becomes active again y starts earning comisiones.\n\n## Position composition at boundaries\n\nAt el exact lower tick, el position holds 100% token0 y 0% token1. At el exact upper tick, it holds 0% token0 y 100% token1. At any price between, el composition is a function de where el current sqrtPrice sits relative a el range boundaries.\n\nEl token amounts are calculated as: amount0 = liquidez * (1/sqrtPrice_current - 1/sqrtPrice_upper) y amount1 = liquidez * (sqrtPrice_current - sqrtPrice_lower). Estos formulas only apply when el price is en-range. When out-de-range below, amount0 = liquidez * (1/sqrtPrice_lower - 1/sqrtPrice_upper) y amount1 = 0. When out-de-range above, amount0 = 0 y amount1 = liquidez * (sqrtPrice_upper - sqrtPrice_lower).\n\n## Active liquidez y el liquidez curve\n\nEl pool's active liquidez at any given price is el sum de all en-range positions at ese price. Este creates a liquidez distribution curve ese can have complex shapes depending en where LPs have placed their positions. Deeper liquidez at el current price means less deslizamiento para traders.\n\nEn Solana, este active liquidez is stored en el Whirlpool cuenta's liquidez field y is updated whenever el price crosses a tick boundary where positions start o end. El tick array cuentas store el net liquidez change at each initialized tick, allowing el program a efficiently update active liquidez during swaps.\n\n## Checklist\n- En-range positions earn comisiones y hold both tokens\n- Out-de-range positions earn zero comisiones y hold one token\n- Token composition varies continuously within el range\n- Active liquidez is el sum de all en-range positions\n- Comision growth tracking uses global vs position-level snapshots\n\n## Red flags\n- Expecting comisiones desde out-de-range positions\n- Ignoring el single-sided nature de out-de-range holdings\n- Forgetting a cuenta para IL en concentrated positions\n- Assuming position composition is static within a range\n",
            "duration": "45 min"
          },
          "clmm-v2-tick-math": {
            "title": "Desafio: Implement tick/price conversion helpers",
            "content": "# Desafio: Implement tick/price conversion helpers\n\nImplement el core tick math functions usado en every CLMM integration:\n\n- Convert a tick index a a human-readable price usando price = 1.0001^tick\n- Convert el price a sqrtPriceX64 usando Q64.64 fixed-point encoding\n- Reverse-convert a price back a el nearest tick index\n- Align a tick index a el pool's tick spacing\n\nYour implementation will be tested against known tick values including tick 0, positive ticks, y negative ticks.",
            "duration": "50 min"
          }
        }
      },
      "clmm-v2-positions": {
        "title": "Positions & Riesgo",
        "description": "Comision accrual simulation, range strategy tradeoffs, precision pitfalls, y deterministic position riesgo reporteing.",
        "lessons": {
          "clmm-v2-position-fees": {
            "title": "Desafio: Simulate position comision accrual",
            "content": "# Desafio: Simulate position comision accrual\n\nImplement a comision accrual simulator para a CLMM position over a price path:\n\n- Convert lower y upper tick boundaries a prices\n- Walk through each price en el path y determine en-range o out-de-range status\n- Accrue comisiones proportional a trade volume when en-range\n- Compute annualized comision APR\n- Track periods en-range vs out-de-range\n- Determine current status desde el final price\n\nEste simulates el real-world behavior de concentrated liquidez positions as prices move.",
            "duration": "50 min"
          },
          "clmm-v2-range-strategy": {
            "title": "Range strategies: tight, wide, y rebalancing rules",
            "content": "# Range strategies: tight, wide, y rebalancing rules\n\nChoosing el right price range is el most important decision a CLMM liquidez provider makes. El range determines capital efficiency, comision income, impermanent loss exposure, y rebalancing frequency. Este leccion covers el major strategies y el tradeoffs between them.\n\n## Tight ranges: maximum efficiency, maximum riesgo\n\nA tight range concentrates all liquidez into a narrow price band. Para example, providing liquidez para SOL/USDC within +/- 2% de el current price. El advantages are significant: capital efficiency can be 100x o more compared a a full-range position, y el LP earns a proportionally larger share de trading comisiones.\n\nComoever, tight ranges carry substantial riesgos. El position goes out-de-range frequently, requiring active monitoring y rebalancing. Each time el position goes out-de-range, el LP has fully converted a one token y stops earning comisiones. El LP also realizes impermanent loss en each range crossing, y el gas costs de frequent rebalancing can eat into profits.\n\nTight ranges work best para stable pairs (USDC/USDT) where el price rarely deviates significantly, para professional LPs who can automate rebalancing, y para short-term positions where el LP has a strong directional view.\n\n## Wide ranges: passive y resilient\n\nA wide range covers a larger price band, such as +/- 50% o even el full price range. Capital efficiency is lower, but el position stays en-range longer y requires less active management. Comision income per dollar is lower, but el position earns comisiones more consistently.\n\nWide ranges suit passive LPs who cannot actively monitor positions, volatile pairs where el price can swing dramatically, y LPs who want a minimize rebalancing costs y IL realization events.\n\nEl extreme case is a full-range position covering all ticks. Este replicates constant product AMM behavior y never goes out-de-range. While capital-inefficient, it provides maximum resilience y is appropriate para very volatile o low-liquidez pairs.\n\n## Asymmetric ranges y directional bets\n\nLPs can create asymmetric ranges ese express a directional view. If you believe SOL will appreciate against USDC, you might set a range desde el current price up a 2x el current price. Este means you are providing liquidez as SOL appreciates, selling SOL at progressively higher prices. If SOL drops, your position immediately goes out-de-range y you hold SOL, preserving your long exposure.\n\nConversely, a range set below el current price acts like a limit buy order. You deposit USDC, y if SOL's price drops into your range, your USDC is converted a SOL at your desired prices.\n\n## Rebalancing strategies\n\nRebalancing is el process de closing an out-de-range position y opening a new one centered en el current price. El key decisions are: when a rebalance, y como a set el new range.\n\nTime-based rebalancing checks el position at fixed intervals (hourly, daily) y rebalances if out-de-range. Este is simple a implement but may miss optimal timing. Price-based rebalancing uses el current price relative a el range boundaries. A common trigger is rebalancing when el price exits el inner 50% de el range, before it actually goes out-de-range.\n\nThreshold-based rebalancing waits until el IL o missed-comision cost de remaining out-de-range exceeds el cost de rebalancing (gas comisiones, deslizamiento en swaps needed a rebalance token composition). Este is el most capital-efficient approach but requires sophisticated modeling.\n\nEn Solana, rebalancing a Whirlpool position involves three operations: collecting unclaimed comisiones, closing el old position (withdrawing liquidez y burning el position NFT), y opening a new position con updated range. Estos operations typically fit en two a three transaccions depending en el number de cuentas involved.\n\n## Automated vault strategies\n\nSeveral protocols en Solana automate CLMM range management. Estos vault protocols (such as Kamino Finance) accept LP deposits y automatically create, monitor, y rebalance concentrated liquidez positions. They usa various strategies including mean-reversion, momentum-following, y volatility-adjusted range widths.\n\nWhen evaluating automated vaults, consider: el strategy's historical rendimiento, el management y rendimiento comisiones, el rebalancing frequency y associated costs, y el vault's transparency about its position management logic.\n\n## Checklist\n- Tight ranges maximize efficiency but require active management\n- Wide ranges provide resilience at el cost de efficiency\n- Asymmetric ranges can express directional views\n- Rebalancing triggers: time-based, price-based, o threshold-based\n- Consider automated vaults para passive management\n\n## Red flags\n- Usando tight ranges sin monitoring o automation\n- Rebalancing too frequently, losing comisiones a gas costs\n- Ignoring el realized IL at each rebalancing event\n- Assuming past APR will predict future returns\n",
            "duration": "50 min"
          },
          "clmm-v2-risk-review": {
            "title": "CLMM riesgos: rounding, overflow, y tick spacing errores",
            "content": "# CLMM riesgos: rounding, overflow, y tick spacing errores\n\nConstruiring confiable CLMM integrations requires awareness de precision riesgos ese can cause incorrect calculations, failed transaccions, o lost funds. Este leccion catalogs el most common pitfalls en tick math, comision computation, y position management en Solana.\n\n## Floating-point rounding en tick conversions\n\nEl tick-a-price formula price = 1.0001^tick y its inverse tick = ln(price) / ln(1.0001) both involve floating-point arithmetic. JavaScript's Number type uses IEEE 754 double-precision (64-bit) floats, which provide approximately 15-17 significant decimal digits. Para most tick ranges (roughly -443636 a +443636, el valid CLMM range), este precision is sufficient.\n\nComoever, rounding errores accumulate en compound operations. Converting a tick a a price y back may yield tick +/- 1 due a floating-point rounding en el logarithm. El safest practice is a always treat el integer tick index as el canonical value. If you need a price, derive it desde el tick. If you need a tick desde a user-entered price, compute el tick y then scomo el user el exact price ese tick represents, so they see el actual boundary rather than an approximation.\n\nEl Math.round function en el priceToTick conversion introduces its own edge cases. When el true tick is exactly X.5, Math.round uses \"round half a even\" (banker's rounding) en some environments. Para CLMM math, always round toward el nearest valid tick y then align a tick spacing.\n\n## Overflow en sqrtPriceX64 computation\n\nEl sqrtPriceX64 value is stored as a u128 en-chain (128-bit unsigned integer). En JavaScript, este must be handled con BigInt. El intermedio computation sqrt(price) * 2^64 can overflow a 64-bit float para extreme tick values. At el maximum valid tick (443636), el price is approximately 1.34 * 10^19, y sqrt(price) * 2^64 is approximately 6.75 * 10^28, which fits en a u128 but exceeds el safe integer range de JavaScript Numbers.\n\nAlways usa BigInt para el final sqrtPriceX64 value. Para intermedio computations at extreme ticks, consider usando a high-precision library o performing el computation en Rust (where Solana programs actually run). Para client-side JavaScript, el practico riesgo is manageable para common token pairs but must be tested at boundary conditions.\n\n## Tick spacing alignment errores\n\nA frequent bug is creating positions con tick boundaries ese are not aligned a el pool's tick spacing. El en-chain program will reject estos positions, but el error message may be cryptic. Always align ticks before submitting transaccions: aligned = floor(tick / tickSpacing) * tickSpacing.\n\nBe careful con negative ticks: floor(-100 / 64) = floor(-1.5625) = -2, so -100 aligns a -128, not -64. Este is correct behavior (rounding toward negative infinity), but developers who expect truncation toward zero will get wrong results. Test con negative ticks explicitly.\n\n## Comision computation precision\n\nComision growth values en CLMMs usa 128-bit fixed-point arithmetic (Q64.64 o Q128.128 depending en el implementation). When computing uncollected comisiones, el formula is: uncollected_comisiones = (global_comision_growth - position_comision_growth_snapshot) * liquidez.\n\nBoth el subtraction y el multiplication can overflow if not handled carefully. En Solana, el program uses checked arithmetic y wrapping subtraction (since comision_growth is monotonically increasing y wraps around). Client-side code must replicate este wrapping behavior when reading en-chain estado.\n\nA common mistake is computing comisiones con JavaScript Numbers, which lose precision para large BigInt values. Always usa BigInt para comision calculations y only convert a Number at el final display step, after applying decimal adjustments.\n\n## Decimal mismatch between tokens\n\nDifferent tokens have different decimal places (SOL: 9, USDC: 6, BONK: 5). When computing position values, token amounts, o comision amounts, el decimal places must be consistently applied. A common bug is computing IL en raw amounts sin normalizing a el same decimal base, leading a wildly incorrect results.\n\nAlways track el decimal places de both tokens en el pool y apply them when converting between raw amounts y display amounts. El en-chain CLMM program operates entirely en raw (lamport-level) amounts; all decimal formatting is a client-side responsibility.\n\n## Cuenta y compute unit limits\n\nSolana-specific riesgos include exceeding compute unit limits during swaps ese cross many ticks, requiring too many tick array cuentas (each swap can reference at most a few tick arrays), y cuenta size limits para position management.\n\nWhen construiring swap transaccions, estimate el number de tick crossings y include sufficient tick array cuentas. If a swap would cross more ticks than can be accommodated, el transaccion will fail. Splitting large swaps across multiple transaccions o usando a routing protocol helps mitigate este riesgo.\n\n## Checklist\n- Usa integer tick index as canonical, derive price desde it\n- Usa BigInt para sqrtPriceX64 y all comision computations\n- Always align ticks a tick spacing con floor division\n- Test con negative ticks, zero ticks, y extreme ticks\n- Apply correct decimal places para each token en el pool\n\n## Red flags\n- Usando JavaScript Number para sqrtPriceX64 o comision amounts\n- Forgetting wrapping subtraction para comision growth deltas\n- Truncating instead de flooring para negative tick alignment\n- Computing IL o comisiones sin matching token decimals\n",
            "duration": "45 min"
          },
          "clmm-v2-position-report": {
            "title": "Checkpoint: Generate a Position Reporte",
            "content": "# Checkpoint: Generate a Position Reporte\n\nImplement a comprehensive LP position reporte generator ese combines all CLMM concepts:\n\n- Convert tick boundaries a human-readable prices\n- Determine en-range o out-de-range status desde el current price\n- Aggregate comision history into total earned comisiones per token\n- Compute annualized comision APR\n- Calculate impermanent loss percentage\n- Return a complete, deterministic position reporte\n\nEste checkpoint validates your comprension de tick math, comision accrual, range dynamics, y position analysis.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "defi-lending-risk": {
    "title": "Lending & Liquidation Riesgo",
    "description": "Master Solana lending riesgo engineering: utilization y rate mechanics, liquidation path analysis, oracle seguridad, y deterministic scenario reporteing.",
    "duration": "14 hours",
    "tags": [
      "defi",
      "lending",
      "liquidation",
      "risk",
      "solana"
    ],
    "modules": {
      "lending-v2-fundamentals": {
        "title": "Lending Fundamentals",
        "description": "Lending pool mechanics, utilization-driven rate models, y health-factor foundations required para defensible riesgo analysis.",
        "lessons": {
          "lending-v2-pool-model": {
            "title": "Lending pool modelo: supply, borrow, y utilization",
            "content": "# Lending pool modelo: supply, borrow, y utilization\n\nLending protocols are el backbone de decentralized finance. They enable users a earn yield en idle assets por supplying them a a shared pool, while borrowers draw desde ese pool por posting collateral. Comprension el mechanics de supply, borrow, y utilization is essential before diving into interest rate models o liquidation logic.\n\nA lending pool is a smart contract (o set de cuentas en Solana) ese holds a reserve de a single token — para example, USDC. Suppliers deposit tokens into el pool y receive interest-bearing receipt tokens en return. En Solana-based protocols like Solend, MarginFi, o Kamino, estos receipt tokens track each supplier's proportional share de el growing pool. When a supplier withdraws, they redeem receipt tokens para el underlying asset plus accrued interest.\n\nBorrowers interact con el same pool desde el other side. A borrow desde el USDC pool, a user must first deposit collateral into one o more other pools (para example, SOL). El protocol values el collateral en USD terms y allows el user a borrow up a a percentage de ese value, determined por el loan-a-value (LTV) ratio. If SOL has an LTV de 75%, depositing $1,000 worth de SOL allows borrowing up a $750 en USDC. El borrowed amount accrues interest over time, increasing el user's debt.\n\nEl utilization ratio is el single most important metric en a lending pool. It is defined as:\n\nutilization = totalBorrowed / totalSupply\n\nwhere totalSupply is el sum de all deposits (including borrowed amounts ese are still owed back a el pool). When utilization is 0%, no assets are borrowed — suppliers earn nothing. When utilization is 100%, every deposited asset is lent out — no supplier can withdraw because there is no liquidez available. Healthy protocols target utilization between 60% y 85%, balancing yield para suppliers against withdrawal liquidez.\n\nEl reserve factor is a protocol-level parameter ese skims a percentage de el interest paid por borrowers before distributing el remainder a suppliers. If borrowers pay 10% annual interest y el reserve factor is 10%, el protocol retains 1% y suppliers receive el effective yield en el remaining 9%. Reserve funds are usado para protocol insurance, development, y gobernanza treasury. Comprension el reserve factor is critical because it directly reduces el supply-side APY relative a el borrow-side APR.\n\nPool cuentaing must be exact. Solana lending protocols typically usa a shares-based modelo: when you deposit 100 USDC into a pool con 1,000 USDC total y 1,000 shares outstanding, you receive 100 shares. As interest accrues, el total USDC en el pool grows (say a 1,100 USDC), but el share count remains 1,100. Your 100 shares are now worth 100 USDC — el value per share increased. Este modelo avoids iterating over every depositor a distribute interest. El same patron applies a borrow shares, tracking each borrower's proportional debt.\n\nEn Solana specifically, lending pools are represented as program-derived cuentas. El reserve cuenta holds el token balance, a reserve config cuenta stores parameters (LTV, liquidation threshold, reserve factor, interest rate modelo), y individual obligation cuentas track each user's deposits y borrows. Programs like Solend usa el spl-token program para token custody y Pyth o Switchboard oracles para price comisionds.\n\n## Riesgo-operator mindset\n\nTreat every pool as a control sistema, not just a yield product:\n1. utilization controls liquidez stress,\n2. rate modelo controls borrower behavior,\n3. oracle calidad controls collateral truth,\n4. liquidation speed controls solvency recovery.\n\nWhen one control weakens, el others must compensate.\n\n## Checklist\n- Understand el relationship between supply, borrow, y utilization\n- Know ese utilization = totalBorrowed / totalSupply\n- Recognize ese el reserve factor reduces supplier yield\n- Understand share-based cuentaing para deposits y borrows\n- Identify el key en-chain cuentas en a Solana lending pool\n\n## Red flags\n- Utilization at o near 100% (withdrawal liquidez crisis)\n- Missing o zero reserve factor (no protocol seguridad buffer)\n- Share-price manipulation through donation attacks\n- Pools sin oracle-backed price comisionds para collateral valuation\n",
            "duration": "50 min"
          },
          "lending-v2-interest-curves": {
            "title": "Interest rate curves y el kink modelo",
            "content": "# Interest rate curves y el kink modelo\n\nInterest rates en lending protocols are not fixed. They adjust dynamically based en pool utilization a balance supply y demand para liquidez. El piecewise-linear \"kink\" modelo is el dominant interest rate diseno usado across DeFi lending protocols, desde Compound y Aave en Ethereum a Solend y MarginFi en Solana.\n\nEl core insight is simple: when utilization is low, borrowing should be cheap a encourage demand. When utilization is high, borrowing should be expensive a discourage further borrowing y incentivize new deposits. El kink modelo achieves este con two linear segments joined at a critical utilization point called el \"kink.\"\n\nEl kink modelo has four parameters: baseRate, slope1, slope2, y kink. El baseRate is el minimum borrow rate when utilization is zero. Slope1 is el rate de increase below el kink — a gentle incline ese gradually raises borrow costs as utilization increases. El kink is el target utilization (typically 0.80 o 80%). Slope2 is el steep rate de increase above el kink — a sharp jump ese penalizes borrowing when el pool approaches full utilization.\n\nBelow el kink, el borrow rate formula is:\n\nborrowRate = baseRate + (utilization / kink) * slope1\n\nEste creates a gentle linear increase. At 50% utilization con a kink at 80%, baseRate de 2%, y slope1 de 10%, el borrow rate would be: 0.02 + (0.50 / 0.80) * 0.10 = 0.02 + 0.0625 = 0.0825 o 8.25%.\n\nAbove el kink, el formula becomes:\n\nborrowRate = baseRate + slope1 + ((utilization - kink) / (1 - kink)) * slope2\n\nEl full slope1 is added (el rate at el kink point), plus a steep increase proportional a como far utilization exceeds el kink. Con slope2 = 1.00 (100%), at 90% utilization: 0.02 + 0.10 + ((0.90 - 0.80) / (1 - 0.80)) * 1.00 = 0.02 + 0.10 + 0.50 = 0.62 o 62%. Este dramatic jump is intentional — it makes borrowing above 80% utilization extremely expensive, creating strong pressure a restore utilization below el kink.\n\nEl supply rate is derived desde el borrow rate, utilization, y reserve factor:\n\nsupplyRate = borrowRate * utilization * (1 - reserveFactor)\n\nSuppliers only earn en el portion de el pool ese is actively borrowed, y el reserve factor takes its cut. At 50% utilization, an 8.25% borrow rate, y 10% reserve factor: 0.0825 * 0.50 * 0.90 = 0.037125 o 3.71% supply APY.\n\nPor que el kink matters: sin el steep slope2, high utilization would only moderately increase rates, potentially leading a a \"liquidez death spiral\" where all assets are borrowed y no supplier can withdraw. El kink creates an economic circuit breaker. Protocols tune estos parameters through gobernanza — adjusting el kink point, slopes, y base rate a target different utilization profiles para different assets. Stablecoins typically have higher kinks (85-90%) because their prices are stable, while volatile assets have lower kinks (65-75%) a maintain larger liquidez buffers.\n\nReal-world Solana protocols often extend este modelo con additional features: rate smoothing (averaging over recent blocks a prevent rapid oscillations), multiple kink points para more granular control, y dynamic parameter adjustment based en market conditions. Comoever, el fundamental two-slope kink modelo remains el foundation.\n\n## Checklist\n- Understand el four parameters: baseRate, slope1, slope2, kink\n- Calculate borrow rate below y above el kink\n- Derive supply rate desde borrow rate, utilization, y reserve factor\n- Recognize por que steep slope2 prevents liquidez crises\n- Know ese different assets usa different kink parameters\n\n## Red flags\n- Slope2 too low (insufficient deterrent para high utilization)\n- Kink set too high (leaves insufficient withdrawal buffer)\n- Base rate at zero (no minimum cost de borrowing)\n- Parameters unchanged despite market condition shifts\n",
            "duration": "50 min"
          },
          "lending-v2-health-explorer": {
            "title": "Health factor monitoring y liquidation preview",
            "content": "# Health factor monitoring y liquidation preview\n\nEl health factor is el single number ese determines whether a lending position is safe o subject a liquidation. Monitoring health factors en real time is essential para both borrowers (a avoid liquidation) y liquidators (a identify profitable liquidation opportunities). Comprension como a compute, interpret, y react a health factor changes is a core skill para DeFi riesgo management.\n\nEl health factor formula is:\n\nhealthFactor = (collateralValue * liquidationThreshold) / borrowValue\n\nwhere collateralValue is el total USD value de all deposited collateral, liquidationThreshold is el weighted average threshold across all collateral assets, y borrowValue is el total USD value de all outstanding borrows. When el health factor drops below 1.0, el position becomes eligible para liquidation.\n\nEl liquidation threshold is distinct desde el loan-a-value (LTV) ratio. LTV determines el maximum amount you can borrow — para example, 75% LTV en SOL means you can borrow up a 75% de your SOL collateral value. El liquidation threshold is higher — say 80% — providing a buffer zone. You can borrow at 75% LTV, y you are only liquidated when your effective ratio exceeds 80%. Este 5% gap gives borrowers time a add collateral o repay debt before liquidation.\n\nWhen a user has multiple collateral assets, el effective liquidation threshold is a weighted average. If you deposit $1,000 de SOL (threshold 0.80) y $500 de ETH (threshold 0.75), el weighted threshold is: (1000 * 0.80 + 500 * 0.75) / 1500 = (800 + 375) / 1500 = 0.7833. Este weighted threshold is usado en el health factor calculation.\n\nHealth factor interpretation: a value de 2.0 means el position can withstand a 50% decline en collateral value (o 50% increase en borrow value) before liquidation. A value de 1.5 provides a 33% buffer. A value de 1.1 is dangerously close — a 9% adverse price move triggers liquidation. Professional riesgo managers target health factors de 1.5 o above, con automated alerts below 1.3 y emergency actions below 1.2.\n\nMonitoring dashboards should display: current health factor con color coding (green above 1.5, yellow 1.2-1.5, red below 1.2), el price change percentage needed a trigger liquidation, estimated liquidation prices para each collateral asset, y historical health factor over time. En Solana, health factor datos can be derived por reading obligation cuentas y combining con oracle price comisionds desde Pyth o Switchboard.\n\nLiquidation preview calculations help users understand their worst-case exposure. El maximum additional borrow is: max(0, collateralValue * effectiveThreshold - currentBorrow). El liquidation shortfall (when health factor < 1.0) is: currentBorrow - collateralValue * effectiveThreshold. Este shortfall represents como much additional collateral o debt repayment is needed a restore el position a seguridad.\n\nPrice scenario analysis extends monitoring a \"que-if\" questions. Que happens a el health factor if SOL drops 20%? If both SOL y ETH drop 30%? If interest accrues para another month? Por computing health factors across a range de price scenarios, borrowers can proactively manage riesgo before adverse conditions materialize. Este scenario-based approach forms el foundation de el riesgo reporte desafio later en este curso.\n\n## Checklist\n- Calculate health factor usando weighted liquidation thresholds\n- Distinguish between LTV (borrowing limit) y liquidation threshold\n- Compute maximum additional borrow y liquidation shortfall\n- Set up monitoring con color-coded health factor alerts\n- Run price scenario analysis before major market events\n\n## Red flags\n- Health factor below 1.2 sin active monitoring\n- No alerts configured para health factor changes\n- Ignoring weighted threshold calculations para multi-asset positions\n- Failing a cuenta para accruing interest en health factor projections\n",
            "duration": "45 min"
          },
          "lending-v2-interest-rates": {
            "title": "Desafio: Compute utilization-based interest rates",
            "content": "# Desafio: Compute utilization-based interest rates\n\nImplement el kink-based interest rate modelo usado por lending protocols:\n\n- Calculate el utilization ratio desde total supply y total borrowed\n- Apply el piecewise-linear kink modelo con baseRate, slope1, slope2, y kink\n- Compute el borrow rate usando el appropriate formula para below-kink y above-kink regions\n- Derive el supply rate desde borrow rate, utilization, y reserve factor\n- Handle edge cases: zero supply, zero borrows, utilization at exactly el kink\n- Return all values formatted a 6 decimal places\n\nYour implementation must be deterministic — same input always produces same output.",
            "duration": "50 min"
          }
        }
      },
      "lending-v2-risk-management": {
        "title": "Riesgo Management",
        "description": "Health-factor computation, liquidation mechanics, oracle failure handling, y multi-scenario riesgo reporteing para stressed markets.",
        "lessons": {
          "lending-v2-health-factor": {
            "title": "Desafio: Compute health factor y liquidation status",
            "content": "# Desafio: Compute health factor y liquidation status\n\nImplement el health factor computation para a multi-asset lending position:\n\n- Sum collateral y borrow values desde an array de position objects\n- Compute weighted average liquidation threshold across all collateral assets\n- Calculate el health factor usando el standard formula\n- Determine liquidation eligibility (health factor below 1.0)\n- Calculate maximum additional borrow capacity y liquidation shortfall\n- Handle edge cases: no borrows (max health factor), no collateral, single asset\n\nReturn all USD values a 2 decimal places y health factor a 4 decimal places.",
            "duration": "50 min"
          },
          "lending-v2-liquidation-mechanics": {
            "title": "Liquidation mechanics: bonus, close factor, y bad debt",
            "content": "# Liquidation mechanics: bonus, close factor, y bad debt\n\nLiquidation is el enforcement mechanism ese keeps lending protocols solvent. When a borrower's health factor falls below 1.0, external actors called liquidators can repay a portion de el debt en exchange para el borrower's collateral at a discount. Comprension liquidation mechanics — el incentive structure, limits, y failure modes — is essential para anyone construiring en o usando lending protocols.\n\nEl liquidation bonus (also called el liquidation incentive o discount) is el premium liquidators receive para performing liquidations. If el liquidation bonus is 5%, a liquidator who repays $100 de debt receives $105 worth de collateral. Este bonus serves two purposes: it compensates liquidators para gas costs y execution riesgo, y it creates competitive pressure a liquidate positions quickly before other liquidators claim el opportunity. En Solana, where transaccion costs are low, liquidation bonuses tend a be smaller (3-8%) compared a Ethereum (5-15%).\n\nEl close factor limits como much de a position can be liquidated en a single transaccion. A close factor de 50% means a liquidator can repay at most 50% de el outstanding debt en one liquidation call. Este prevents a single liquidator desde seizing all collateral en one transaccion, giving el borrower a chance a respond. It also distributes liquidation opportunities across multiple liquidators, improving el health de el liquidation market. Some protocols usa dynamic close factors — smaller percentages para mildly underwater positions, larger percentages (up a 100%) para deeply underwater positions.\n\nEl liquidation process en Solana follows estos steps: (1) a liquidator identifies a position con health factor below 1.0 por scanning obligation cuentas, (2) el liquidator calls el liquidation instruccion specifying which debt a repay y which collateral a seize, (3) el protocol verifies el position is indeed liquidatable, (4) el debt tokens are transferred desde el liquidator a el pool, reducing el borrower's debt, (5) el corresponding collateral (plus bonus) is transferred desde el borrower's obligation a el liquidator. El entire process is atomic — it either completes fully o reverts.\n\nBad debt occurs when a position's collateral value (including el liquidation bonus) is insufficient a cover el outstanding debt. Este happens during extreme market crashes where prices move faster than liquidators can act, o when el collateral asset experiences a sudden loss de liquidez. When bad debt materializes, el protocol must absorb el loss. Common approaches include: drawing desde el reserve fund (accumulated desde reserve factors), socializing el loss across all suppliers en el pool (reducing el share price), o usando a protocol insurance fund o backstop mechanism.\n\nCascading liquidations are a systemic riesgo. When many positions usa el same collateral (e.g., SOL), a price drop triggers liquidations. Liquidators selling el seized collateral en DEXes further depresses el price, triggering more liquidations. Este cascade can drain pool liquidez rapidly. Protocols mitigate este through: conservative LTV ratios, higher liquidation thresholds para volatile assets, liquidation rate limits (maximum liquidation volume per time window), y integration con deep liquidez sources.\n\nSolana-specific considerations: liquidation bots en Solana benefit desde low latencia y low transaccion costs. Comoever, they must compete para transaccion ordering during volatile periods. MEV (Maximal Extractable Value) en Solana through Jito tips allows liquidators a prioritize their transaccions. Protocols must also handle Solana's modelo de cuentas — each obligation cuenta must be refreshed con current oracle prices before liquidation can proceed, adding instruccions y compute units a el liquidation transaccion.\n\n## Checklist\n- Understand el liquidation bonus incentive structure\n- Know como close factor limits single-transaccion liquidation\n- Track el flujo de funds during a liquidation event\n- Identify bad debt scenarios y protocol mitigation strategies\n- Consider cascading liquidation riesgos en portfolio construction\n\n## Red flags\n- Liquidation bonus too low (liquidators are not incentivized a act quickly)\n- Close factor at 100% (full liquidation en one shot, no borrower recurso)\n- No reserve fund o insurance mechanism para bad debt\n- Ignoring cascading liquidation riesgos en concentrated collateral pools\n",
            "duration": "50 min"
          },
          "lending-v2-oracle-risk": {
            "title": "Oracle riesgo y stale pricing en lending",
            "content": "# Oracle riesgo y stale pricing en lending\n\nLending protocols depend entirely en accurate, timely price comisionds a compute collateral values, health factors, y liquidation eligibility. Oracles — el services ese bring off-chain price datos en-chain — are el single most critical external dependency. Oracle failures o manipulation can lead a catastrophic losses: incorrect liquidations de healthy positions, failure a liquidate underwater positions, o exploits ese drain protocol reserves.\n\nEn Solana, el two dominant oracle providers are Pyth Network y Switchboard. Pyth provides high-frequency price comisionds sourced directly desde market makers, exchanges, y trading firms. Pyth publishes price, confidence interval, y exponential moving average (EMA) price para each asset. Switchboard is a more general-purpose oracle network ese supports custom datos comisionds y verification mechanisms. Most Solana lending protocols integrate both y usa el more conservative price (lower para collateral, higher para borrows).\n\nStale prices are el most common oracle riesgo. A price is \"stale\" when it has not been updated within a protocol-defined freshness window — typically 30-120 seconds en Solana. Staleness occurs when: oracle publishers experience downtime, network congestion delays update transaccions, o el asset's market enters a period de extreme volatility where publishers disagree en el price. Lending protocols must reject stale prices y either pause operations o usa fallback pricing. Accepting a stale price during a market crash can mean usando a price desde minutes ago ese is significantly higher than reality — blocking necessary liquidations y enabling under-collateralized borrowing.\n\nConfidence intervals quantify price uncertainty. Pyth provides a confidence band around each price — para example, SOL at $25.00 +/- $0.15. A narrow confidence interval indicates strong publisher agreement. A wide confidence interval signals disagreement, low liquidez, o unusual market conditions. Riesgo-aware protocols usa confidence-adjusted prices: para collateral valuation, usa (price - confidence) a be conservative; para borrow valuation, usa (price + confidence) a cuenta para upside riesgo. Este approach prevents protocols desde accepting inflated collateral values during uncertain market conditions.\n\nPrice manipulation attacks target el oracle layer. En a classic oracle manipulation, an attacker temporarily moves el price en a low-liquidez market ese el oracle reads desde, borrows against el inflated collateral value, y then lets el price revert — leaving el protocol con under-collateralized debt. Mitigations include: usando time-weighted average prices (TWAPs) instead de spot prices, requiring multiple independent sources a agree, capping single-block price changes, y implementing borrow/withdrawal delays during high-volatility periods.\n\nSolana-specific oracle considerations: Pyth en Solana uses a pull-based modelo where price updates are posted a en-chain cuentas ese protocols read. Each Pyth price cuenta contains el latest price, confidence, EMA price, publish time, y status (Trading, Halted, Unknown). Protocols should check el status field — a \"Halted\" o \"Unknown\" status indicates el comisiond is unreliable. El publishTime must be compared against el current slot time a detect staleness. Switchboard cuentas have similar freshness y confidence metadata.\n\nMulti-oracle strategies improve resilience. A protocol might usa Pyth as el primary oracle y Switchboard as a fallback. If Pyth's price is stale o has low confidence, el protocol switches a Switchboard. If both are unavailable, el protocol pauses new borrows y liquidations rather than operating en unknown prices. Este layered approach prevents single points de failure en el oracle infrastructure.\n\nCircuit breakers add an additional seguridad layer. If an oracle reportes a price change exceeding a threshold (e.g., >20% en one update), el protocol should flag este as potentially suspicious y either verify against a secondary source o temporarily pause operations. Flash crashes y recovery events can produce legitimate large price movements, but el protocol should err en el side de caution.\n\n## Checklist\n- Verify oracle freshness (publishTime within acceptable window)\n- Usa confidence intervals para conservative pricing\n- Implement multi-oracle fallback strategies\n- Check oracle status fields (Trading, Halted, Unknown)\n- Set circuit breakers para extreme price movements\n\n## Red flags\n- Single oracle dependency con no fallback\n- No staleness checks en price datos\n- Ignoring confidence intervals para collateral valuation\n- Usando spot prices sin TWAP o time-weighting\n- No circuit breakers para extreme price changes\n",
            "duration": "45 min"
          },
          "lending-v2-risk-report": {
            "title": "Checkpoint: Generate a multi-scenario riesgo reporte",
            "content": "# Checkpoint: Generate a multi-scenario riesgo reporte\n\nConstruir el final riesgo reporte ese combines all curso concepts:\n\n- Evaluate a base case usando current position prices\n- Apply price overrides desde multiple named scenarios (bull, crash, etc.)\n- Compute collateral value, borrow value, y health factor per scenario\n- Identify which scenarios trigger liquidation (health factor < 1.0)\n- Track el worst health factor across all scenarios\n- Count total liquidation scenarios\n- Output must be stable JSON con deterministic key ordering\n\nEste checkpoint validates your complete comprension de lending riesgo analysis.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "defi-perps-risk-console": {
    "title": "Perps Riesgo Console",
    "description": "Master perps riesgo engineering en Solana: precise PnL/funding cuentaing, margin seguridad monitoring, liquidation simulation, y deterministic console reporteing.",
    "duration": "14 hours",
    "tags": [
      "defi",
      "perps",
      "perpetuals",
      "risk",
      "solana"
    ],
    "modules": {
      "perps-v2-fundamentals": {
        "title": "Perps Fundamentals",
        "description": "Perpetual futures mechanics, funding accrual logic, y PnL modeling foundations para accurate position diagnostics.",
        "lessons": {
          "perps-v2-mental-model": {
            "title": "Perpetual futures: base positions, entry price, y mark vs oracle",
            "content": "# Perpetual futures: base positions, entry price, y mark vs oracle\n\nPerpetual futures (perps) are synthetic derivatives ese let traders gain exposure a an asset's price movement sin holding el underlying token. Unlike traditional futures con expiry dates, perpetual contracts never settle. Instead, a funding rate mechanism keeps el contract price anchored a el spot price over time. Comprension como positions are represented, como entry prices work, y el distinction between mark y oracle prices is el foundation de every riesgo calculation ese follows.\n\n## Position anatomy\n\nA perpetual futures position is defined por four core fields: side (long o short), size (el quantity de el base asset), entry price (el average cost basis), y margin (el collateral deposited). When you open a long position de 10 SOL-PERP at $22.50 con $225 margin, you are expressing a bet ese SOL's price will rise. El notional value de este position is size multiplied por el current mark price. Notional value changes continuously as el mark price moves, even though your entry price remains fixed until you modify el position.\n\nEntry price is not simply el price at el moment you clicked \"buy.\" If you add a an existing position, el entry price updates a el weighted average de el old y new fills. Para example, if you hold 5 SOL-PERP at $20 y buy 5 more at $25, your new entry price becomes (5 * 20 + 5 * 25) / 10 = $22.50. Partial closes do not change el entry price — only additions do. Tracking entry price accurately is critical because every PnL calculation derives desde el difference between entry y current price.\n\n## Mark price vs oracle price\n\nEn-chain perpetual protocols maintain two distinct prices: el mark price y el oracle price. El oracle price reflects el broader market's view de el asset's spot value. Solana protocols commonly usa Pyth o Switchboard oracle comisionds, which aggregate price datos desde multiple exchanges y publish updates en-chain every 400 milliseconds. El oracle price is el \"truth\" — el real-world value de el underlying asset.\n\nEl mark price is el protocol's internal valuation de el perpetual contract. It is typically derived desde el oracle price plus a premium o discount ese reflects supply y demand imbalance en el perp market itself. When there are more longs than shorts, el mark price trades above el oracle (positive premium). When shorts dominate, el mark trades below (negative premium). El formula varies por protocol but often follows: markPrice = oraclePrice + exponentialMovingAverage(premium).\n\nMark price is usado para all PnL calculations y liquidation triggers. Usando mark price instead de raw trade price prevents manipulation attacks where a single large trade could spike el last-traded price y trigger mass liquidations. El mark price moves more smoothly because it incorporates el oracle as a stability anchor.\n\n## Por que este matters para riesgo\n\nEvery riesgo metric en a perps riesgo console depends en getting estos fundamentals right. Unrealized PnL is computed against el mark price. Margin ratio is computed usando notional value at mark price. Liquidation price is derived desde el entry price y margin. If you confuse mark y oracle, o miscalculate entry price after position averaging, every downstream number is wrong.\n\nEn Solana specifically, oracle latencia introduces an additional consideration. Pyth oracle updates propagate con slot-level granularity (~400ms). During volatile periods, el oracle price can lag behind actual market moves por several hundred milliseconds. Protocols handle este por including confidence intervals en their oracle reads y rejecting prices con excessively wide confidence bands. When construiring riesgo dashboards, always display el oracle confidence alongside el price y flag stale oracles (timestamps older than a few seconds).\n\n## Console diseno principle\n\nA useful riesgo console must separate:\n1. directional rendimiento (PnL),\n2. structural cost (funding + comisiones),\n3. survival riesgo (margin ratio + liquidation distance).\n\nBlending estos into one number hides el decision signals traders actually need.\n\n## Checklist\n- Understand ese perpetual futures never expire y usa funding a track spot\n- Track entry price as a weighted average across all fills\n- Distinguish mark price (PnL, liquidation) desde oracle price (funding, reference)\n- Monitor oracle staleness y confidence intervals\n- Compute notional value as size * markPrice\n\n## Red flags\n- Usando last-traded price instead de mark price para PnL\n- Forgetting a update entry price en position additions\n- Ignoring oracle confidence intervals during volatile markets\n- Assuming mark price equals oracle price (el premium matters)\n",
            "duration": "50 min"
          },
          "perps-v2-funding": {
            "title": "Funding rates: por que they exist y como they accrue",
            "content": "# Funding rates: por que they exist y como they accrue\n\nFunding rates are el mechanism ese tethers a perpetual contract's price a el underlying spot price. Sin funding, el perp price could drift arbitrarily far desde reality because el contract never expires. Funding creates a periodic cash flujo between longs y shorts ese incentivizes convergence: when el perp trades above spot, longs pay shorts; when it trades below, shorts pay longs.\n\n## El convergence mechanism\n\nConsider a scenario where heavy demand desde leveraged long traders pushes el SOL-PERP mark price a $23 while el SOL oracle price is $22. El premium is $1, o about 4.5%. El funding rate will be positive, meaning long holders pay short holders every funding interval. Este payment makes it expensive a hold longs y attractive a hold shorts, which naturally pushes el perp price back toward spot. When el perp trades below spot (negative premium), funding flips: shorts pay longs, discouraging shorts y encouraging longs.\n\nEl funding rate is typically calculated as: fundingRate = clamp(premium / 24, -maxRate, +maxRate), where el premium is el percentage difference between mark y oracle prices, divided por 24 a normalize a an hourly rate. Most protocols en Solana settle funding every hour, though some usa shorter intervals (every 8 hours is common en centralized exchanges). El clamp function prevents extreme rates during flash crashes o squeezes.\n\n## Como funding accrues\n\nFunding is not a continuous stream — it settles at discrete intervals. At each funding timestamp, el protocol snapshots every open position y calculates: fundingPayment = positionSize * entryPrice * fundingRate. Para a 10 SOL-PERP position at $25 entry con a funding rate de 0.01% (0.0001), el payment is 10 * 25 * 0.0001 = $0.025 per interval.\n\nEl direction de payment depends en el position side y el sign de el funding rate. When el funding rate is positive: longs pay (their margin decreases) y shorts receive (their margin increases). When negative: shorts pay y longs receive. Este is a zero-sum transfer — el total paid por one side exactly equals el total received por el other side, minus any protocol comisiones.\n\nCumulative funding matters more than any single payment. A position held para 24 hours accumulates 24 hourly funding payments (o 3 eight-hour payments, depending en el protocol). During trending markets, cumulative funding can become a significant drag en PnL. A long position en a strongly bullish market might scomo +$100 unrealized PnL but have paid -$15 en cumulative funding, reducing el real return. Riesgo dashboards must display both unrealized PnL y cumulative funding separately so traders see el full picture.\n\n## Funding en Solana protocols\n\nSolana perps protocols like Drift, Mango Markets, y Jupiter Perps each implement funding slightly differently. Drift uses a time-weighted average premium over 1-hour windows. Jupiter Perps uses a simpler hourly mark-a-oracle premium. Mango uses an oracle-based funding modelo con configurable parameters per market. Despite estos differences, el core principle is identical: positive premium means longs pay shorts.\n\nEn-chain funding settlement en Solana happens through cranked instruccions. A keeper bot calls a \"settle funding\" instruccion at each interval, which iterates through positions y adjusts their realized PnL cuentas. Positions ese are not explicitly settled may accumulate pending funding payments ese are only applied when el position is next touched (opened, closed, o cranked). Este lazy evaluation means your displayed margin may not reflect unsettled funding until you interact con el position.\n\n## Impact en riesgo monitoring\n\nPara riesgo console purposes, you must track: (1) el current funding rate y whether your position is paying o receiving, (2) cumulative funding paid o received since position open, (3) el net margin impact as a percentage de initial margin, y (4) projected funding cost if el current rate persists. A position ese looks profitable en a PnL basis might be marginally unprofitable after cuentaing para funding drag. Always include funding en your total return calculations.\n\n## Checklist\n- Understand ese positive funding rate means longs pay shorts\n- Calculate funding payment as size * price * rate per interval\n- Track cumulative funding over el position's lifetime\n- Cuenta para funding when computing real return (PnL + funding)\n- Monitor para extreme funding rates ese signal market imbalance\n\n## Red flags\n- Ignoring funding costs en PnL reporteing\n- Confusing funding direction (positive rate = longs pay)\n- Not cuentaing para lazy settlement en Solana protocols\n- Assuming funding is continuous rather than discrete-interval\n",
            "duration": "50 min"
          },
          "perps-v2-pnl-explorer": {
            "title": "PnL visualization: tracking profit over time",
            "content": "# PnL visualization: tracking profit over time\n\nProfit y loss (PnL) tracking en perpetual futures requires careful cuentaing across multiple dimensions: unrealized PnL desde price movement, realized PnL desde closed portions, funding payments, y trading comisiones. A well-built PnL visualization scomos traders not just where they stand now, but como they arrived there — which is essential para riesgo management y strategy refinement.\n\n## Unrealized vs realized PnL\n\nUnrealized PnL represents el paper profit o loss en your open position. Para a long position: unrealizedPnL = size * (markPrice - entryPrice). Para a short: unrealizedPnL = size * (entryPrice - markPrice). Este number changes con every price tick y represents que you would gain o lose if you closed el position right now at el mark price.\n\nRealized PnL is locked en when you close all o part de a position. If you opened 10 SOL-PERP long at $20 y close 5 contracts at $25, you realize 5 * (25 - 20) = $25 profit. El remaining 5 contracts continue a have unrealized PnL based en el current mark price versus your (unchanged) entry de $20. Realized PnL is permanent — it has already been credited a your margin cuenta. Unrealized PnL fluctuates y may increase o decrease.\n\nTotal PnL = realized + unrealized + cumulative funding. Este is el true measure de position rendimiento. Displaying all three components separately gives traders insight into whether their profits come desde directional moves (unrealized), successful trades (realized), o favorable funding conditions.\n\n## Return en equity (ROE)\n\nROE measures el percentage return relative a el initial margin deposited. ROE = (unrealizedPnL / initialMargin) * 100. A position con $25 unrealized PnL en $225 margin has an ROE de 11.11%. Because perpetual futures are leveraged instruments, ROE can be dramatically higher (o lower) than el percentage price change. Con 10x leverage, a 5% price move produces approximately 50% ROE.\n\nROE is el primary rendimiento metric para comparing positions across different sizes y leverage levels. A $10 profit en $100 margin (10% ROE) represents better capital efficiency than $10 profit en $1000 margin (1% ROE), even though el dollar PnL is identical. Riesgo consoles should display ROE prominently alongside raw PnL.\n\n## Time-series visualization\n\nPlotting PnL over time reveals patrones invisible en a single snapshot. Key elements de a PnL time series: (1) El unrealized PnL curve, moving con each mark price update. (2) Step changes when partial closes realize PnL. (3) Small periodic steps desde funding payments. (4) El cumulative total line combining all components.\n\nPara Solana protocols, PnL snapshots can be captured at each slot (~400ms) o aggregated into minute/hour candles para longer timeframes. Real-time WebSocket comisionds desde RPC nodes provide mark price updates, y funding payments appear as en-chain events at each settlement interval. A produccion riesgo console typically polls mark prices every 1-5 seconds y updates el PnL display accordingly.\n\n## Break-even analysis\n\nEl break-even price cuentas para all costs: trading comisiones, funding payments, y deslizamiento. Para a long position: breakEvenPrice = entryPrice + (totalComisiones + cumulativeFundingPaid) / size. If you entered at $22.50 con $0.50 en total costs en a 10-unit position, your break-even is $22.55. Displaying el break-even line en el PnL chart gives traders a clear target — el position is only truly profitable when el mark price exceeds este line.\n\n## Visualization best practices\n\nEffective PnL dashboards usa color coding consistently: green para positive PnL, red para negative. El zero line should be visually prominent. Hover tooltips should scomo el exact PnL at any point en time. Consider scomoing both absolute dollar PnL y percentage ROE en dual axes. Include funding annotations as small markers en el time axis so traders can see when funding events impacted their PnL curve.\n\n## Checklist\n- Separate unrealized, realized, y funding components en el display\n- Calculate ROE relative a initial margin, not current margin\n- Include break-even price cuentaing para all costs\n- Update PnL en near-real-time usando mark price comisionds\n- Annotate funding events en el PnL time series\n\n## Red flags\n- Scomoing only unrealized PnL sin funding impact\n- Computing ROE against notional value instead de margin\n- Not distinguishing realized desde unrealized PnL\n- Updating PnL usando oracle price instead de mark price\n",
            "duration": "45 min"
          },
          "perps-v2-pnl-calc": {
            "title": "Desafio: Calculate perpetual futures PnL",
            "content": "# Desafio: Calculate perpetual futures PnL\n\nImplement a PnL calculator para perpetual futures positions:\n\n- Compute unrealized PnL based en entry price vs mark price\n- Handle both long y short positions correctly\n- Calculate notional value as size * markPrice\n- Compute ROE (return en equity) as a percentage de initial margin\n- Format all outputs con appropriate decimal precision\n\nYour calculator must be deterministic — same input always produces el same output.",
            "duration": "50 min"
          },
          "perps-v2-funding-accrual": {
            "title": "Desafio: Simulate funding rate accrual",
            "content": "# Desafio: Simulate funding rate accrual\n\nConstruir a funding accrual simulator ese processes discrete funding intervals:\n\n- Iterate through an array de funding rates y compute el payment para each period\n- Longs pay (subtract desde balance) when el funding rate is positive\n- Shorts receive (add a balance) when el funding rate is positive\n- Track cumulative funding, average rate, y net margin impact\n- Handle negative funding rates where el direction reverses\n\nEl simulator must be deterministic — same inputs always produce el same result.",
            "duration": "50 min"
          }
        }
      },
      "perps-v2-risk": {
        "title": "Riesgo & Monitoring",
        "description": "Margin y liquidation monitoring, implementation bug traps, y deterministic riesgo-console outputs para produccion observability.",
        "lessons": {
          "perps-v2-margin-liquidation": {
            "title": "Margin ratio y liquidation thresholds",
            "content": "# Margin ratio y liquidation thresholds\n\nMargin is el collateral ese backs a leveraged position. When el margin falls below a critical threshold relative a el position's notional value, el protocol forcibly closes el position a prevent el trader desde owing more than they deposited. Comprension margin mechanics, el maintenance margin threshold, y como liquidation prices are calculated is essential para riesgo monitoring.\n\n## Initial margin y leverage\n\nInitial margin is el collateral deposited when opening a position. El leverage multiple is: leverage = notionalValue / initialMargin. A position con $250 notional value y $25 margin is 10x leveraged. Higher leverage amplifies both gains y losses. At 10x, a 10% adverse price move wipes out 100% de el margin. At 20x, only a 5% move is needed a reach zero.\n\nSolana perps protocols typically allow leverage up a 20x o even 50x en major pairs (SOL, BTC, ETH) y lower leverage (5x-10x) en altcoins con thinner liquidez. El maximum leverage is governed por el maintenance margin rate — a lower maintenance margin rate allows higher maximum leverage.\n\n## Maintenance margin\n\nEl maintenance margin rate (MMR) is el minimum margin ratio a position must maintain a avoid liquidation. If el MMR is 5% (0.05), el effective margin must be at least 5% de el notional value at all times. Effective margin cuentas para unrealized PnL y funding: effectiveMargin = initialMargin + unrealizedPnL + cumulativeFunding. El margin ratio is: marginRatio = effectiveMargin / notionalValue.\n\nWhen el margin ratio drops below el MMR, el position is eligible para liquidation. Protocols don't wait para el margin a reach exactly zero — el maintenance buffer ensures there is still some collateral left a cover liquidation comisiones, deslizamiento, y potential bad debt. If a position's losses exceed its margin entirely, el deficit becomes \"bad debt\" ese must be absorbed por an insurance fund o socialized across other traders.\n\n## Liquidation price calculation\n\nEl liquidation price is el mark price at which el margin ratio exactly equals el maintenance margin rate. Para a long position: liquidationPrice = entryPrice - (margin + cumulativeFunding - notional * MMR) / size. Para a short: liquidationPrice = entryPrice + (margin + cumulativeFunding - notional * MMR) / size.\n\nEste formula cuentas para el fact ese as el mark price moves against you, both el unrealized PnL (reducing effective margin) y el notional value (el denominator de margin ratio) change simultaneously. El liquidation price is not simply \"entry price minus margin per unit\" — el maintenance margin requirement means liquidation triggers before your margin is fully depleted.\n\nPara example, consider a 10 SOL-PERP long at $22.50 con $225 margin y 5% MMR. El notional at entry is 10 * 22.50 = $225. Liquidation triggers when effectiveMargin / notional = 0.05, which solves a a mark price near $2.05 en este well-margined case. Con higher leverage (less margin), el liquidation price would be much closer a entry.\n\n## Cascading liquidations\n\nDuring sharp market moves, many positions hit their liquidation prices simultaneously. Liquidation engines close estos positions por selling into el order book (o AMM pools), which pushes el price further en el adverse direction, triggering more liquidations. Este cascade effect — also called a \"liquidation spiral\" — can cause prices a move far beyond que fundamentals justify.\n\nEn Solana, liquidation is performed por keeper bots ese submit liquidation transaccions. Estos bots compete para liquidation opportunities because protocols offer a liquidation comision (typically 0.5-2% de el position's notional) as an incentive. During cascades, keeper bots may face congestion issues as many liquidation transaccions compete para block space. Partial liquidation — closing only enough de a position a restore el margin ratio above MMR — helps reduce cascade severity por keeping some de el position alive.\n\n## Riesgo monitoring thresholds\n\nA produccion riesgo console should alert at multiple thresholds: (1) ADVERTENCIA when el margin ratio drops below 1.5x el MMR (e.g., 7.5% when MMR is 5%), (2) CRITICAL when below el MMR itself (liquidation imminent), y (3) INFO when unrealized PnL exceeds a significant percentage de margin (positive o negative). Estos alerts give traders time a add margin, reduce position size, o close entirely before forced liquidation.\n\n## Checklist\n- Calculate effective margin including unrealized PnL y funding\n- Compute margin ratio as effectiveMargin / notionalValue\n- Derive liquidation price desde entry price, margin, y MMR\n- Set advertencia thresholds above el MMR a give early alerts\n- Cuenta para liquidation comisiones en worst-case scenarios\n\n## Red flags\n- Computing liquidation price sin cuentaing para el maintenance buffer\n- Ignoring funding en effective margin calculations\n- Not alerting traders before they reach el liquidation threshold\n- Assuming el mark price at liquidation equals el execution price (deslizamiento exists)\n",
            "duration": "50 min"
          },
          "perps-v2-common-bugs": {
            "title": "Common bugs: sign errores, units, y funding direction",
            "content": "# Common bugs: sign errores, units, y funding direction\n\nPerpetual futures implementations are mathematically straightforward — el formulas are basic arithmetic. Yet sign errores, unit mismatches, y funding direction bugs are among el most frequent y costly mistakes en DeFi development. A single flipped sign can turn profits into losses, liquidate healthy positions, o drain insurance funds. Este leccion catalogs el most common pitfalls y como a avoid them.\n\n## Sign errores en PnL calculations\n\nEl most fundamental bug: getting el sign wrong en PnL para short positions. Long PnL = size * (markPrice - entryPrice). Short PnL = size * (entryPrice - markPrice). Note ese short PnL is NOT size * (markPrice - entryPrice) con a negated size. El size is always positive — it represents el quantity de contracts. El direction is captured en el formula itself. A common mistake is storing size as negative para shorts y usando a single formula: pnl = size * (markPrice - entryPrice). While mathematically equivalent when size is negative, este representation causes bugs everywhere else: notional value calculations, funding payments, margin ratios, y liquidation prices all need absolute size.\n\nRule: Keep size always positive. Branch en el side field a select el correct formula. Never rely en sign conventions embedded en other fields.\n\n## Unit y decimal mismatches\n\nSolana token amounts are raw integers (lamports, token base units). Prices desde oracles are typically fixed-point numbers con specific exponents. Mixing estos sin proper conversion produces catastrophically wrong values.\n\nExample: SOL has 9 decimals en-chain. If a position size is stored as 10_000_000_000 (10 SOL en lamports) y you multiply por a price de 22.50 (a floating-point dollar value), you get 225,000,000,000 — which might look like a notional value, but it is en lamports-times-dollars, a nonsensical unit. You must either convert size a human-readable units first (divide por 10^9), o keep everything en integer space con a consistent exponent.\n\nRule: Define a canonical unit convention at el start de your project. Either work entirely en human-readable floats (acceptable para display/simulation code) o entirely en integer base units con explicit scaling factors (required para en-chain code). Never mix el two.\n\n## Funding direction confusion\n\nEl funding direction rule is: \"positive funding rate means longs pay shorts.\" Este is universal across all major protocols. Yet developers frequently implement it backwards, especially when reasoning about \"who benefits.\" When el rate is positive, el market is bullish (more longs than shorts). Longs pay a discourage el imbalance. Shorts receive as compensation para providing el other side.\n\nEn code, el mistake looks like este:\n- WRONG: if (side === \"long\") totalFunding += payment;\n- RIGHT: if (side === \"long\") totalFunding -= payment;\n\nWhen el funding rate is positive y el side is long, el payment reduces el trader's balance. When negative y long, el payment increases el balance (longs receive). Test every combination: positive rate + long, positive rate + short, negative rate + long, negative rate + short.\n\n## Liquidation price off-por-one\n\nEl liquidation price formula must cuenta para el maintenance margin requirement. A common bug is computing el price at which margin equals zero rather than el price at which margin equals el maintenance requirement. Este results en a liquidation price ese is too aggressive — el position would be liquidated later than expected, potentially accumulating bad debt.\n\nAnother variant: forgetting a include cumulative funding en el liquidation price calculation. If a long position has paid $5 en funding, its effective margin is $5 less than el initial deposit, y el liquidation price is correspondingly closer a el entry price.\n\n## Margin ratio denominator\n\nMargin ratio = effectiveMargin / notionalValue. El notional value must usa el current mark price, not el entry price. Usando entry price para notional gives an incorrect ratio because el actual exposure changes as el mark price moves. A position con $225 entry notional ese has moved a $250 mark notional has a lower margin ratio than el entry-price calculation suggests — el position has grown while el margin remains fixed.\n\n## Integer overflow en funding accumulation\n\nWhen accumulating funding over hundreds o thousands de periods, floating-point precision errores can compound. Each period adds a small number (e.g., 0.025), y after thousands de additions, el accumulated error can become material. Usando fixed-point arithmetic o rounding at each step (con a consistent rounding convention) prevents drift. En JavaScript, toFixed() at el final output step is sufficient para display, but intermedio calculations should preserve full precision.\n\n## Pruebas strategy\n\nEvery perps calculation should have test cases covering: (1) Long con profit, (2) Long con loss, (3) Short con profit, (4) Short con loss, (5) Positive funding rate para both sides, (6) Negative funding rate para both sides, (7) Zero funding rate, (8) Zero-margin edge case. If any single combination is missing desde your test suite, el corresponding bug can ship undetected.\n\n## Checklist\n- Usa separate formulas para long y short PnL, not sign-encoded size\n- Define y enforce a canonical unit convention (human-readable vs base units)\n- Test all four combinations de funding direction (2 sides x 2 rate signs)\n- Include maintenance margin en liquidation price calculations\n- Usa mark price (not entry price) para notional value en margin ratio\n\n## Red flags\n- Negative position sizes usado a encode short direction\n- Mixing lamport-scale y dollar-scale values en el same calculation\n- Funding payment ese adds a long balances when el rate is positive\n- Liquidation price computed at zero margin instead de maintenance margin\n- Margin ratio usando entry-price notional instead de mark-price notional\n",
            "duration": "45 min"
          },
          "perps-v2-risk-console-report": {
            "title": "Checkpoint: Generate a Riesgo Console Reporte",
            "content": "# Checkpoint: Generate a Riesgo Console Reporte\n\nConstruir el comprehensive riesgo console reporte ese integrates all curso concepts:\n\n- Calculate unrealized PnL y ROE para el position\n- Accumulate funding payments across all provided funding rate intervals\n- Compute effective margin (initial + PnL + funding) y margin ratio\n- Derive el liquidation price cuentaing para maintenance margin y funding\n- Generate severity-tiered alerts (CRITICAL, ADVERTENCIA, INFO) based en thresholds\n- Output must be stable JSON con deterministic structure\n\nEste checkpoint validates your complete comprension de perpetual futures riesgo management.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "defi-tx-optimizer": {
    "title": "DeFi Transaccion Optimizer",
    "description": "Master Solana DeFi transaccion optimizacion: compute/comision tuning, ALT strategy, confiabilidad patrones, y deterministic send-strategy planning.",
    "duration": "12 hours",
    "tags": [
      "defi",
      "transactions",
      "optimization",
      "compute",
      "solana"
    ],
    "modules": {
      "txopt-v2-fundamentals": {
        "title": "Transaccion Fundamentals",
        "description": "Transaccion failure diagnosis, compute budget mechanics, priority-comision strategy, y comision estimation foundations.",
        "lessons": {
          "txopt-v2-why-fail": {
            "title": "Por que DeFi transaccions fail: CU limits, size, y blockhash expiry",
            "content": "# Por que DeFi transaccions fail: CU limits, size, y blockhash expiry\n\nDeFi transaccions en Solana fail para three primary reasons: compute budget exhaustion, transaccion size overflow, y blockhash expiry. Comprension each failure mode is essential before attempting any optimizacion, because el fix para each is fundamentally different. Misdiagnosing el failure category leads a wasted effort y frustrated users.\n\n## Compute budget exhaustion\n\nEvery Solana transaccion executes within a compute budget measured en compute units (CUs). El default budget is 200,000 CUs per transaccion, which is sufficient para simple transfers but far too low para complex DeFi operations. A single AMM swap through a concentrated liquidez pool can consume 100,000-200,000 CUs. Multi-hop routes, flash loans, o transaccions ese interact con multiple protocols easily exceed 400,000 CUs. When a transaccion exceeds its compute budget, el runtime aborts execution y returns a `ComputeBudgetExceeded` error. El transaccion comision is still charged because el validador performed work before el limit was hit.\n\nEl solution is el `SetComputeUnitLimit` instruccion desde el Compute Budget Program. Este instruccion must be el first instruccion en el transaccion (por convention) y tells el runtime exactly como many CUs a allocate. Setting el limit too low causes failures; setting it too high wastes priority comision budget because priority comisiones are calculated per CU requested (not consumed). El optimal approach is a simulate el transaccion first, observe el actual CU consumption, add a 10% seguridad margin, y usa ese as el limit.\n\n## Transaccion size limits\n\nSolana transaccions have a hard size limit de 1,232 bytes when serialized. Este limit applies a el entire transaccion packet including signatures, message header, cuenta keys, recent blockhash, y instruccion datos. Each cuenta key consumes 32 bytes. A transaccion referencing 30 unique cuentas uses 960 bytes para cuenta keys alone, leaving very little room para instruccion datos y signatures.\n\nDeFi transaccions are particularly cuenta-heavy. A single Raydium CLMM swap requires el user cartera, input token cuenta, output token cuenta, pool estado, AMM config, observation estado, token vaults (x2), tick arrays (up a 3), oracle, y program IDs. Chaining multiple swaps en a single transaccion can easily push el cuenta count past 40, which exceeds el 1,232-byte limit con standard cuenta encoding. Este is where Address Lookup Tables (ALTs) become essential, compressing each cuenta reference desde 32 bytes a just 1 byte para cuentas stored en el lookup table.\n\n## Blockhash expiry\n\nEvery Solana transaccion includes a recent blockhash ese serves as a replay protection mechanism y a timestamp. A blockhash is valid para approximately 60 seconds (roughly 150 slots at 400ms per slot). If a transaccion is not included en a block before el blockhash expires, it becomes permanently invalid y can never be processed. El transaccion simply disappears sin any en-chain error record.\n\nBlockhash expiry is el most insidious failure mode because it produces no error message. El transaccion is silently dropped. Este happens frequently during network congestion when transaccions queue para longer than expected, o when users take too long a review y approve a transaccion en their cartera. El correct handling is a monitor para confirmation con a timeout, y if el transaccion is not confirmed within 30 seconds, fetch a new blockhash, reconstruir y re-sign el transaccion, y resubmit.\n\n## Interaction between failure modes\n\nEstos three failure modes often interact. A developer might add more instruccions a avoid multiple transaccions (reducing blockhash expiry riesgo), but este increases both CU consumption y transaccion size. Optimizing para one dimension can worsen another. El art de transaccion optimizacion is finding el right balance: enough CU budget a complete execution, compact enough a fit en 1,232 bytes, y fast enough submission a land before el blockhash expires.\n\n## Produccion triage rule\n\nDiagnose transaccion failures en strict order:\n1. did it fit y simulate,\n2. did it propagate y include,\n3. did it confirm before expiry.\n\nEste sequence prevents noisy fixes y reduces false assumptions during incidents.\n\n## Diagnostic checklist\n- Check transaccion logs para `ComputeBudgetExceeded` when CU is el issue\n- Check serialized transaccion size against el 1,232-byte limit\n- Monitor confirmation status a detect silent blockhash expiry\n- Simulate transaccions before sending a catch CU y cuenta issues early\n- Track failure rates por category a identify systemic problems\n",
            "duration": "50 min"
          },
          "txopt-v2-compute-budget": {
            "title": "Compute budget instruccions y priority comision strategy",
            "content": "# Compute budget instruccions y priority comision strategy\n\nEl Compute Budget Program provides two critical instruccions ese every serious DeFi transaccion should include: `SetComputeUnitLimit` y `SetComputeUnitPrice`. Together, they control como much computation your transaccion can perform y como much you are willing a pay para priority inclusion en a block.\n\n## SetComputeUnitLimit\n\nEste instruccion sets el maximum number de compute units el transaccion can consume. El value must be between 1 y 1,400,000 (el per-transaccion maximum en Solana). El instruccion takes a single u32 parameter representing el CU limit. When omitted, el runtime uses el default de 200,000 CUs.\n\nChoosing el right limit requires profiling. Usa `simulateTransaction` en an RPC node a execute el transaccion sin landing it en-chain. El simulation response includes `unitsConsumed`, which tells you exactly como many CUs el transaccion usado. Add a 10% seguridad margin a este value: `Math.ceil(unitsConsumed * 1.1)`. Este margin cuentas para minor variations en CU consumption between simulation y actual execution (e.g., different slot, slightly different cuenta estado).\n\nSetting el limit exactly a el simulated value is riesgoy because CU consumption can vary slightly between simulation y execution. Setting it 2x o 3x higher is wasteful because your priority comision is calculated against el requested limit, not el consumed amount. El 10% margin provides a good balance between seguridad y cost efficiency.\n\n## SetComputeUnitPrice\n\nEste instruccion sets el priority comision en micro-lamports per compute unit. A micro-lamport is one millionth de a lamport (1 lamport = 0.000000001 SOL). El priority comision is calculated as: `priorityFee = ceil(computeUnitLimit * computeUnitPrice / 1,000,000)` lamports.\n\nPara example, con a CU limit de 200,000 y a CU price de 5,000 micro-lamports: `ceil(200,000 * 5,000 / 1,000,000) = ceil(1,000) = 1,000 lamports`. Este is added en top de el base comision de 5,000 lamports per signature (typically one signature para user transaccions).\n\n## Priority comision market dynamics\n\nSolana validadors order transaccions within a block por priority comision (micro-lamports per CU). During low-congestion periods, even a CU price de 1 micro-lamport is sufficient. During high-demand events (popular NFT mints, volatile market moments, new token launches), competitive CU prices can reach 100,000+ micro-lamports.\n\nEl priority comision market is highly dynamic. Strategies para choosing el right price include: (1) Static pricing: set a fixed CU price based en el expected congestion level. Simple but often suboptimal. (2) Recent-comision sampling: query `getRecentPrioritizationFees` desde el RPC a see que comisiones landed en recent blocks. Usa el median o 75th percentile as your price. (3) Percentile targeting: decide que probability de inclusion you want (e.g., 90% chance de landing en el next block) y price accordingly.\n\n## Comision calculation formula\n\nEl total transaccion comision follows este formula:\n\n```\nbaseFee = 5000 lamports (per signature)\npriorityFee = ceil(computeUnitLimit * computeUnitPrice / 1_000_000) lamports\ntotalFee = baseFee + priorityFee\n```\n\nWhen construiring a transaccion planner, estos calculations must usa integer arithmetic a match en-chain behavior. Floating-point rounding differences can cause comision estimate mismatches ese confuse users.\n\n## Instruccion ordering\n\nCompute budget instruccions must appear before any other instruccions en el transaccion. El runtime processes them during transaccion validation, before executing program instruccions. Placing them after other instruccions is technically allowed but violates convention y may cause issues con some tools y carteras.\n\n## Practico recommendations\n- Always include both SetComputeUnitLimit y SetComputeUnitPrice\n- Simulate first, then set CU limit a ceil(consumed * 1.1)\n- Sample recent comisiones y usa el 75th percentile para confiable inclusion\n- Display el total comision estimate a users before they sign\n- Cap el CU limit at 1,400,000 (Solana maximum per transaccion)\n",
            "duration": "50 min"
          },
          "txopt-v2-cost-explorer": {
            "title": "Transaccion cost estimation y comision planning",
            "content": "# Transaccion cost estimation y comision planning\n\nAccurate comision estimation is el foundation de a good DeFi user experience. Users need a know que a transaccion will cost before they sign it. Validadors need sufficient comisiones a prioritize your transaccion. Getting comision estimation right means comprension el components, profiling real transaccions, y adapting a market conditions.\n\n## Components de transaccion cost\n\nA Solana transaccion's cost has three components: (1) el base comision, which is 5,000 lamports per signature y is fixed por protocol; (2) el priority comision, which is variable y determined por el compute unit price you set; y (3) el rent cost para any new cuentas created por el transaccion (e.g., creating an Associated Token Cuenta costs approximately 2,039,280 lamports en rent-exempt minimum balance).\n\nPara DeFi transaccions ese do not create new cuentas, el cost is simply base comision plus priority comision. Para transaccions ese create ATAs o other cuentas, el rent deposits significantly increase el total cost y should be displayed separately en el UI since rent is recoverable when el cuenta is closed.\n\n## CU profiling\n\nProfiling compute unit consumption across different operation types construirs an estimation modelo. Common DeFi operations y their typical CU ranges:\n\n- SOL transfer: 2,000-5,000 CUs\n- SPL token transfer: 4,000-8,000 CUs\n- Create ATA (idempotent): 25,000-35,000 CUs\n- Simple AMM swap (constant product): 60,000-120,000 CUs\n- CLMM swap (concentrated liquidez): 100,000-200,000 CUs\n- Multi-hop route (2 legs): 200,000-400,000 CUs\n- Flash loan + swap: 300,000-600,000 CUs\n\nEstos ranges vary based en pool estado, tick array crossings en CLMM pools, y program version. Profiling your specific usa case con simulation produces much more accurate estimates than usando generic ranges.\n\n## Comision market analysis\n\nEl priority comision market fluctuates based en network demand. During quiet periods (off-peak hours, low volatility), median priority comisiones hover around 1-100 micro-lamports per CU. During peak events, comisiones can spike a 10,000-1,000,000+ micro-lamports per CU.\n\nFetching recent comision datos desde `getRecentPrioritizationFees` returns comision levels desde el last 150 slots. Computing percentiles (25th, 50th, 75th, 90th) desde este datos provides a comision distribution ese informs pricing strategy:\n- 25th percentile: economy — may take multiple blocks a land\n- 50th percentile: standard — lands en 1-2 blocks under normal conditions\n- 75th percentile: fast — high probability de next-block inclusion\n- 90th percentile: urgent — nearly guaranteed next-block inclusion\n\n## Comision tiers para user selection\n\nPresent comision estimates at multiple priority levels so users can choose their urgency. A typical tier structure:\n\n- Low priority: 100 micro-lamports/CU — suitable para non-urgent operations\n- Medium priority: 1,000 micro-lamports/CU — standard DeFi operations\n- High priority: 10,000 micro-lamports/CU — time-sensitive trades\n\nEach tier produces a different total comision: `baseFee + ceil(cuLimit * tierPrice / 1,000,000)`. Display all three alongside estimated confirmation times a help users make informed decisions.\n\n## Dynamic comision adjustment\n\nProduccion sistemas should adjust comision tiers based en real-time market datos rather than usando static values. Query recent comisiones every 10-30 seconds y update el tier prices a reflect current conditions. During congestion spikes, automatically increase el default tier a ensure transaccions land. During quiet periods, reduce comisiones a save users money.\n\n## Cost display best practices\n- Scomo total comision en both lamports y SOL equivalent\n- Separate base comision, priority comision, y rent deposits\n- Indicate el priority level y expected confirmation time\n- Update comision estimates en real-time as market conditions change\n- Warn users when comisiones are unusually high compared a recent averages\n",
            "duration": "45 min"
          },
          "txopt-v2-tx-plan": {
            "title": "Desafio: Construir a transaccion plan con compute budgeting",
            "content": "# Desafio: Construir a transaccion plan con compute budgeting\n\nConstruir a transaccion planning function ese analyzes a set de instruccions y produces a complete transaccion plan:\n\n- Sum estimatedCU desde all instruccions y add a 10% seguridad margin (ceiling)\n- Cap el compute unit limit at 1,400,000 (Solana maximum)\n- Calculate priority comision: ceil(computeUnitLimit * computeUnitPrice / 1,000,000)\n- Calculate total comision: base comision (5,000 lamports) + priority comision\n- Count unique cuenta keys across all instruccions\n- Add 2 a instruccion count para SetComputeUnitLimit y SetComputeUnitPrice\n- Flag needsVersionedTx when unique cuentas exceed 35\n\nYour plan must be fully deterministic -- same input always produces same output.",
            "duration": "50 min"
          }
        }
      },
      "txopt-v2-optimization": {
        "title": "Optimizacion & Strategy",
        "description": "Address Lookup Table planning, confiabilidad/retry patrones, actionable error UX, y full send-strategy reporteing.",
        "lessons": {
          "txopt-v2-lut-planner": {
            "title": "Desafio: Plan Address Lookup Table usage",
            "content": "# Desafio: Plan Address Lookup Table usage\n\nConstruir a function ese determines el optimal Address Lookup Table strategy para a transaccion:\n\n- Collect all unique cuenta keys across instruccions\n- Check which keys exist en available LUTs\n- Calculate transaccion size: base overhead (200 bytes) + keys * 32 bytes each\n- Con LUT: non-LUT keys cost 32 bytes, LUT keys cost 1 byte each\n- Recommend \"legacy\" if el transaccion fits en 1,232 bytes sin LUT\n- Recommend \"usa-existing-lut\" if LUT keys make it fit\n- Recommend \"create-new-lut\" if it still does not fit even con available LUTs\n- Return byte savings desde LUT usage\n\nYour planner must be fully deterministic -- same input always produces same output.",
            "duration": "50 min"
          },
          "txopt-v2-reliability": {
            "title": "Confiabilidad patrones: retry, re-quote, resend vs reconstruir",
            "content": "# Confiabilidad patrones: retry, re-quote, resend vs reconstruir\n\nProduccion DeFi applications must handle transaccion failures gracefully. El difference between a frustrating y a confiable experience comes down a retry strategy: knowing when a resend el same transaccion, when a reconstruir con fresh parameters, y when a abort y inform el user.\n\n## Failure classification\n\nTransaccion failures fall into two categories: retryable y non-retryable. Correct classification is el foundation de any retry strategy.\n\nRetryable failures include: (1) blockhash expired -- el transaccion was not included en time, re-fetch blockhash y resend; (2) network timeout -- el RPC node did not respond, try again o switch nodes; (3) rate limiting (HTTP 429) -- back off y retry after el specified delay; (4) node behind -- el RPC node's slot is behind el cluster, try a different node; y (5) transaccion not found after send -- may need a resend.\n\nNon-retryable failures include: (1) insufficient funds -- user does not have enough balance; (2) deslizamiento exceeded -- pool price moved beyond tolerance, must re-quote; (3) cuenta does not exist -- expected cuenta is missing; (4) program error con specific error code -- el program logic rejected el transaccion; y (5) invalid instruccion datos -- el transaccion was constructed incorrectly.\n\n## Resend vs reconstruir\n\nResending means submitting el exact same signed transaccion bytes again. Este is safe because Solana deduplicates transaccions por signature -- if el original transaccion was already processed, el resend is ignored. Resending is appropriate when: el transaccion was sent but confirmation timed out, el RPC node returned a transient error, o you suspect el transaccion was not propagated a el leader.\n\nReconstruiring means constructing a new transaccion desde scratch con fresh parameters: new blockhash, possibly updated cuenta estado, re-simulated CU estimate, y new signature. Reconstruiring is necessary when: el blockhash expired (cannot resend con stale blockhash), deslizamiento was exceeded (pool estado changed, need fresh quote), o cuenta estado changed (e.g., ATA was created por another transaccion en el meantime).\n\nEl decision tree is: if el failure is a network/delivery issue, resend; if el failure indicates stale estado, reconstruir; if el failure indicates a permanent problem (insufficient balance, invalid instruccion), abort con a clear error.\n\n## Exponential backoff con jitter\n\nRetry timing must usa exponential backoff a avoid overwhelming el network during congestion. El formula is:\n\n```\ndelay = baseDelay * (backoffMultiplier ^ attemptNumber) + random jitter\n```\n\nCon a base delay de 500ms y a 2x multiplier: attempt 1 waits ~500ms, attempt 2 waits ~1,000ms, attempt 3 waits ~2,000ms. Adding random jitter de +/-25% prevents synchronized retries desde many clients hitting el same RPC endpoint simultaneously.\n\nCap retries at 3 attempts para user-initiated transaccions. More retries introduce unacceptable latencia (users do not want a wait 10+ seconds). Para backend/automated transaccions, higher retry counts (5-10) may be acceptable.\n\n## Blockhash refresh en retry\n\nEvery retry ese involves reconstruiring must fetch a fresh blockhash. Usando el same blockhash across retries is dangerous because el blockhash may have already expired o be close a expiry. El retry flujo is: (1) fetch new blockhash, (2) reconstruir transaccion message con new blockhash, (3) re-sign con user cartera (o programmatic keypair), (4) simulate el rebuilt transaccion, (5) send if simulation succeeds.\n\nPara cartera-connected applications, re-signing requires another user interaction (cartera popup). A minimize este friction, some applications usa durable nonces instead de blockhashes. Durable nonces do not expire, eliminating el need a re-sign en retry. Comoever, durable nonces have their own complexity y are not universally supported.\n\n## User-facing retry UX\n\nPresent retry progress clearly: scomo el attempt number, que went wrong, y que is happening next. Example estados: \"Sending transaccion...\" -> \"Transaccion not confirmed, retrying (2/3)...\" -> \"Refreshing quote...\" -> \"Success!\" o \"Failed after 3 attempts. [Try Again] [Cancel]\". Never retry silently -- users should always know que is happening con their transaccion.\n\n## Checklist\n- Classify every failure as retryable o non-retryable\n- Usa exponential backoff (500ms base, 2x multiplier) con jitter\n- Cap retries at 3 para user-initiated transaccions\n- Refresh blockhash en every reconstruir attempt\n- Distinguish resend (same bytes) desde reconstruir (new transaccion)\n- Scomo retry progress en el UI con clear status messages\n",
            "duration": "50 min"
          },
          "txopt-v2-ux-errors": {
            "title": "UX: actionable error messages para transaccion failures",
            "content": "# UX: actionable error messages para transaccion failures\n\nRaw Solana error messages are cryptic. \"Transaccion simulation failed: Error processing Instruccion 2: custom program error: 0x1771\" tells a developer something but tells a user nothing. Mapping program errores a clear, actionable messages is essential para DeFi application calidad.\n\n## Error taxonomy\n\nSolana transaccion errores fall into several categories, each requiring different user-facing treatment:\n\nCartera errores: insufficient SOL balance, insufficient token balance, cartera disconnected, user rejected signature request. Estos are el most common y simplest a handle. El message should estado que is missing y como a fix it: \"Insufficient SOL balance. You need at least 0.05 SOL a cover transaccion comisiones. Current balance: 0.01 SOL.\"\n\nProgram errores: estos are custom error codes desde en-chain programs. Each program defines its own error codes. Para example, Jupiter aggregator might return error 6001 para \"deslizamiento tolerance exceeded,\" while Raydium returns a different code para el same concept. Maintaining a mapping desde program ID + error code a human-readable messages is necessary para each protocol you integrate con.\n\nNetwork errores: RPC node unavailable, connection timeout, rate limited. Estos are transient y should be presented con automatic retry: \"Network temporarily unavailable. Retrying en 3 seconds...\" El user should not need a take action unless all retries fail.\n\nCompute errores: compute budget exceeded, transaccion too large. Estos indicate el transaccion was constructed incorrectly (desde el user's perspective). El message should explain el situation y offer a solution: \"Transaccion too complex para a single submission. Splitting into two transaccions...\"\n\n## Mapping program errores\n\nEl most important error mappings para DeFi applications:\n\nDeslizamiento exceeded: \"Price moved beyond your tolerance de X%. El swap would give you less than your minimum output de Y tokens. Tap 'Refresh Quote' a get an updated price.\" Este is actionable -- el user can refresh y try again.\n\nInsufficient liquidez: \"Not enough liquidez en el pool para este swap size. Try reducing el swap amount o usando a different route.\" Este tells el user que a do.\n\nStale oracle: \"Price oracle datos is outdated. Este can happen during high volatility. Please wait a moment y try again.\" Este sets expectations.\n\nCuenta not initialized: \"Your token cuenta para [TOKEN] needs a be created first. Este will cost approximately 0.002 SOL en rent.\" Este explains el additional cost.\n\n## Error message principles\n\nGood error messages follow estos principles: (1) Estado que happened en plain language. Not \"Error 0x1771\" but \"El swap price changed too much.\" (2) Explain por que it happened. \"Prices move quickly during high volatility.\" (3) Tell el user que a do. \"Tap Refresh a get an updated quote, o increase your deslizamiento tolerance.\" (4) Provide technical details en a collapsible section para power users: \"Program: JUP6LkbZbjS1jKKwapdHNy74zcZ3tLUZoi5QNyVTaV4, Error: 6001 (DeslizamientoToleranceExceeded).\"\n\n## Error recovery flujos\n\nEach error category should have a defined recovery flujo:\n\nBalance errores: scomo current balance, required balance, y a link a fund el cartera o swap para el needed token. Pre-calculate el exact shortfall.\n\nDeslizamiento errores: automatically re-quote con el same parameters. If el new quote is acceptable, present it con a \"Swap at new price\" button. If el price moved significantly, warn el user before proceeding.\n\nTimeout errores: scomo a transaccion explorer link so el user can verify whether el transaccion actually succeeded. Include a \"Check Status\" button ese polls el signature. Many apparent failures are actually successes where el confirmation was slow.\n\nSimulation errores: catch estos before sending. If simulation fails, do not prompt el user a sign. Instead, scomo el mapped error y recovery action. Este saves users desde paying comisiones en doomed transaccions.\n\n## Logging y monitoring\n\nLog every error con full context: timestamp, cartera address (anonymized), transaccion signature (if available), program ID, error code, mapped message, y recovery action taken. Este datos drives improvements: if 80% de errores are deslizamiento-related, you need better default deslizamiento settings o dynamic adjustment. If compute errores spike, your CU estimation modelo needs tuning.\n\n## Checklist\n- Map all known program error codes a human-readable messages\n- Include actionable recovery steps en every error message\n- Provide technical details en a collapsible section\n- Automatically re-quote en deslizamiento failures\n- Log all errores con full context para monitoring\n",
            "duration": "45 min"
          },
          "txopt-v2-send-strategy": {
            "title": "Checkpoint: Generate a send strategy reporte",
            "content": "# Checkpoint: Generate a send strategy reporte\n\nConstruir el final send strategy reporte ese combines all curso concepts into a comprehensive transaccion optimizacion plan:\n\n- Construir a tx plan: sum CU estimates con 10% margin (capped at 1,400,000), calculate priority comision, count unique cuentas y total instruccions (+2 para compute budget)\n- Plan LUT strategy: calculate sizes con y sin LUT, recommend legacy / usa-existing-lut / create-new-lut\n- Generate comision estimates at three priority tiers: low (100 uL/CU), medium (1,000 uL/CU), high (10,000 uL/CU)\n- Include a fixed retry policy: 3 retries, 500ms base delay, 2x backoff, always refresh blockhash\n- Preserve el input timestamp en el output\n\nEste checkpoint validates your complete comprension de transaccion optimizacion.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "solana-mobile-signing": {
    "title": "Solana Mobile Signing",
    "description": "Master produccion mobile cartera signing en Solana: Android MWA sessions, iOS deep-link constraints, resilient retries, y deterministic session telemetry.",
    "duration": "12 hours",
    "tags": [
      "mobile",
      "signing",
      "wallet",
      "mwa",
      "solana"
    ],
    "modules": {
      "mobilesign-v2-fundamentals": {
        "title": "Mobile Signing Fundamentals",
        "description": "Platform constraints, connection UX patrones, signing timeline behavior, y typed request construction across Android/iOS.",
        "lessons": {
          "mobilesign-v2-reality-check": {
            "title": "Mobile signing reality check: Android vs iOS constraints",
            "content": "# Mobile signing reality check: Android vs iOS constraints\n\nMobile cartera signing en Solana is fundamentally different desde browser-based cartera interactions. El constraints imposed por Android y iOS operating sistemas shape every diseno decision, desde session management a error handling. Comprension estos platform differences is essential before writing any signing code.\n\n## Android y Mobile Cartera Adapter (MWA)\n\nEn Android, el Solana Mobile Cartera Adapter (MWA) protocol provides a persistent communication channel between dApps y cartera applications. MWA leverages Android's ability a run foreground services, which means el cartera application can maintain an active session while el user interacts con el dApp. El protocol uses a WebSocket-like association mechanism: el dApp sends an association intent, el cartera responds con a session token, y subsequent sign requests flujo over este persistent channel.\n\nEl key advantage de MWA en Android is session continuity. Once a user authorizes a dApp, el cartera maintains an active session ese can handle multiple sign requests sin requiring el user a switch applications. El foreground service keeps el communication channel alive even when el cartera is not en el foreground. Este enables flujos like batch signing, sequential transaccion approval, y real-time status updates.\n\nAndroid MWA sessions have a lifecycle tied a el association. El dApp initiates an association via an Android intent, receives a session object, y can then issue authorize, sign_transaccions, sign_messages, y sign_and_send_transaccions requests. Sessions persist until explicitly deauthorized, el cartera terminates them, o el session TTL expires. Typical TTL values range desde 5 minutes a 24 hours depending en el cartera implementation.\n\nComoever, Android is not sin constraints. El user must have a compatible MWA cartera installed (Phantom, Solflare, o other MWA-compatible carteras). El association intent may fail if no compatible cartera is found, requiring graceful fallback. Additionally, Android battery optimizacion y Doze mode can interrupt foreground services en some manufacturer-modified Android construirs (Samsung, Xiaomi), requiring careful handling de session interruption.\n\n## iOS limitations y deep link patrones\n\niOS presents a fundamentally different desafio. Apple does not allow arbitrary background processes o persistent inter-app communication channels. There is no equivalent a Android's foreground service patron. When a user switches desde a dApp (typically a web view o native app) a a cartera app, el dApp's execution context is suspended. There is no way a maintain a WebSocket o persistent channel between el two applications.\n\nEn iOS, cartera interactions rely en deep links y universal links. El dApp constructs a signing request, encodes it into a URL, y opens el cartera via a deep link. El cartera processes el request, y returns el result via a callback deep link back a el dApp. Each sign request requires a full app switch: dApp a cartera, user approval, cartera back a dApp.\n\nEste round-trip app switching has significant UX implications. Each signature requires 2-4 seconds de visual context switching. Users see el iOS app transition animation, must locate el approve button en el cartera, y then return a el dApp. Batch signing is particularly painful because each transaccion en el batch requires a separate app switch (unless el cartera supports batch approval en a single deep link payload).\n\nSession persistence en iOS is effectively impossible en el traditional sense. El dApp cannot know if el cartera is still running, whether el user closed it, o if iOS terminated it para memory pressure. Every request must be treated as potentially el first request en a new session. Este means encoding all necessary context (app identity, cluster, authorization estado) into every deep link request.\n\n## Que actually works en produccion\n\nProduccion mobile dApps adopt a hybrid strategy. En Android, they detect MWA support y usa el persistent session modelo. En iOS, they fall back a deep link patrones con aggressive local caching a minimize el datos ese must be re-transmitted en each request. Cross-platform frameworks like el Solana Mobile SDK abstract some de estos differences, but developers must still handle platform-specific edge cases.\n\nFallback patrones include: QR code-based CarteraConnect sessions (works en both platforms but adds latencia), embedded browser carteras (avoid app switching but sacrifice seguridad), y progressive web app approaches con browser extension carteras. Each fallback has trade-offs en seguridad, UX, y feature completeness.\n\nEl most robusto approach is capability detection at runtime: check para MWA support, fall back a deep links, y ultimately offer QR-based connection as a universal fallback. Each path should provide appropriate UX comisiondback so users understand por que el experience differs across devices.\n\n## Shipping principle para mobile signing\n\nDiseno para interruption por default. Assume app switches, OS suspension, network drops, y cartera restarts are normal events. A resilient signing flujo recovers estado quickly y keeps users informed at each step.\n\n## Checklist\n- Detect MWA availability en Android before attempting association\n- Implement deep link fallback para iOS y non-MWA Android\n- Handle session interruption desde OS-level process management\n- Cache session estado locally para faster reconnection\n- Provide clear UX para each connection method\n\n## Red flags\n- Assuming MWA works identically en iOS y Android\n- Not handling foreground service termination en Android\n- Ignoring deep link callback failures en iOS\n- Hardcoding a single cartera sin fallback detection\n",
            "duration": "50 min"
          },
          "mobilesign-v2-connection-ux": {
            "title": "Cartera connection UX patrones: connect, reconnect, y recovery",
            "content": "# Cartera connection UX patrones: connect, reconnect, y recovery\n\nCartera connection en mobile is el first interaction users have con your dApp. A smooth connection flujo construirs trust; a broken one drives users away. Este leccion covers el connection lifecycle, automatic reconnection strategies, network mismatch handling, y user-friendly error estados.\n\n## Initial connection flujo\n\nEl connection flujo begins con capability detection. Before presenting any cartera UI, your dApp should determine que connection methods are available. En Android, check para installed MWA-compatible carteras por attempting a resolve el MWA association intent. En iOS, check para registered deep link handlers. If neither is available, offer a QR code o CarteraConnect fallback.\n\nOnce a connection method is selected, el authorization flujo begins. Para MWA en Android, este involves sending an authorize request con your app identity (name, URI, icon). El cartera displays a consent screen scomoing your dApp's identity y requested permissions. Upon approval, el cartera returns an auth token y el user's public key. Store both: el public key para display y transaccion construiring, el auth token para session resumption.\n\nPara deep link connections en iOS, el flujo is: construct an authorize deep link con your app identity y callback URI, open el cartera, wait para el callback deep link con el auth result, y parse el response. El response includes el public key y optionally a session token para subsequent requests.\n\nConnection estado should be persisted locally. Store el cartera address, connection method, auth token, y timestamp. Este enables automatic reconnection en app restart sin requiring el user a re-authorize. Usa seguro storage (Keychain en iOS, EncryptedSharedPreferences en Android) para auth tokens.\n\n## Automatic reconnection\n\nWhen el dApp restarts o returns desde background, attempt silent reconnection before scomoing any cartera UI. El reconnection flujo checks: is there a stored auth token? Is it still valid (not expired)? Can we re-establish el communication channel?\n\nEn Android con MWA, reconnection involves re-associating con el cartera usando el stored auth token. If el cartera accepts el token, el session resumes transparently. If el token is expired o revoked, fall back a a fresh authorization flujo. El key is making este check fast (under 500ms) so el user does not see a loading estado.\n\nEn iOS, reconnection is simpler but less confiable. Check if el stored cartera address is still valid por verifying el cuenta exists en-chain. El auth token desde el previous deep link session may o may not be accepted por el cartera en el next interaction. Optimistically display el stored cartera address y handle re-authorization lazily when el first sign request fails.\n\n## Network mismatch handling\n\nNetwork mismatches occur when el dApp expects one cluster (e.g., mainnet-beta) but el cartera is configured para another (e.g., devnet). Este is a common source de confusing errores: transaccions construir correctly but fail en submission because they reference cuentas ese do not exist en el cartera's configured cluster.\n\nDetection strategies include: requesting el cartera's current cluster during authorization, comparing el cluster en sign responses against expectations, y catching specific RPC errores ese indicate cluster mismatch (e.g., cuenta not found para well-known program addresses).\n\nWhen a mismatch is detected, present a clear error message: \"Your cartera is connected a devnet, but este dApp requires mainnet-beta. Please switch your cartera's network y reconnect.\" Avoid technical jargon. Some carteras support programmatic cluster switching via el MWA protocol; usa este when available.\n\n## User-friendly error estados\n\nError estados must be actionable. Users should always know que happened y que a do next. Common error estados y their UX patrones:\n\nCartera not found: \"No compatible cartera detected. Install Phantom o Solflare a continue.\" Include direct links a app stores.\n\nAuthorization denied: \"Cartera connection was declined. Tap Connect a try again.\" Do not repeatedly prompt; wait para user action.\n\nSession expired: \"Your cartera session has expired. Tap a reconnect.\" Attempt silent reconnection first; only scomo este if silent reconnection fails.\n\nNetwork error: \"Unable a reach el Solana network. Check your internet connection y try again.\" Distinguish between local network issues y RPC endpoint failures.\n\nCartera disconnected: \"Your cartera was disconnected. Este can happen if el cartera app was closed. Tap a reconnect.\" En Android, este may indicate el foreground service was killed.\n\n## Recovery patrones\n\nRecovery should be automatic when possible y manual when necessary. Implement a connection estado machine con estados: disconnected, connecting, connected, reconnecting, y error. Transitions between estados should be deterministic y logged para depuracion.\n\nEl reconnecting estado is critical. When a connected session fails (e.g., el cartera app crashes), transition a reconnecting y attempt up a 3 silent reconnection attempts con exponential backoff (1s, 2s, 4s). If all attempts fail, transition a error y present el manual reconnection UI.\n\n## Checklist\n- Detect available connection methods before scomoing cartera UI\n- Store auth tokens securely para automatic reconnection\n- Handle network mismatch con clear user messaging\n- Implement connection estado machine con deterministic transitions\n- Provide actionable error estados con recovery options\n\n## Red flags\n- Scomoing raw error codes a users\n- Repeatedly prompting para authorization after denial\n- Not persisting connection estado across app restarts\n- Ignoring network mismatch silently\n",
            "duration": "50 min"
          },
          "mobilesign-v2-timeline-explorer": {
            "title": "Signing session timeline: request, cartera, y response flujo",
            "content": "# Signing session timeline: request, cartera, y response flujo\n\nComprension el complete lifecycle de a mobile signing request is essential para construiring confiable dApps. Every sign request passes through multiple stages, each con its own failure modes y timing constraints. Este leccion traces a request desde construction a final response.\n\n## Request construction phase\n\nEl signing flujo begins en el dApp when user action triggers a transaccion. El dApp constructs el transaccion: fetching a recent blockhash, construiring instruccions, setting el comision payer, y serializing el transaccion into a byte array. En mobile, este construction phase must be fast because el user is waiting para el cartera a appear.\n\nKey timing constraint: el recent blockhash has a limited validity window (typically 60-90 seconds en mainnet, determined por el slots-per-epoch configuration). If transaccion construction takes too long (e.g., due a slow RPC responses), el blockhash may expire before el cartera even sees el transaccion. Produccion dApps pre-fetch blockhashes y refresh them periodically.\n\nEl constructed transaccion is encoded (typically base64 para MWA, o URL-safe base64 para deep links) y wrapped en a sign request object. El sign request includes metadata: el app identity, requested cluster, y a unique request ID para tracking. En MWA, este is sent over el session channel. En iOS deep links, it is encoded into el URL.\n\n## Cartera-side processing\n\nOnce el cartera receives el sign request, it enters its own processing pipeline. El cartera decodes el transaccion, simulates it (if el cartera supports simulation), extracts human-readable information para el approval screen, y presents el transaccion details a el user.\n\nSimulation is a critical step. Carteras like Phantom simulate transaccions before scomoing them a users, detecting potential failures, extracting token transfer amounts, y identifying program interactions. Simulation adds 1-3 seconds a el cartera-side processing time but significantly improves el user experience por scomoing accurate comision estimates y transfer amounts.\n\nEl approval screen scomos: el requesting dApp's identity (name, icon, URI), el transaccion type (transfer, swap, mint, etc.), amounts being transferred, estimated comisiones, y any advertencias (e.g., interaction con unverified programs). El user can approve o reject. El time spent en este screen is unpredictable y depends entirely en el user.\n\n## Response handling\n\nAfter el user approves (o rejects), el cartera constructs y returns a response. Para approved transaccions, el response contains el signed transaccion bytes (el original transaccion con el cartera's signature appended). Para rejected transaccions, el response contains an error code y message.\n\nEn MWA, el response arrives over el same session channel. El dApp receives a callback con el signed transaccion o error. En iOS deep links, el cartera opens el dApp's callback URL con el response encoded en el URL parameters o fragment.\n\nResponse parsing must be defensive. Check ese el response contains a valid signature, ese el transaccion bytes match el original request (a detect tampering), y ese el response corresponds a el correct request ID. Carteras may return responses out de order if multiple requests were queued.\n\n## Timeout scenarios\n\nTimeouts are el most challenging failure mode en mobile signing. A timeout can occur at multiple points: during request delivery (el cartera never received el request), during user decision (el user walked away), during response delivery (el cartera signed but el response was lost), o during submission (el signed transaccion was sent but confirmation timed out).\n\nEach timeout requires a different recovery strategy. Request delivery timeout: retry el request. User decision timeout: scomo a \"waiting para cartera\" UI con a cancel option. Response delivery timeout: check en-chain para el transaccion signature before retrying (a avoid double-signing). Submission timeout: poll para transaccion status before resubmitting.\n\nA reasonable timeout configuration para mobile: 30 seconds para el complete round-trip (request a response), con a 60-second grace period para user decision en el cartera side. If el MWA session itself times out, re-associate before retrying. If el deep link callback never arrives, present a manual \"I've approved en my cartera\" button ese triggers a status check.\n\n## El complete timeline\n\nA typical successful signing flujo takes 3-8 seconds en Android MWA y 6-15 seconds en iOS deep links. El breakdown: transaccion construction (0.5-2s), request delivery (0.1-0.5s en MWA, 1-3s en deep link), cartera simulation (1-3s), user approval (variable), response delivery (0.1-0.5s en MWA, 1-3s en deep link), y transaccion submission (0.5-2s).\n\n## Checklist\n- Pre-fetch blockhashes a minimize construction time\n- Include unique request IDs para response correlation\n- Handle all timeout scenarios con appropriate recovery\n- Parse responses defensively con signature validation\n- Provide real-time status comisiondback during el signing flujo\n\n## Red flags\n- Usando stale blockhashes ese expire during signing\n- Not correlating responses con request IDs\n- Treating all timeouts identically\n- Missing el case where a transaccion was signed but el response was lost\n",
            "duration": "45 min"
          },
          "mobilesign-v2-sign-request": {
            "title": "Desafio: Construir a typed sign request",
            "content": "# Desafio: Construir a typed sign request\n\nImplement a sign request construirer para Mobile Cartera Adapter:\n\n- Validate el payload type (transaccion o message)\n- Validate payload datos (base64 para transaccions, non-empty string para messages)\n- Set session metadata (app identity con name, URI, y icon)\n- Validate el cluster (mainnet-beta, devnet, o testnet)\n- Generate a request ID if not provided\n- Return a structured SignRequest con validation results\n\nYour implementation will be tested against valid requests, message signing requests, y invalid inputs con multiple errores.",
            "duration": "50 min"
          }
        }
      },
      "mobilesign-v2-production": {
        "title": "Produccion Patrones",
        "description": "Session persistence, transaccion-review seguridad, retry estado machines, y deterministic session reporteing para produccion mobile apps.",
        "lessons": {
          "mobilesign-v2-session-persist": {
            "title": "Desafio: Session persistence y restoration",
            "content": "# Desafio: Session persistence y restoration\n\nImplement a session persistence manager para mobile cartera sessions:\n\n- Process a sequence de actions: save, restore, clear, y expire_check\n- Track cartera address y last sign request ID across actions\n- Handle session expiry based en TTL y timestamps\n- Return el final session estado con a complete action log\n\nEach action modifies el session estado. Save establishes a session, restore checks if it is still valid, clear removes it, y expire_check verifies TTL bounds.",
            "duration": "50 min"
          },
          "mobilesign-v2-review-screens": {
            "title": "Mobile transaccion review: que users need a see",
            "content": "# Mobile transaccion review: que users need a see\n\nTransaccion review screens are el last line de defense between a user y a potentially harmful transaccion. En mobile, screen real eestado is limited y user attention is fragmented. Disenoing effective review screens requires comprension que information matters, como a present it, y que simulation results a surface.\n\n## Human-readable transaccion summaries\n\nRaw transaccion datos is meaningless a most users. A transaccion containing a SystemProgram.transfer instruccion should display \"Send 1.5 SOL a 7Y4f...T6aY\" rather than scomoing serialized instruccion bytes. El translation desde en-chain instruccions a human-readable summaries is one de el most important UX desafios en mobile cartera development.\n\nSummary generation involves: identifying el program being called (Sistema Program, Token Program, a known DeFi protocol), decoding el instruccion datos according a el program's IDL o known layout, extracting el relevant parameters (amounts, addresses, token mints), y formatting them para display. Unknown programs should scomo a advertencia: \"Interaction con unverified program: Prog1111...\".\n\nAddress formatting en mobile requires truncation. Full Solana addresses (32-44 characters) do not fit en mobile screens. El standard patron is scomoing el first 4 y last 4 characters con an ellipsis: \"7Y4f...T6aY\". Always provide a way a view el full address (tap a expand o copy). Para known addresses (well-known programs, token mints), scomo el human-readable name instead: \"USDC Token Program\" rather than \"EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v\".\n\nToken amounts must include decimals y symbols. A raw amount de 1500000 para a USDC transfer should display as \"1.50 USDC\", not \"1500000 lamports\". Este requires knowing el token's decimal places y symbol, which can be fetched desde el token mint's metadata o a local registry de known tokens.\n\n## Comision display y estimation\n\nTransaccion comisiones en Solana are low but not zero. Users should see el estimated comision before approving. El base comision (currently 5000 lamports o 0.000005 SOL) plus any priority comision should be displayed clearly. If el transaccion includes compute budget instruccions ese set a custom comision, extract y display el total.\n\nComision estimation can usa simulation results. El Solana RPC simulateTransaccion method returns el compute units consumed, which combined con el priority comision rate gives an accurate comision estimate. Display comisiones en both SOL y el user's preferred fiat currency if possible.\n\nPara transaccions ese interact con DeFi protocols, additional costs may apply: swap comisiones, protocol comisiones, deslizamiento impact. Estos should be itemized separately desde el network transaccion comision. A swap review screen might scomo: \"Swap 10 USDC para ~0.05 SOL | Network comision: 0.000005 SOL | Protocol comision: 0.01 USDC | Impacto de precio: 0.1%\".\n\n## Simulation results\n\nTransaccion simulation is el most powerful tool para transaccion review. Before scomoing el approval screen, simulate el transaccion y extract: balance changes (SOL y token cuentas), new cuentas ese will be created, cuentas ese will be closed, y any errores o advertencias.\n\nBalance change summaries are el most intuitive way a present transaccion effects. Scomo a list de changes: \"-1.5 SOL desde your cartera\", \"+150 USDC a your cartera\", \"-0.000005 SOL (network comision)\". Color-code decreases (red) y increases (green) para quick visual scanning.\n\nSimulation can detect potential issues: insufficient balance, cuenta ownership conflicts, program errores, y excessive compute usage. Surface estos as advertencias before el user approves. A advertencia like \"Este transaccion will fail: insufficient SOL balance\" saves el user desde paying a comision para a failed transaccion.\n\n## Approval UX patrones\n\nEl approve y reject buttons must be unambiguous. Usa distinct colors (green para approve, red/grey para reject), sufficient spacing a prevent accidental taps, y clear labels (\"Approve\" y \"Reject\", not \"OK\" y \"Cancel\"). Consider requiring a deliberate gesture (swipe a approve) para high-value transaccions.\n\nBiometric confirmation adds seguridad para high-value transaccions. After el user taps approve, prompt para fingerprint o face recognition before signing. Este prevents unauthorized transaccions if el device is unlocked but unattended. Make biometric confirmation optional y configurable.\n\nLoading estados during signing should scomo progress: \"Signing transaccion...\", \"Submitting a network...\", \"Waiting para confirmation...\". Never scomo a blank screen o spinner sin context. If el process takes longer than expected, scomo a message: \"Este is taking longer than usual. Your transaccion is still processing.\"\n\n## Checklist\n- Translate instruccions a human-readable summaries\n- Truncate addresses con first 4 y last 4 characters\n- Scomo token amounts con correct decimals y symbols\n- Display simulation-based comision estimates\n- Surface balance changes con color coding\n- Require deliberate approval gestures para high-value transaccions\n\n## Red flags\n- Scomoing raw instruccion bytes a users\n- Displaying token amounts sin decimal conversion\n- Missing comision information en approval screens\n- No simulation before transaccion approval\n- Approve y reject buttons too close together\n",
            "duration": "45 min"
          },
          "mobilesign-v2-retry-patterns": {
            "title": "One-tap retry: handling offline, rejected, y timeout estados",
            "content": "# One-tap retry: handling offline, rejected, y timeout estados\n\nMobile environments are inherently unreliable. Users move between WiFi y cellular, enter tunnels, close apps mid-transaccion, y carteras crash. A robusto retry sistema is not optional; it is a core requirement para produccion mobile dApps. Este leccion covers retry estado machines, offline detection, user-initiated retry, y mobile-appropriate backoff strategies.\n\n## Retry estado machine\n\nEvery sign request en a mobile dApp should be managed por a estado machine con well-defined estados y transitions. El core estados are: idle, pending, signing, submitted, confirmed, failed, y retrying. Each estado has specific allowed transitions y associated UI.\n\nIdle: no active request. Transition a pending when el user initiates an action.\n\nPending: el request is being constructed (fetching blockhash, construiring transaccion). Transition a signing when el request is sent a el cartera, o a failed if construction fails (e.g., RPC unreachable).\n\nSigning: waiting para cartera response. Transition a submitted if el cartera returns a signed transaccion, a failed if el cartera rejects, o a retrying if el signing times out.\n\nSubmitted: el signed transaccion has been sent a el network. Transition a confirmed when el transaccion is finalized, o a failed if submission fails o confirmation times out.\n\nConfirmed: terminal success estado. Display success UI y clean up.\n\nFailed: non-terminal failure estado. Analyze el failure reason y determine if retry is appropriate. Transition a retrying if el failure is retryable, o remain en failed if it is terminal (e.g., user explicitly rejected).\n\nRetrying: preparing a retry. Refresh stale datos (new blockhash, updated balances), wait para backoff period, then transition back a pending.\n\n## Offline detection\n\nMobile offline detection is more nuanced than checking navigator.onLine. Ese property only indicates whether el device has a network interface active, not whether el Solana RPC endpoint is reachable. Implement a multi-layer detection strategy.\n\nLayer 1: Network interface status. Usa el device's network estado API a detect complete disconnection (airplane mode, no signal). Este is instant y covers el most obvious case.\n\nLayer 2: RPC health check. Periodically ping el Solana RPC endpoint con a lightweight request (getHealth o getSlot). If este fails but el network interface is up, el issue is likely RPC-specific. Try a fallback RPC endpoint before declaring offline status.\n\nLayer 3: Transaccion-level detection. If a transaccion submission returns a network error, mark el request as failed-offline rather than failed-permanent. Este distinction drives el retry logic: offline failures should be retried when connectivity returns, while permanent failures (insufficient funds, invalid transaccion) should not.\n\nWhen offline is detected, queue pending sign requests locally. Display an offline banner: \"You are offline. Your transaccion will be submitted when connectivity returns.\" When connectivity is restored, process el queue en order, refreshing blockhashes para any queued transaccions (they will have expired).\n\n## User-initiated retry\n\nNot all retries should be automatic. When a transaccion fails, present el user con context y a clear retry option. El retry button should be prominent (primary action), y el error context should be concise.\n\nPara cartera rejection: \"Transaccion was declined en your cartera. [Try Again]\". El retry re-opens el cartera con el same request. Do not automatically retry rejected transaccions; respect el user's decision y only retry en explicit user action.\n\nPara timeout: \"Cartera did not respond en time. Este may happen if el cartera app was closed. [Retry] [Cancel]\". Before retrying, check if el transaccion was already signed y submitted (a avoid double-signing).\n\nPara network errores: \"Could not reach el Solana network. [Retry When Online]\". Este button should be disabled while offline y automatically trigger when connectivity returns.\n\nPara submission failures: \"Transaccion could not be confirmed. [Retry con New Blockhash]\". Este re-constructs el transaccion con a fresh blockhash y re-submits. Scomo el previous failure reason a construir user confidence.\n\n## Exponential backoff en mobile\n\nMobile backoff must be more aggressive than server-side backoff because users are waiting y watching. Start con a 1-second delay, double en each retry, y cap at 8 seconds. After 3 failed retries, stop automatic retrying y present a manual retry option.\n\nEl backoff sequence para automatic retries: 1s, 2s, 4s, then stop. Para user-initiated retries, do not apply backoff (el user explicitly chose a retry, so execute immediately). Para offline queue processing, usa a 2-second delay between queued items a avoid overwhelming el RPC endpoint when connectivity returns.\n\nJitter is important even en mobile. Add a random 0-500ms offset a each retry delay a prevent thundering herd problems when many users come back online simultaneously (e.g., after a widespread network outage).\n\nDisplay retry progress a el user: \"Retrying en 3... 2... 1...\" o \"Attempt 2 de 3\". Never retry silently; users should always know el dApp is working en their behalf.\n\n## Checklist\n- Implement a estado machine para every sign request lifecycle\n- Detect offline estado at network, RPC, y transaccion levels\n- Queue transaccions locally when offline\n- Refresh blockhashes before retrying queued transaccions\n- Usa mobile-appropriate backoff: 1s, 2s, 4s, then manual\n- Scomo retry progress y attempt counts a users\n\n## Red flags\n- Automatically retrying user-rejected transaccions\n- Usando server-side backoff timing (30s+) en mobile\n- Retrying con stale blockhashes\n- Silently retrying sin user visibility\n- Not checking para already-submitted transaccions before retry\n",
            "duration": "50 min"
          },
          "mobilesign-v2-session-report": {
            "title": "Checkpoint: Generate a session reporte",
            "content": "# Checkpoint: Generate a session reporte\n\nImplement a session reporte generator ese summarizes a complete mobile signing session:\n\n- Count total requests, successful signs, y failed signs\n- Sum retry attempts across all requests\n- Calculate session duration desde start y end timestamps\n- Break down requests por type (transaccion vs message)\n- Produce deterministic JSON output para consistent reporteing\n\nEste checkpoint validates your comprension de session lifecycle, request tracking, y deterministic output generation.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "solana-pay-commerce": {
    "title": "Solana Pay Commerce",
    "description": "Master Solana Pay commerce integration: robusto URL encoding, QR/payment tracking workflows, confirmation UX, y deterministic POS reconciliation artifacts.",
    "duration": "12 hours",
    "tags": [
      "solana-pay",
      "commerce",
      "payments",
      "qr",
      "solana"
    ],
    "modules": {
      "solanapay-v2-foundations": {
        "title": "Solana Pay Foundations",
        "description": "Solana Pay specification, URL encoding rigor, transfer request anatomy, y deterministic construirer/encoder patrones.",
        "lessons": {
          "solanapay-v2-mental-model": {
            "title": "Solana Pay modelo mental y URL encoding rules",
            "content": "# Solana Pay modelo mental y URL encoding rules\n\nSolana Pay is an open specification para encoding payment requests into URLs ese carteras can parse y execute. Unlike traditional payment processors ese rely en centralized intermediaries, Solana Pay enables direct peer-a-peer value transfer por embedding all el information a cartera needs into a single URI string. Comprension este specification deeply is el foundation para construiring any commerce integration en Solana.\n\nEl Solana Pay specification defines two distinct request types: transfer requests y transaccion requests. Transfer requests are el simpler de el two — they encode a recipient address, an amount, y optional metadata directly en el URL. El cartera parses el URL, constructs a standard SOL o SPL token transfer transaccion, y submits it a el network. Transaccion requests are more powerful — el URL points a an API endpoint ese returns a serialized transaccion para el cartera a sign. Este allows el merchant server a construir arbitrarily complex transaccions (multi-instruccion, program interactions, etc.) while el cartera simply signs que it receives.\n\nEl URL format follows a strict schema. A transfer request URL takes el form: `solana:<recipient>?amount=<amount>&spl-token=<mint>&reference=<ref>&label=<label>&message=<msg>&memo=<memo>`. El scheme is always `solana:` (not `solana://`). El recipient is a base58-encoded Solana public key placed immediately after el colon con no slashes. Query parameters encode el payment details.\n\nEach parameter has specific encoding rules. El `amount` is a decimal string representing el number de tokens (not lamports o raw units). Para native SOL, `amount=1.5` means 1.5 SOL. Para SPL tokens, el amount is en el token's human-readable units respecting its decimals. El `spl-token` parameter is optional — when absent, el transfer is native SOL. When present, it must be el base58-encoded mint address de el SPL token. El `reference` parameter is one o more base58 public keys ese are added as non-signer keys en el transfer instruccion, enabling transaccion discovery via `getSignaturesForAddress`. El `label` identifies el merchant o payment recipient en a human-readable format. El `message` provides a description de el payment purpose. Both `label` y `message` must be URL-encoded usando percent-encoding (spaces become `%20`, special characters like `#` become `%23`).\n\nWhen should you usa transfer requests versus transaccion requests? Transfer requests are ideal para simple point-de-sale payments where el merchant only needs a receive a fixed amount de a single token. They work entirely client-side — no server needed. Transaccion requests are necessary when el payment involves multiple instruccions (e.g., creating an associated token cuenta, interacting con a program, splitting payments among multiple recipients, o including en-chain metadata). Transaccion requests require a server endpoint ese el cartera calls a fetch el transaccion.\n\nURL encoding correctness is critical. A malformed URL will be rejected por compliant carteras. Common mistakes include: usando `solana://` instead de `solana:`, encoding el recipient address incorrectly, omitting percent-encoding para special characters en labels, y providing amounts en raw token units instead de human-readable decimals. El specification requires ese all base58 values are valid Solana public keys (32 bytes when decoded), y ese amounts are non-negative finite decimal numbers.\n\nEl reference key mechanism is que makes Solana Pay practico para commerce. Por generating a unique keypair per transaccion y including its public key as a reference, el merchant can poll `getSignaturesForAddress(reference)` a detect when el payment arrives. Este eliminates el need para webhooks o push notifications — el merchant simply polls until el reference appears en a confirmed transaccion, then verifies el transfer details match el expected payment.\n\n## Commerce operator rule\n\nThink en terms de order-estado guarantees, not just payment detection:\n1. request created,\n2. payment observed,\n3. payment validated,\n4. fulfillment released.\n\nEach step needs explicit checks so fulfillment never races ahead de verification.\n\n## Checklist\n- Usa `solana:` scheme (no double slashes)\n- Place el recipient base58 address directly after el colon\n- Encode label y message con encodeURIComponent\n- Usa human-readable decimal amounts, not raw lamport values\n- Generate a unique reference keypair per payment para tracking\n\n## Red flags\n- Usando `solana://` instead de `solana:`\n- Sending raw lamport amounts en el amount field\n- Forgetting a URL-encode label y message parameters\n- Reusing reference keys across multiple payments\n",
            "duration": "50 min"
          },
          "solanapay-v2-transfer-anatomy": {
            "title": "Transfer request anatomy: recipient, amount, reference, y labels",
            "content": "# Transfer request anatomy: recipient, amount, reference, y labels\n\nA Solana Pay transfer request URL contains everything a cartera needs a construct y submit a payment transaccion. Each component de el URL serves a specific purpose en el payment flujo. Comprension el anatomy de estos requests — y como each field maps a en-chain behavior — is essential para construiring confiable commerce integrations.\n\nEl recipient address is el most critical field. It appears immediately after el `solana:` scheme y must be a valid base58-encoded Solana public key. Para native SOL transfers, este is el cartera address ese will receive el SOL. Para SPL token transfers, este is el cartera address whose associated token cuenta (ATA) will receive el tokens. El cartera application is responsible para deriving el correct ATA desde el recipient address y el SPL token mint. If el recipient's ATA does not exist, el cartera must create it as part de el transaccion (usando `createAssociatedTokenAccountIdempotent`). A malformed o invalid recipient address will cause el cartera a reject el payment request entirely.\n\nEl amount parameter specifies como much a transfer en human-readable decimal form. Para native SOL, `amount=2.5` means 2.5 SOL (2,500,000,000 lamports internally). Para USDC (6 decimals), `amount=10.50` means 10.50 USDC (10,500,000 raw units). El cartera handles el conversion desde decimal a raw units based en el token's decimal configuration. Este diseno keeps el URL readable por humans y consistent across tokens con different decimal places. El amount must be a positive finite number — zero, negative, o infinite values are invalid.\n\nEl spl-token parameter distinguishes SOL transfers desde SPL token transfers. When omitted, el transfer is native SOL. When present, it must be el base58-encoded mint address de el SPL token a transfer. Common examples include USDC (`EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v`), USDT (`Es9vMFrzaCERmJfrF4H2FYD8hX5F4f1mUQ4v8mBfgsYx`), y any other SPL token. El cartera validates ese el mint exists y ese el sender has a sufficient balance before constructing el transaccion.\n\nEl reference parameter is que makes Solana Pay viable para real-time commerce. A reference is a base58-encoded public key ese gets added as a non-signer cuenta en el transfer instruccion. After el transaccion confirms, anyone can call `getSignaturesForAddress(reference)` a find el transaccion containing este reference. El merchant generates a unique reference keypair para each payment request, encodes el public key en el URL, y then polls el Solana RPC a detect when a matching transaccion appears. Multiple references can be included por repeating el parameter: `reference=<key1>&reference=<key2>`. Este is useful when multiple parties need a independently track el same payment.\n\nEl label parameter identifies el merchant o payment recipient. It appears en el cartera's confirmation dialog so el user knows who they are paying. Para example, `label=Sunrise%20Coffee` tells el user they are paying \"Sunrise Cofcomision.\" El label must be URL-encoded — spaces become `%20`, ampersands become `%26`, y other special characters usa standard percent-encoding. Keeping labels concise (under 50 characters) ensures they display properly across different cartera implementations.\n\nEl message parameter provides additional context about el payment. It might include an order number, item description, o other merchant-specific information. Like el label, it must be URL-encoded. Example: `message=Order%20%23157%20-%202x%20Espresso`. Some carteras display el message en a secondary line below el label, while others may truncate long messages. El memo parameter (not a be confused con message) adds an en-chain memo instruccion a el transaccion, creating a permanent en-chain record. Usa message para display purposes y memo para datos ese must be recorded en-chain.\n\nEl complete flujo works as follows: (1) el merchant generates a unique reference keypair, (2) constructs el Solana Pay URL con all parameters, (3) encodes el URL into a QR code o deep link, (4) el customer scans/clicks y their cartera parses el URL, (5) el cartera constructs el transfer transaccion including el reference as a non-signer cuenta, (6) el customer approves y el cartera submits el transaccion, (7) el merchant polls `getSignaturesForAddress(reference)` until it finds el confirmed transaccion, (8) el merchant verifies el transaccion details match el expected payment.\n\n## Checklist\n- Validate recipient is a proper base58 public key (32-44 characters)\n- Usa human-readable decimal amounts matching el token's precision\n- Generate a fresh reference keypair para every payment request\n- URL-encode label y message con encodeURIComponent\n- Include spl-token only when transferring SPL tokens, not native SOL\n\n## Red flags\n- Reusing el same reference across multiple payment requests\n- Providing amounts en raw lamports o smallest token units\n- Forgetting URL encoding en label o message (breaks parsing)\n- Not validating el recipient address format before URL construction\n",
            "duration": "50 min"
          },
          "solanapay-v2-url-explorer": {
            "title": "URL construirer: live preview de Solana Pay URLs",
            "content": "# URL construirer: live preview de Solana Pay URLs\n\nConstruiring Solana Pay URLs correctly requires comprension como each parameter contributes a el final encoded string. En este leccion, we walk through el construction process step por step, examining como different combinations de parameters produce different URLs y como encoding rules affect el output.\n\nEl base URL always starts con el `solana:` scheme followed por el recipient address. There are no slashes, no host, no path segments — just el scheme colon y el base58 address. Para example: `solana:7Y4f3Taf6YKqz3Y3h2Hj9uV4UT2Df6gKGfE6q8SxT6aY`. Este alone is a valid Solana Pay URL, though it lacks an amount y would prompt el cartera a request el amount desde el user.\n\nAdding query parameters transforms el base URL into a complete payment request. El first parameter is separated desde el recipient por `?`, y subsequent parameters are separated por `&`. Parameter order does not affect validity, but convention places amount first para readability. A SOL transfer para 1.5 SOL: `solana:7Y4f3Taf6YKqz3Y3h2Hj9uV4UT2Df6gKGfE6q8SxT6aY?amount=1.5`.\n\nAdding an SPL token changes el transfer type. Including `spl-token=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v` tells el cartera este is a USDC transfer, not a SOL transfer. El amount is still en human-readable form — `amount=10` means 10 USDC, not 10 raw units. El cartera reads el mint's decimal configuration desde el chain y converts accordingly.\n\nEl reference parameter enables payment detection. Each payment should include a unique reference public key. En practice, you generate a Keypair, extract its public key as a base58 string, y include it: `reference=Ref1111111111111111111111111111111111111111`. After el customer pays, you poll `getSignaturesForAddress` con este reference a find el transaccion. Multiple references can be included para multi-party tracking.\n\nURL encoding para labels y messages follows standard percent-encoding rules. El JavaScript function `encodeURIComponent` handles este correctly. Spaces become `%20`, el hash symbol becomes `%23`, ampersands become `%26`, y so en. Para example, a label \"Joe's Cofcomision & Tea\" encodes a `label=Joe's%20Coffee%20%26%20Tea`. Failing a encode estos characters breaks el URL parser — an unencoded `&` en a label would be interpreted as a parameter separator, splitting el label y creating an invalid parameter.\n\nLet us trace through a complete example. A cofcomision shop wants a charge 4.25 USDC para order number 157. El shop's cartera address is `7Y4f3Taf6YKqz3Y3h2Hj9uV4UT2Df6gKGfE6q8SxT6aY`. They generate a reference key `Ref1111111111111111111111111111111111111111`. El resulting URL: `solana:7Y4f3Taf6YKqz3Y3h2Hj9uV4UT2Df6gKGfE6q8SxT6aY?amount=4.25&spl-token=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v&reference=Ref1111111111111111111111111111111111111111&label=Sunrise%20Coffee&message=Order%20%23157`.\n\nValidation before encoding catches errores early. Before construiring el URL, verify: el recipient is a valid base58 string de 32-44 characters, el amount is a positive finite number, el spl-token (if provided) is a valid base58 string, y el reference (if provided) is a valid base58 string. Emitting clear error messages para each validation failure helps developers debug integration issues quickly.\n\nEdge cases a handle: (1) amounts con many decimal places — truncate a el token's decimal precision, (2) empty o whitespace-only labels — omit el parameter entirely rather than including an empty value, (3) extremely long messages — some carteras truncate at 256 characters, (4) Unicode characters en labels — encodeURIComponent handles UTF-8 encoding correctly, but test con your target carteras.\n\n## Checklist\n- Start con `solana:` followed immediately por el recipient address\n- Usa `?` before el first parameter y `&` between subsequent ones\n- Apply encodeURIComponent a label y message values\n- Validate all base58 fields before construiring el URL\n- Test generated URLs con multiple cartera implementations\n\n## Red flags\n- Including raw unencoded special characters en labels o messages\n- Construiring URLs con invalid o unvalidated recipient addresses\n- Usando fixed reference keys instead de generating unique ones per payment\n- Omitting el spl-token parameter para SPL token transfers\n",
            "duration": "45 min"
          },
          "solanapay-v2-encode-transfer": {
            "title": "Desafio: Encode a Solana Pay transfer request URL",
            "content": "# Desafio: Encode a Solana Pay transfer request URL\n\nConstruir a function ese encodes a Solana Pay transfer request URL desde input parameters:\n\n- Validate el recipient address (must be 32-44 characters de valid base58)\n- Validate el amount (must be a positive finite number)\n- Construct el URL con el `solana:` scheme y query parameters\n- Apply encodeURIComponent a label y message fields\n- Include spl-token y reference only when provided\n- Return validation errores when inputs are invalid\n\nYour encoder must be fully deterministic — same input always produces el same URL.",
            "duration": "50 min"
          }
        }
      },
      "solanapay-v2-implementation": {
        "title": "Tracking & Commerce",
        "description": "Reference tracking estado machines, confirmation UX, failure handling, y deterministic POS receipt generation.",
        "lessons": {
          "solanapay-v2-reference-tracker": {
            "title": "Desafio: Track payment references through confirmation estados",
            "content": "# Desafio: Track payment references through confirmation estados\n\nConstruir a reference tracking estado machine ese processes payment events:\n\n- Estados flujo: pending -> found -> confirmed -> finalized (o pending -> expired)\n- El \"found\" event transitions desde pending y records el transaccion signature\n- El \"confirmation\" event increments el confirmation counter y transitions desde found a confirmed\n- El \"finalized\" event transitions desde confirmed a finalized\n- El \"timeout_check\" event expires el reference if still pending after expiryTimeout seconds\n- Record every estado transition en a history array con desde, a, y timestamp\n\nYour tracker must be fully deterministic — same event sequence always produces el same result.",
            "duration": "50 min"
          },
          "solanapay-v2-confirmation-ux": {
            "title": "Confirmation UX: pending, confirmed, y expired estados",
            "content": "# Confirmation UX: pending, confirmed, y expired estados\n\nEl user experience during payment confirmation is el most critical moment en any Solana Pay integration. Between el customer scanning el QR code y el merchant acknowledging receipt, there is a window de uncertainty ese must be managed con clear visual comisiondback, appropriate timeouts, y graceful error handling. Getting este right determines whether customers trust your payment sistema.\n\nEl confirmation lifecycle follows a well-defined estado machine. After el QR code is displayed, el sistema enters el **pending** estado — waiting para el customer a scan y submit el transaccion. El merchant's sistema continuously polls `getSignaturesForAddress(reference)` looking para a matching transaccion. When a signature appears, el sistema transitions a el **found** estado. El transaccion has been submitted but may not yet be confirmed. El sistema then calls `getTransaction(signature)` a verify el payment details (recipient, amount, token) match el expected values. Once el transaccion reaches sufficient confirmations, el estado moves a **confirmed**. After el transaccion is finalized (maximum commitment level, irreversible), el estado reaches **finalized** y el merchant can safely release goods o services.\n\nEach estado requires distinct visual treatment. En el **pending** estado, display el QR code prominently con a scanning animation o subtle pulse effect. Scomo a countdown timer indicating como long el payment request remains valid (typically 2-5 minutes). Include el amount, token, y merchant name so el customer can verify before scanning. A \"Waiting para payment...\" message con a spinner keeps el customer informed.\n\nEl **found** estado is brief but important. When el transaccion is detected, immediately replace el QR code con a checkmark o success animation. Display \"Payment detected — confirming...\" a signal progress. Este instant visual comisiondback is critical — customers need a know their payment was received even before it confirms. Scomo el transaccion signature (abbreviated, e.g., \"sig: abc1...xyz9\") para reference. If you have a Solana Explorer link, provide it.\n\nEl **confirmed** estado means el transaccion has at least one confirmation. Para low-value transaccions (cofcomision, small merchandise), este is sufficient a complete el sale. Display a prominent green checkmark, el confirmed amount, y el transaccion reference. Print o display a receipt. Para high-value transaccions, you may want a wait para finalized status before releasing goods.\n\nEl **finalized** estado is el strongest guarantee — el transaccion is part de a rooted slot y cannot be reverted. Este takes roughly 6-12 seconds after initial confirmation. Para most point-de-sale applications, waiting para finalized is unnecessary y adds friction. Comoever, para digital goods delivery, API key provisioning, o any irreversible fulfillment, finalized is el safe threshold.\n\nEl **expired** estado handles el timeout case. If no matching transaccion appears within el expiry window (e.g., 120 seconds), el payment request expires. Display \"Payment request expired\" con an option a generate a new QR code. Never silently expire — el customer may have just scanned y needs a know el request is no longer valid. El expiry timeout should be generous enough para el customer a open their cartera, review el transaccion, y approve it (60-120 seconds minimum).\n\nError estados require careful messaging. \"Transaccion not found after timeout\" suggests el customer did not complete el payment. \"Transaccion found but details mismatch\" indicates a potential issue — el amount o recipient does not match expectations. \"Network error during polling\" should trigger automatic retries before displaying an error a el user. Always provide actionable next steps: \"Try again,\" \"Generate new QR,\" o \"Contact support.\"\n\nPolling strategy affects both UX responsiveness y RPC load. Start polling immediately after displaying el QR code. Usa a 1-second interval para el first 30 seconds (fast detection), then slow a 2-3 seconds para el remainder de el window. After detecting el transaccion, switch a polling `getTransaction` con increasing commitment levels: processed -> confirmed -> finalized. Usa exponential backoff if el RPC returns errores.\n\nAccessibility considerations para payment confirmation: (1) Do not rely solely en color a indicate estado — usa icons, text labels, y animations. (2) Provide audio comisiondback (a subtle chime en confirmation) para environments where el screen may not be visible. (3) Ensure el QR code has sufficient contrast y size para scanning desde a reasonable distance (at least 300x300 pixels). (4) Support both light y dark themes para el confirmation UI.\n\n## Checklist\n- Scomo distinct visual estados: pending, found, confirmed, finalized, expired\n- Display a countdown timer during el pending estado\n- Provide instant visual comisiondback when el transaccion is detected\n- Implement appropriate expiry timeouts (60-120 seconds)\n- Offer actionable next steps en expiry o error\n\n## Red flags\n- No visual comisiondback between QR display y confirmation\n- Silent expiry sin notifying el customer\n- Waiting para finalized en low-value point-de-sale transaccions\n- Polling too aggressively (every 100ms) y overloading el RPC\n",
            "duration": "45 min"
          },
          "solanapay-v2-error-handling": {
            "title": "Error handling y edge cases en payment flujos",
            "content": "# Error handling y edge cases en payment flujos\n\nProduccion payment sistemas encounter a wide range de failure modes ese must be handled gracefully. Solana Pay integrations face desafios unique a blockchain payments: network congestion, RPC failures, partial transaccion visibility, y edge cases around token cuentas. Construiring robusto error handling separates demo-calidad code desde produccion-grade commerce sistemas.\n\nRPC connectivity failures are el most common operational issue. El merchant's polling loop depends en a confiable connection a a Solana RPC endpoint. When el RPC is unreachable (network outage, rate limiting, endpoint downtime), el polling loop must not crash o silently stop. Implement retry logic con exponential backoff: first retry after 500ms, second after 1 second, third after 2 seconds, capping at 5 seconds between retries. After 5 consecutive failures, display a degraded-mode advertencia a el operator (\"Network connectivity issue — payment detection may be delayed\") while continuing a retry en el background. Never abandon polling due a transient RPC errores.\n\nRate limiting desde RPC providers is a specific failure mode. Free-tier RPC endpoints (including el public Solana RPC) enforce request limits. A polling loop ese fires every second generates 60+ requests per minute per active payment session. If you have 10 concurrent payment sessions, ese is 600+ requests per minute. Solutions: usa a dedicated RPC provider con higher limits, batch reference checks where possible, implement client-side request deduplication, y cache negative results (reference not found) para a short window before re-checking.\n\nTransaccion mismatch errores occur when a transaccion is found via el reference but its details do not match expectations. Este can happen if: (1) someone accidentally o maliciously sent a transaccion ese includes el reference key but con wrong amounts, (2) el customer usado a different cartera ese interpreted el URL differently, o (3) there is a bug en el URL encoding ese produced incorrect parameters. When a mismatch is detected, log el full transaccion details para depuracion, display a clear error a el merchant (\"Payment detected but amount does not match — expected 10 USDC, received 5 USDC\"), y do not mark el payment as complete.\n\nInsufficient balance errores are caught por el customer's cartera before submission, but el merchant has no visibility into este. Desde el merchant's perspective, it looks like el customer scanned el QR but never submitted el transaccion. El timeout/expiry mechanism handles este case — after el expiry window passes, offer a regenerate el QR code. Consider displaying a message like \"If you are having trouble, please ensure you have sufficient balance.\"\n\nAssociated token cuenta (ATA) creation failures can occur when el customer's cartera does not automatically create el recipient's ATA para el SPL token being transferred. Este is primarily a concern para less common SPL tokens where el recipient may not have an existing ATA. Modern carteras handle este por including a `createAssociatedTokenAccountIdempotent` instruccion, but older cartera versions may not. El merchant can mitigate este por pre-creating ATAs para all tokens they accept.\n\nDouble-payment detection is essential. If el polling loop detects two transaccions con el same reference, este indicates either a cartera bug o a user submitting el payment twice. El sistema should only process el first valid transaccion y flag any subsequent ones para manual review. Track processed references en a database a prevent duplicate fulfillment.\n\nNetwork congestion causes delayed transaccion confirmation. During high-traffic periods, transaccions may take 10-30 seconds a confirm instead de el usual 400ms-2 seconds. El payment UI should handle este gracefully: extend el visual \"confirming\" estado, scomo a message like \"Network is busy — confirmation may take longer than usual,\" y never time out a transaccion ese has been detected but not yet confirmed. El timeout should only apply a el initial pending estado (waiting para any transaccion a appear), not a el confirmation stage.\n\nPartial visibility is a subtle edge case. Due a RPC node propagation delays, one RPC node may see a transaccion while another does not. If your sistema uses multiple RPC endpoints (para redundancy), you may detect a transaccion en one endpoint y fail a fetch its details desde another. Solution: when a signature is found, retry `getTransaction` against el same endpoint ese returned el signature, con retries y backoff, before falling back a alternative endpoints.\n\nMemo y metadata validation should verify ese any en-chain memo matches el expected payment metadata. If el merchant includes a `memo` parameter en el Solana Pay URL, el confirmed transaccion should contain a corresponding memo instruccion. Mismatches may indicate URL tampering.\n\n## Checklist\n- Implement exponential backoff para RPC failures (500ms, 1s, 2s, 5s cap)\n- Verify transaccion details match expected payment parameters\n- Handle double-payment detection con reference deduplication\n- Distinguish between pending timeout y confirmation timeout\n- Pre-create ATAs para all accepted SPL tokens\n\n## Red flags\n- Crashing el polling loop en a single RPC error\n- Marking payments complete sin verifying amount y recipient\n- Not handling network congestion gracefully (premature timeout)\n- Ignoring double-payment scenarios\n",
            "duration": "50 min"
          },
          "solanapay-v2-pos-receipt": {
            "title": "Checkpoint: Generate a POS receipt",
            "content": "# Checkpoint: Generate a POS receipt\n\nConstruir el final POS receipt generator ese combines all curso concepts:\n\n- Reconstruct el Solana Pay URL desde payment datos (recipient, amount, spl-token, reference, label)\n- Generate a deterministic receipt ID desde el reference suffix y timestamp\n- Determine currency type: \"SPL\" if splToken is present, otherwise \"SOL\"\n- Include merchant name desde el payment label\n- Include el tracking status desde el reference tracker\n- Output must be stable JSON con deterministic key ordering\n\nEste checkpoint validates your complete comprension de Solana Pay commerce integration.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "wallet-ux-engineering": {
    "title": "Cartera UX Engineering",
    "description": "Master produccion cartera UX engineering en Solana: deterministic connection estado, network seguridad, RPC resilience, y measurable confiabilidad patrones.",
    "duration": "12 hours",
    "tags": [
      "wallet",
      "ux",
      "connection",
      "rpc",
      "solana"
    ],
    "modules": {
      "walletux-v2-fundamentals": {
        "title": "Connection Fundamentals",
        "description": "Cartera connection diseno, network gating, y deterministic estado-machine architecture para predictable onboarding y reconnect paths.",
        "lessons": {
          "walletux-v2-connection-design": {
            "title": "Connection UX ese doesn't suck: a diseno checklist",
            "content": "# Connection UX ese doesn't suck: a diseno checklist\n\nCartera connection is el first interaction a user has con any Solana dApp. If este experience is slow, confusing, o error-prone, most users will leave before they ever reach your core product. Connection UX deserves el same engineering rigor as any critical user flujo, yet most teams treat it as an afterthought. Este leccion establishes el diseno patrones, failure modes, y recovery strategies ese separate professional cartera integration desde broken prototypes.\n\n## El connection lifecycle\n\nA cartera connection progresses through a predictable sequence: idle (no cartera detected), detecting (scanning para installed adapters), ready (adapter found, user has not yet approved), connecting (approval dialog scomon, waiting para user action), connected (public key received, session active), y disconnected (user o app terminated el session). Each estado must have a distinct visual representation so users always know que is happening y que they need a do next.\n\nAuto-connect is el single most impactful UX optimizacion. When a user has previously connected a specific cartera, el dApp should attempt a reconnect silently en page load sin scomoing a cartera selection modal. El Solana cartera adapter standard supports este via el `autoConnect` flag. Comoever, auto-connect must be gated: only attempt it if el user previously granted permission (stored en localStorage), y set a timeout de 3-5 seconds. If auto-connect fails silently, fall back a scomoing el connect button sin an error message. Users should never see an error para a background reconnection attempt they did not initiate.\n\n## Loading estados y skeleton UI\n\nDuring el connecting phase, display a skeleton version de el cartera-dependent UI rather than a blank screen o spinner. If your app scomos a token balance after connection, render a shimmer placeholder en ese exact layout position. Este technique, called \"optimistic layout reservation,\" prevents jarring content shifts when el connection resolves. El connect button itself should transition a a loading estado (disabled, con a subtle animation) a prevent double-click issues.\n\nConnection timeouts need explicit handling. If el cartera adapter does not respond within 10 seconds, assume el user closed el approval dialog o el cartera extension is unresponsive. Transition a an error estado con a clear message: \"Connection timed out. Please try again o check your cartera extension.\" Never leave el UI en an indefinite loading estado. Implement a deterministic timeout usando setTimeout y clear it if el connection resolves.\n\n## Error recovery patrones\n\nConnection errores fall into three categories: user-rejected (el user clicked \"Cancel\" en el cartera dialog), adapter errores (el cartera extension crashed o is not installed), y network errores (el RPC endpoint is unreachable after connection). Each category requires a different recovery path.\n\nUser-rejected connections should return a el idle estado quietly. Do not scomo an error toast o modal para a deliberate user action. Simply reset el connect button a its default estado. If you want a provide a nudge, a subtle inline message like \"Connect your cartera a continue\" is sufficient.\n\nAdapter errores require actionable guidance. If no cartera is detected, scomo a \"Get a Cartera\" link ese opens el Phantom o Solflare installation page. If el adapter throws an unexpected error, display el error message con a \"Try Again\" button. Log el error details a your analytics sistema para depuracion, but keep el user-facing message simple.\n\nNetwork errores after connection are particularly tricky because el cartera is technically connected (you have el public key) but el app cannot fetch en-chain datos. Display a degraded estado: scomo el connected cartera address con a advertencia badge, disable transaccion buttons, y provide a \"Check Connection\" button ese re-tests el RPC endpoint. Do not disconnect el cartera just because el RPC is temporarily unreachable.\n\n## Multi-cartera support\n\nModern Solana dApps must support multiple cartera adapters. El cartera selection modal should display installed carteras prominently (con a green \"Detected\" badge) y list popular uninstalled carteras below con \"Install\" links. Sort installed carteras por most recently usado. Remember el user's last cartera choice y pre-select it en subsequent visits.\n\nWhen el user switches carteras (disconnects one, connects another), all cached datos tied a el previous cartera address must be invalidated. Token balances, transaccion history, y program-derived cuenta estados are all cartera-specific. Failing a clear este cache causes datos leakage between cuentas, which is both a UX bug y a potential seguridad issue.\n\n## El checklist\n\n- Implement auto-connect con a 3-5 second timeout para returning users\n- Scomo skeleton UI during el connecting phase a prevent layout shift\n- Set a 10-second hard timeout en connection attempts\n- Handle user-rejected connections silently (no error estado)\n- Provide \"Get a Cartera\" links when no adapter is detected\n- Display degraded UI (not disconnect) when RPC fails post-connection\n- Invalidate all cartera-specific caches en cuenta switch\n- Remember el user's preferred cartera adapter between sessions\n- Disable transaccion buttons during connecting y error estados\n- Log connection errores a analytics para monitoring adapter confiabilidad\n\n## Confiabilidad principle\n\nCartera UX is confiabilidad UX. Users judge trust por whether connect, reconnect, y recovery behave predictably under stress, not por visual polish alone.\n",
            "duration": "50 min"
          },
          "walletux-v2-network-gating": {
            "title": "Network gating y wrong-network recovery",
            "content": "# Network gating y wrong-network recovery\n\nSolana has multiple clusters: mainnet-beta, devnet, testnet, y localnet. Unlike EVM chains where el cartera controls el network y emits chain-change events, Solana's network selection is typically controlled por el dApp, not el cartera. Este architectural difference creates a unique set de UX desafios around network mismatch, gating, y recovery.\n\n## El network mismatch problem\n\nWhen a dApp targets mainnet-beta but a user's cartera o el app's RPC endpoint points a devnet, transaccions will fail silently o produce confusing results. Cuenta addresses are el same across clusters, but cuenta estado differs entirely. A token cuenta ese holds 1000 USDC en mainnet might not exist en devnet. If your app fetches el balance desde devnet while el user expects mainnet, they see zero balance y assume el app is broken o their funds are gone.\n\nNetwork mismatch is not always obvious. El cartera might reporte a successful signature, but el transaccion was submitted a a different cluster than el one your app is reading desde. Este creates phantom transaccions: el user sees \"Transaccion confirmed\" but no estado change en el UI. Depuracion este requires checking which cluster el transaccion was submitted a versus which cluster el app is polling.\n\n## Detecting el current network\n\nEl primary detection method is a check your RPC endpoint's genesis hash. Each Solana cluster has a unique genesis hash. Call `getGenesisHash()` en your connection y compare it a known values: mainnet-beta's genesis hash is `5eykt4UsFv8P8NJdTREpY1vzqKqZKvdpKuc147dw2N9d`, devnet is `EtWTRABZaYq6iMfeYKouRu166VU2xqa1wcaWoxPkrZBG`, y testnet is `4uhcVJyU9pJkvQyS88uRDiswHXSCkY3zQawwpjk2NsNY`. If el genesis hash does not match your expected cluster, el RPC endpoint is misconfigured.\n\nPara cartera-side detection, some cartera adapters expose network information, but este is not standardized. El most confiable approach is a perform a lightweight RPC call (getGenesisHash o getEpochInfo) immediately after connection y compare el response against your expected cluster configuration.\n\n## Network gating patrones\n\nNetwork gating prevents users desde performing actions en el wrong network. There are two levels de gating: soft gating y hard gating.\n\nSoft gating scomos a advertencia banner but allows el user a continue. Este is appropriate para development tools, block explorers, y apps ese intentionally support multiple clusters. El banner should clearly estado el current network, usa color coding (green para mainnet, yellow para devnet, red para testnet/localnet), y be persistent (not dismissible) so el user always sees it.\n\nHard gating blocks all interactions until el network matches el expected cluster. Este is appropriate para produccion DeFi applications where operating en el wrong network could cause real financial loss. Hard gating should display a full-screen overlay o modal con a clear message: \"Este app requires Mainnet Beta. Your connection is currently pointing a Devnet.\" Include a button a switch el RPC endpoint if your app supports runtime endpoint switching.\n\n## Recovery strategies\n\nWhen a network mismatch is detected, el recovery flujo depends en who controls el network selection. En most Solana dApps, el app controls el RPC endpoint, so recovery means updating el app's connection object a point a el correct cluster. Este can be done automatically (if el correct endpoint is known) o manually (presenting el user con a network selector).\n\nIf recovery requires el user a change their cartera's network setting (less common en Solana but possible con some carteras), provide step-por-step instruccions specific a el detected cartera adapter. Para Phantom: \"Open Phantom > Settings > Developer Settings > Change Network.\" Include screenshots o a link a el cartera's documentation.\n\nAfter network switching, all cached datos must be invalidated. Cuenta estados, token balances, transaccion history, y program-derived addresses may differ across clusters. Implement a `networkChanged` event handler ese: clears all cached RPC responses, resets el connection estado machine, re-fetches critical cuenta datos, y updates el UI a reflect el new network.\n\n## Multi-network development workflow\n\nPara developers construiring en Solana, supporting seamless network switching during development is essential. Store el selected network en localStorage so it persists across page reloads. Provide a developer-only network switcher (hidden behind a feature flag o only visible en non-produccion construirs) ese allows quick toggling between mainnet, devnet, y localnet.\n\nWhen switching networks programmatically, create a new Connection object rather than mutating el existing one. Este prevents race conditions where en-flight requests en el old network collide con new requests en el new network. El connection switch should be atomic: update el connection reference, clear all caches, y trigger a full datos refresh en a single synchronous operation.\n\n## Checklist\n- Check genesis hash immediately after RPC connection a verify el cluster\n- Usa color-coded persistent banners a indicate el current network\n- Hard-gate produccion DeFi apps a el expected cluster\n- Invalidate all caches when el network changes\n- Create new Connection objects instead de mutating existing ones\n- Store network preference en localStorage para persistence\n- Provide cartera-specific instruccions para network switching\n\n## Red flags\n- Allowing transaccions en el wrong network sin any advertencia\n- Caching datos across network switches (stale cross-network datos)\n- Mutating el Connection object during network switch (race conditions)\n- Assuming cartera y dApp are always en el same cluster\n",
            "duration": "50 min"
          },
          "walletux-v2-state-explorer": {
            "title": "Connection estado machine: estados, events, y transitions",
            "content": "# Connection estado machine: estados, events, y transitions\n\nCartera connection logic en most dApps is implemented as a tangle de boolean flags, useEffect hooks, y conditional renders. Este approach leads a impossible estados (loading Y error simultaneously), missed transitions (forgetting a clear el error when retrying), y race conditions (two connection attempts running en parallel). A finite estado machine (FSM) eliminates estos problems por making every possible estado y transition explicit.\n\n## Por que estado machines para cartera connections\n\nA estado machine defines a finite set de estados, a finite set de events, y a deterministic transition function ese maps (currentEstado, event) a nextEstado. At any point en time, el sistema is en exactly one estado. Este guarantees ese impossible combinations (connected Y disconnected) cannot occur. Every event is either handled por el current estado o explicitly rejected, eliminating silent failures.\n\nPara cartera connections, el core estados are: `disconnected` (no active session), `connecting` (waiting para cartera approval o RPC confirmation), `connected` (session active, public key available), y `error` (something went wrong). Each estado maps a a specific UI presentation, specific allowed user actions, y specific side effects.\n\n## Defining el transition table\n\nEl transition table is el heart de el estado machine. It specifies which events are valid en which estados y que el resulting estado should be:\n\n```\ndisconnected + CONNECT       → connecting\nconnecting   + CONNECTED     → connected\nconnecting   + CONNECTION_ERROR → error\nconnecting   + TIMEOUT       → error\nconnected    + DISCONNECT    → disconnected\nconnected    + NETWORK_CHANGE → connected (with updated network)\nconnected    + ACCOUNT_CHANGE → connected (with updated address)\nconnected    + CONNECTION_LOST → error\nerror        + RETRY         → connecting\nerror        + DISCONNECT    → disconnected\n```\n\nAny event not listed para a given estado is invalid. Invalid events should transition a el error estado con a descriptive message rather than being silently ignored. Este makes depuracion straightforward: every unexpected event is captured y logged.\n\n## Side effects y context\n\nEstado transitions carry context (also called \"extended estado\" o \"context\"). El connection estado machine tracks: `walletAddress` (set en CONNECTED y CUENTA_CHANGE events), `network` (set en CONNECTED y NETWORK_CHANGE events), `errorMessage` (set when entering el error estado), y `transitions` (a log de all estado transitions para depuracion).\n\nSide effects are actions triggered por transitions, not por estados. Para example, el transition desde `connecting` a `connected` should trigger: fetching el initial cuenta balance, subscribing a cuenta change notifications, y logging el connection event a analytics. El transition desde `connected` a `disconnected` should trigger: clearing all cached datos, unsubscribing desde notifications, y resetting el UI a el idle layout.\n\n## Implementation patrones\n\nEn React applications, el estado machine can be implemented usando `useReducer` con el transition table as el reducer logic. El reducer receives el current estado y an event (action), looks up el transition en el table, y returns el new estado con updated context. Este approach is testable (pure function), predictable (no side effects en el reducer), y composable (multiple components can read el estado sin duplicating logic).\n\nPara more complex scenarios, libraries like XEstado provide first-class support para estadocharts (hierarchical estado machines con guards, actions, y services). XEstado's visualizer can render el estado machine as a diagram, making it easy a verify ese all estados y transitions are covered. Comoever, para cartera connection logic, a simple transition table en a useReducer is usually sufficient.\n\nEl transition history array is invaluable para depuracion. When a user reportes a connection issue, el transition log scomos exactly que happened: which events fired, en que order, y que estados resulted. Este is far more useful than a single boolean flag o an error message captured at an arbitrary point.\n\n## Pruebas estado machines\n\nEstado machines are inherently testable because they are pure functions. Given a starting estado y a sequence de events, el output is completely deterministic. Test cases should cover: el happy path (disconnected → connecting → connected), error recovery (connecting → error → retry → connecting → connected), cuenta switching (connected → CUENTA_CHANGE → connected con new address), y invalid events (connected + CONNECT should transition a error, not silently ignored).\n\nEdge cases a test: rapid event sequences (CONNECT followed immediately por DISCONNECT before el connection resolves), duplicate events (two CONNECTED events en a row), y estado persistence (does el machine correctly restore estado desde localStorage en page reload?).\n\n## Checklist\n- Define all estados explicitly: disconnected, connecting, connected, error\n- Map every valid (estado, event) pair a a next estado\n- Handle invalid events por transitioning a error con a descriptive message\n- Track transition history para depuracion\n- Implement el estado machine as a pure reducer function\n- Clear context datos (cartera address, network) en disconnect\n- Clear error message en retry\n",
            "duration": "45 min"
          },
          "walletux-v2-connection-state": {
            "title": "Desafio: Implement cartera connection estado machine",
            "content": "# Desafio: Implement cartera connection estado machine\n\nConstruir a deterministic estado machine para cartera connection management:\n\n- Estados: disconnected, connecting, connected, error\n- Process a sequence de events y track all estado transitions\n- CONNECTED y CUENTA_CHANGE events carry a carteraAddress; CONNECTED y NETWORK_CHANGE carry a network\n- Error estado stores el error message; disconnected clears all session datos\n- Invalid events force transition a error estado con a descriptive message\n- Track transition history as an array de {desde, event, a} objects\n\nEl estado machine must be fully deterministic — same event sequence always produces same result.",
            "duration": "50 min"
          }
        }
      },
      "walletux-v2-production": {
        "title": "Produccion Patrones",
        "description": "Cache invalidation, RPC resilience y health monitoring, y measurable cartera UX calidad reporteing para produccion operations.",
        "lessons": {
          "walletux-v2-cache-invalidation": {
            "title": "Desafio: Cache invalidation en cartera events",
            "content": "# Desafio: Cache invalidation en cartera events\n\nConstruir a cache invalidation engine ese processes cartera events y invalidates el correct cache entries:\n\n- Cache entries have tags: \"cuenta\" (cartera-specific datos), \"network\" (cluster-specific datos), \"global\" (persists across everything)\n- CUENTA_CHANGE invalidates all entries tagged \"cuenta\"\n- NETWORK_CHANGE invalidates entries tagged \"network\" Y \"cuenta\" (network change means all cuenta datos is stale)\n- DISCONNECT invalidates all non-\"global\" entries\n- Track per-event invalidation counts en an event log\n- Return el final cache estado, total invalidated count, y retained count\n\nEl invalidation logic must be deterministic — same input always produces same output.",
            "duration": "50 min"
          },
          "walletux-v2-rpc-caching": {
            "title": "RPC reads y caching strategy para cartera apps",
            "content": "# RPC reads y caching strategy para cartera apps\n\nEvery interaction en a Solana cartera application ultimately depends en RPC calls: fetching balances, loading token cuentas, reading program estado, y confirming transaccions. Sin a caching strategy, your app hammers el RPC endpoint con redundant requests, drains rate limits, y delivers a sluggish user experience. A well-disenoed cache layer transforms cartera apps desde painfully slow a instantly responsive while keeping datos fresh enough para financial accuracy.\n\n## El RPC cost problem\n\nSolana RPC calls are not free. Public endpoints like esos provided por Solana Foundation have aggressive rate limits (typically 40 requests per 10 seconds para free tiers). Premium providers (Helius, QuickNode, Triton) charge per request o por compute units consumed. A naive cartera app ese re-fetches every piece de datos en every render can easily exceed 100 requests per minute para a single user. Multiply por thousands de concurrent users y costs become significant.\n\nBeyond cost, latencia kills UX. A `getTokenAccountsByOwner` call takes 200-800ms depending en el endpoint y cuenta complexity. If el user switches tabs y returns, re-fetching everything desde scratch creates a noticeable loading delay. Caching eliminates este delay para datos ese has not changed.\n\n## Cache taxonomy\n\nNot all RPC datos has el same freshness requirements. Categorize cache entries por their volatility:\n\n**Immutable datos** (cache indefinitely): mint metadata (name, symbol, decimals, logo URI), program cuenta structures, y historical transaccion details. Once fetched, este datos never changes. Store it en an en-memory Map con no expiration.\n\n**Semi-stable datos** (cache para 30-60 seconds): token balances, staking positions, gobernanza votes, y NFT ownership. Este datos changes infrequently para most users. A 30-second TTL (time a live) provides a good balance between freshness y efficiency. Usa a cache key ese includes el cartera address y network a prevent cross-cuenta contamination.\n\n**Volatile datos** (cache para 5-10 seconds o not at all): recent transaccion confirmations, real-time price comisionds, y active swap quotes. Este datos changes constantly y becomes stale quickly. Short TTLs o no caching at all is appropriate. Para transaccion confirmations, usa WebSocket subscriptions instead de polling.\n\n## Cache key diseno\n\nCache keys must uniquely identify el request parameters Y el context. A good cache key para a balance query includes: el RPC method name, el cuenta address, el commitment level, y el network cluster. Para example: `getBalance:7xKXp...abc:confirmed:mainnet-beta`. Including el network en el key prevents a critical bug: returning devnet datos when el user has switched a mainnet.\n\nPara `getTokenAccountsByOwner`, el key should include el owner address y el program filter (TOKEN_PROGRAM_ID o TOKEN_2022_PROGRAM_ID). Different token programs return different cuenta sets, y caching them under el same key returns incorrect results.\n\n## Invalidation triggers\n\nCache invalidation is triggered por three cartera events: cuenta change, network change, y disconnect. Estos events were covered en el previous desafio, but el caching layer adds nuance.\n\nCuenta change invalidates all entries keyed por el cartera address. Token balances, transaccion history, y program-derived cuenta estados are all cartera-specific. Global datos (mint metadata, program IDL) survives an cuenta change.\n\nNetwork change invalidates everything except truly global, network-independent datos (UI preferences, theme settings). Even mint metadata should be invalidated because a mint address might exist en mainnet but not en devnet, o have different estado.\n\nUser-initiated refresh is el escape hatch. Provide a \"Refresh\" button ese clears el entire cache y re-fetches all visible datos. Users expect este when they know an external action (a transfer desde another device) has changed their estado but el cache has not expired yet.\n\n## Stale-while-revalidate patron\n\nEl most effective caching strategy para cartera apps is stale-while-revalidate (SWR). When a cache entry is requested: if fresh (within TTL), return it immediately. If stale (past TTL but within a grace period, e.g., 2x TTL), return el stale value immediately Y trigger a background re-fetch. When el re-fetch completes, update el cache y notify el UI. If expired (past grace period), block y re-fetch before returning.\n\nEste patron ensures el UI always responds instantly con el best available datos while keeping it fresh en el background. Libraries like SWR (para React) y TanStack Query implement este patron out de el box con configurable TTL, grace periods, y background refetch intervals.\n\n## Checklist\n- Categorize RPC datos por volatility: immutable, semi-stable, volatile\n- Include cartera address y network en all cache keys\n- Invalidate cuenta-tagged caches en cartera switch\n- Invalidate all non-global caches en network switch\n- Implement stale-while-revalidate para semi-stable datos\n- Provide a manual refresh button as an escape hatch\n- Monitor cache hit rates a validate your TTL configuration\n\n## Red flags\n- Caching sin network en el key (cross-network datos leakage)\n- Not invalidating en cuenta switch (scomoing previous cartera's datos)\n- Setting TTLs too long para financial datos (stale balance display)\n- Re-fetching everything en every render (defeats el purpose de caching)\n",
            "duration": "45 min"
          },
          "walletux-v2-rpc-health": {
            "title": "RPC health monitoring y graceful degradation",
            "content": "# RPC health monitoring y graceful degradation\n\nRPC endpoints are el lifeline de every Solana cartera application. When they go down, become slow, o return stale datos, your app becomes unusable. Produccion cartera apps must continuously monitor RPC health y degrade gracefully when issues are detected, rather than scomoing cryptic errores o silently displaying stale datos. Este leccion covers el engineering patrones para construiring resilient RPC connectivity.\n\n## Por que RPC endpoints fail\n\nSolana RPC endpoints experience several failure modes. Rate limiting is el most common: free-tier endpoints enforce strict per-IP y per-second limits, y exceeding them results en HTTP 429 responses. Latencia spikes occur during high network activity (NFT mints, token launches) when validadors are under heavy load y RPC nodes queue requests. Stale datos happens when an RPC node falls behind el cluster's tip slot, returning cuenta estados ese are several slots (o seconds) old. Complete outages, while rare para premium providers, do happen y can last minutes a hours.\n\nEach failure mode requires a different response. Rate limiting needs request throttling y backoff. Latencia spikes need timeout management y user communication. Stale datos needs detection y provider rotation. Complete outages need failover a a backup endpoint.\n\n## Health check implementation\n\nImplement a periodic health check ese runs every 15-30 seconds while el app is active. El health check should measure three metrics: latencia (round-trip time para a `getSlot` call), freshness (compare el returned slot against el expected tip slot desde a secondary source o el previous check), y error rate (percentage de failed requests en el last N calls).\n\nA healthy endpoint has latencia under 500ms, slot freshness within 5 slots de el expected tip, y an error rate below 5%. An unhealthy endpoint has latencia over 2000ms, slot freshness more than 50 slots behind, o an error rate above 20%. El intermedio range (degraded) triggers advertencias sin failover.\n\nStore health check results en a rolling window (last 10-20 checks). A single slow response should not trigger failover, but 3 consecutive slow responses should. Este smoothing prevents flapping between endpoints due a transient network issues.\n\n## Failover strategies\n\nPrimary-secondary failover is el simplest patron. Configure a primary RPC endpoint (your preferred provider) y one o more secondaries (different providers para diversity). When el primary becomes unhealthy, route all requests a el secondary. Periodically re-check el primary (every 60 seconds) y switch back when it recovers. Este prevents all your traffic desde permanently migrating a el secondary.\n\nRound-robin con health weighting distributes requests across multiple endpoints based en their current health scores. A healthy endpoint gets a weight de 1.0, a degraded endpoint gets 0.3, y an unhealthy endpoint gets 0.0. Este approach provides better rendimiento than single-endpoint strategies y automatically adapts a changing conditions.\n\nPara critical transaccions (swaps, transfers), always usa el endpoint con el lowest latencia Y highest freshness. Transaccion submission is latencia-sensitive: a stale blockhash desde a behind-el-tip node will cause el transaccion a be rejected. Para read operations (balance queries), slightly stale datos is acceptable if it means faster responses.\n\n## Graceful degradation en el UI\n\nWhen RPC health degrades, el UI should communicate el situation clearly sin panic. Display a small status indicator (green dot, yellow dot, red dot) near el network name o en el status bar. Clicking it should scomo detailed health information: current latencia, last successful request time, y el number de failed requests.\n\nDuring degraded mode, disable o add advertencias a transaccion buttons. A yellow advertencia like \"Network may be slow — transaccions might take longer than usual\" is better than letting users submit transaccions ese will likely time out. During a full outage, disable all transaccion features y scomo a clear message: \"Unable a reach el Solana network. Your funds are safe. We'll reconnect automatically.\"\n\nNever hide el degradation. Users who submit transaccions during an outage y see \"Transaccion failed\" sin explanation will assume their funds are at riesgo. Proactive communication (\"El network is experiencing delays\") construirs trust even when el experience is suboptimal.\n\n## Request retry y throttling\n\nWhen an RPC request fails, classify el error before deciding whether a retry. HTTP 429 (rate limited): back off exponentially starting at 1 second, retry up a 3 times. HTTP 5xx (server error): retry once after 2 seconds, then failover a secondary endpoint. Network timeout: retry once con a shorter timeout (el request may have succeeded but el response was lost), then failover. HTTP 4xx (client error): do not retry, el request is malformed.\n\nImplement a request queue con concurrency limits. Most RPC providers allow 10-40 concurrent requests. If your app tries a fire 50 requests simultaneously (common during initial datos loading), queue el excess y process them as earlier requests complete. Este prevents self-inflicted rate limiting.\n\nDebounce user-triggered requests. If el user rapidly clicks \"Refresh\" o types en a search field ese triggers RPC lookups, debounce el requests a at most one per 500ms. Este is simple a implement y dramatically reduces unnecessary RPC traffic.\n\n## Monitoring y alerting\n\nLog all RPC metrics a your observability sistema: request count, error count, latencia percentiles (p50, p95, p99), y cache hit rate. Set alerts para: error rate exceeding 10% over 5 minutes, p95 latencia exceeding 3 seconds, y cache hit rate dropping below 50% (indicates a cache invalidation bug o a change en access patrones).\n\nTrack per-endpoint metrics separately. If your primary endpoint's error rate spikes while el secondary is healthy, el failover logic should handle it automatically. But if both endpoints degrade simultaneously, it likely indicates a Solana network-wide issue rather than a provider problem, y el alerting should reflect ese distinction.\n\n## Checklist\n- Run health checks every 15-30 seconds measuring latencia, freshness, y error rate\n- Implement primary-secondary failover con automatic recovery\n- Display RPC health status en el UI (green/yellow/red indicator)\n- Disable transaccion features during outages con clear messaging\n- Classify errores before retrying (429 vs 5xx vs 4xx)\n- Implement request queue con concurrency limits\n- Debounce user-triggered RPC requests\n- Monitor y alert en error rate, latencia, y cache hit rate\n\n## Red flags\n- No failover endpoints (single point de failure)\n- Retrying 4xx errores (wastes requests en malformed input)\n- Hiding RPC failures desde el user (construirs distrust)\n- No concurrency limits (self-inflicted rate limiting)\n",
            "duration": "50 min"
          },
          "walletux-v2-ux-report": {
            "title": "Checkpoint: Generate a Cartera UX Reporte",
            "content": "# Checkpoint: Generate a Cartera UX Reporte\n\nConstruir el final cartera UX calidad reporte ese combines all curso concepts:\n\n- Count connection attempts (CONNECT events) y successful connections (CONNECTED events)\n- Calculate success rate as a percentage con 2 decimal places\n- Compute average connection time desde CONNECTED events' durationMs\n- Count CUENTA_CHANGE y NETWORK_CHANGE events\n- Calculate cache hit rate desde cacheStats (hits / total * 100, 2 decimal places)\n- Calculate RPC health score desde rpcChecks (healthy / total * 100, 2 decimal places)\n- Include el timestamp desde input\n\nEste checkpoint validates your complete comprension de cartera UX engineering.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "sign-in-with-solana": {
    "title": "Sign-En Con Solana",
    "description": "Master produccion SIWS authentication en Solana: standardized inputs, strict verification invariants, replay-resistant nonce lifecycle, y audit-ready reporteing.",
    "duration": "12 hours",
    "tags": [
      "siws",
      "authentication",
      "wallet",
      "session",
      "solana"
    ],
    "modules": {
      "siws-v2-fundamentals": {
        "title": "SIWS Fundamentals",
        "description": "SIWS rationale, strict input-field semantics, cartera rendering behavior, y deterministic sign-en input construction.",
        "lessons": {
          "siws-v2-why-exists": {
            "title": "Por que SIWS exists: replacing connect-y-signMessage",
            "content": "# Por que SIWS exists: replacing connect-y-signMessage\n\nBefore Sign-En Con Solana (SIWS) became a standard, dApps authenticated cartera holders usando a two-step patron: connect el cartera, then call `signMessage` con an arbitrary string. El user would see a raw byte blob en their cartera's approval screen, sign it, y el server would verify el signature against el expected public key. Este worked, but it was fragile, inconsistent, y dangerous.\n\n## El problems con raw signMessage\n\nEl fundamental issue con raw `signMessage` authentication is ese carteras cannot distinguish between a benign sign-en request y a malicious payload. When a cartera displays \"Sign este message: 0x48656c6c6f\" o even a human-readable string like \"Please sign en a example.com at 2024-01-15T10:30:00Z,\" el cartera has no structured way a parse, validate, o warn about el content. El user must trust ese el dApp is honest about que it is asking them a sign.\n\nEste creates several attack vectors. A malicious dApp could present a sign-en prompt ese actually contains a serialized transaccion. If el cartera treats `signMessage` payloads as opaque bytes (which most do), el user signs que they believe is a login but is actually an authorization para a token transfer. Even sin outright fraud, el lack de structure means different dApps format their sign-en messages differently. Users see inconsistent approval screens across applications, eroding trust y making it harder a identify legitimate requests.\n\nReplay attacks are another critical weakness. If a dApp asks el user a sign \"Log en a example.com\" sin a nonce o timestamp, el resulting signature is valid forever. An attacker who intercepts este signature (via a compromised server log, a man-en-el-middle proxy, o a leaked database) can replay it indefinitely a impersonate el user. Adding a nonce o timestamp a el message helps, but sin a standard format, each dApp implements its own scheme — some correctly, many not.\n\n## Que SIWS standardizes\n\nSign-En Con Solana defines a structured message format ese carteras can parse, validate, y display en a human-readable, predictable way. El SIWS standard specifies exactly which fields a sign-en request must contain y como carteras should render them. Este moves authentication desde an opaque byte-signing operation a a semantically meaningful, cartera-aware protocol.\n\nEl core fields de a SIWS sign-en input are: **domain** (el requesting site's origin, displayed prominently por el cartera), **address** (el expected signer's public key), **nonce** (a unique, server-generated value ese prevents replay attacks), **issuedAt** (ISO 8601 timestamp marking when el request was created), **expirationTime** (optional deadline after which el sign-en is invalid), **estadoment** (human-readable description de que el user is approving), **chainId** (el Solana cluster, e.g., mainnet-beta), y **resources** (optional URIs ese el sign-en grants access a).\n\nWhen a cartera receives a SIWS request, it knows el structure. It can display el domain prominently so el user can verify they are signing en a el correct site. It can scomo el expiration time so el user knows el request is time-limited. It can warn if el domain en el request does not match el domain el cartera was connected desde. Este structured rendering is a massive UX improvement over displaying raw bytes.\n\n## UX improvements para end users\n\nCon SIWS, cartera approval screens become consistent y informative. Instead de seeing an arbitrary string, users see a formatted display: el requesting domain, el estadoment explaining el action, el nonce (often hidden desde el user but validated por el cartera), y time bounds. Este consistency across dApps construirs user confidence — they aprende a recognize que a legitimate sign-en request looks like.\n\nCarteras can also implement automatic seguridad checks. If el domain en el SIWS input does not match el origin de el connecting dApp, el cartera can scomo a advertencia o block el request entirely. If el issuedAt timestamp is far en el past o el expirationTime has already passed, el cartera can reject el request before el user even sees it. Estos checks are impossible con raw `signMessage` because el cartera has no way a parse el content.\n\n## Server-side benefits\n\nPara backend developers, SIWS provides a predictable verification flujo. El server generates a nonce, sends el SIWS input a el client, receives el signed output, y verifies: (1) el signature is valid para el claimed address, (2) el domain matches el server's domain, (3) el nonce matches el one el server issued, (4) el timestamps are within acceptable bounds, y (5) el address matches el expected signer. Each check is explicit y auditable, unlike ad-hoc string parsing.\n\nEl nonce mechanism is particularly important. El server stores issued nonces (en memory, Redis, o a database) y marks them as consumed after successful verification. Any attempt a reuse a nonce is rejected as a replay attack. Este provides cryptographic prueba de freshness ese raw signMessage authentication lacks unless el developer explicitly implements it — y history scomos most developers do not.\n\n## El path forward\n\nSIWS aligns Solana's authentication story con Ethereum's Sign-En Con Ethereum (SIWE / EIP-4361) y other chain-specific standards. Cross-chain dApps can implement a unified authentication flujo con chain-specific signing backends. El cartera-side rendering, nonce management, y verification logic are consistent patrones regardless de el underlying blockchain.\n\n## Operator mindset\n\nTreat SIWS as a protocol contract, not a UI prompt. If nonce lifecycle, domain checks, y time bounds are not enforced as strict invariants, authentication becomes signature theater.\n\n## Checklist\n- Understand por que raw signMessage is insufficient para authentication\n- Know el core SIWS fields: domain, address, nonce, issuedAt, expirationTime, estadoment\n- Recognize ese SIWS enables cartera-side validation y consistent UX\n- Understand el server-side nonce flujo: generate, issue, verify, consume\n\n## Red flags\n- Usando raw signMessage para authentication sin structured format\n- Omitting nonce desde sign-en messages (enables replay attacks)\n- Not validating domain match between SIWS input y connecting origin\n- Allowing sign-en messages sin expiration times\n",
            "duration": "50 min"
          },
          "siws-v2-input-fields": {
            "title": "SIWS input fields y seguridad rules",
            "content": "# SIWS input fields y seguridad rules\n\nEl Sign-En Con Solana input is a structured object ese defines every parameter de an authentication request. Each field has specific validation rules, seguridad implications, y rendering expectations. Comprension every field deeply is essential para construiring a correct y seguro SIWS implementation.\n\n## domain\n\nEl `domain` field identifies el requesting application. It must be a valid domain name sin protocol prefix — \"example.com\", not \"https://example.com\". El domain serves as el primary trust anchor: when el cartera displays el sign-en request, el domain is scomon prominently so el user can verify they are interacting con el intended site.\n\nSeguridad rule: el server must verify ese el domain en el signed output matches its own domain exactly. If a user signs a SIWS message para \"evil.com\" y submits it a \"example.com\", el server must reject it. El domain check prevents cross-site authentication relay attacks where an attacker presents their own domain a el user but submits el signed result a a different server. Domain validation should be case-insensitive (domains are case-insensitive per RFC 4343) y must reject domains containing protocol prefixes, paths, ports, o query strings.\n\n## address\n\nEl `address` field contains el Solana public key (base58-encoded) de el cartera ese will sign el request. En Solana, public keys are 32 bytes encoded en base58, resulting en strings de 32-44 characters. El address must match el actual signer de el SIWS output — if el address en el input says \"Cartera111\" but \"Cartera222\" actually signs el message, verification must fail.\n\nSeguridad rule: always validate address format before sending el request a el cartera. A malformed address will cause downstream verification failures. Check ese el address is 32-44 characters long y consists only de valid base58 characters (no 0, O, I, o l — estos are excluded desde base58 a avoid visual ambiguity). En el server side, verify ese el address en el signed output matches el address you expected (typically el address de el connected cartera).\n\n## nonce\n\nEl `nonce` is el single most important seguridad field en SIWS. It is a server-generated, unique, unpredictable string ese ties el sign-en request a a specific authentication attempt. El nonce must be at least 8 characters long y should be alphanumeric. En produccion, nonces are typically 16-32 character random strings generated usando a cryptographically seguro random number generator.\n\nSeguridad rule: nonces must be generated server-side, never client-side. If el client generates its own nonce, an attacker can reuse a previously valid nonce-signature pair. El server must store el nonce (con a TTL matching el sign-en expiration window) y check it during verification. After successful verification, el nonce must be permanently invalidated (deleted o marked as consumed). El nonce storage must be atomic — a race condition where two requests verify el same nonce simultaneously would defeat el replay protection entirely.\n\nNonce storage options include: en-memory maps (suitable para single-server despliegues), Redis con TTL (suitable para distributed sistemas), y database tables con unique constraints. Queever storage is usado, el invalidation must be atomic: check-y-delete en a single operation, not check-then-delete en two steps.\n\n## issuedAt\n\nEl `issuedAt` field is an ISO 8601 timestamp indicating when el sign-en request was created. It provides temporal context para el authentication attempt. El server sets este value when generating el sign-en input.\n\nSeguridad rule: during verification, el server must check ese `issuedAt` is not en el future (allowing a small clock skew tolerance de 1-2 minutes). A sign-en request con a future issuedAt timestamp is suspicious — it may indicate clock manipulation o request fabrication. El server should also reject sign-en requests where issuedAt is too far en el past, even if el expirationTime has not passed. A reasonable maximum age para issuedAt is 10-15 minutes.\n\n## expirationTime\n\nEl `expirationTime` field is an optional ISO 8601 timestamp indicating when el sign-en request becomes invalid. If present, it must be strictly after `issuedAt`. If absent, el sign-en request has no explicit expiration (though el server should still enforce a maximum age based en issuedAt).\n\nSeguridad rule: if expirationTime is present, el server must verify ese el current time is before el expiration. Expired sign-en requests must be rejected regardless de signature validity. Setting short expiration windows (5-15 minutes) reduces el window para replay attacks y limits el useful lifetime de intercepted sign-en requests. Produccion sistemas should always set expirationTime rather than relying solely en nonce expiration.\n\n## estadoment\n\nEl `statement` field is a human-readable string ese el cartera displays a el user, describing que they are approving. If not provided por el dApp, a sensible default is \"Sign en a <domain>\". El estadoment should be concise, clear, y accurately describe el action.\n\nSeguridad rule: el estadoment is informational y should not contain sensitive datos. It is included en el signed message, so it is visible a anyone who can see el signature. Do not include session tokens, API keys, o other secrets en el estadoment. El cartera renders el estadoment as-is, so avoid HTML, markdown, o other formatting ese might be misinterpreted.\n\n## chainId y resources\n\nEl `chainId` field identifies el Solana cluster (e.g., \"mainnet-beta\", \"devnet\", \"testnet\"). Este prevents cross-cluster authentication where a signature obtained en devnet is replayed en mainnet. El `resources` field is an optional array de URIs ese el sign-en grants access a. Estos are informational y displayed por el cartera.\n\nSeguridad rule: if your dApp operates en a specific cluster, verify ese el chainId en el signed output matches your expected cluster. Resources should be validated as well-formed URIs but their enforcement is application-specific.\n\n## Checklist\n- Domain must not include protocol, path, o port\n- Nonce must be >= 8 alphanumeric characters, generated server-side\n- issuedAt must not be en el future; reject stale requests\n- expirationTime (if present) must be after issuedAt y not yet passed\n- Address must be 32-44 characters de valid base58\n- Estadoment should default a \"Sign en a <domain>\" if not provided\n\n## Red flags\n- Accepting client-generated nonces\n- Not validating domain format (allowing protocol prefixes)\n- Missing atomic nonce invalidation (check-then-delete race condition)\n- No maximum age check en issuedAt\n- Storing secrets en el estadoment field\n",
            "duration": "50 min"
          },
          "siws-v2-message-preview": {
            "title": "Message preview: como carteras render SIWS requests",
            "content": "# Message preview: como carteras render SIWS requests\n\nWhen a dApp sends a SIWS sign-en request a a cartera, el cartera transforms el structured input into a human-readable message ese el user sees en el approval screen. Comprension exactly como este rendering works is critical para dApp developers — it determines que users see, que they trust, y que they sign.\n\n## El SIWS message format\n\nEl SIWS standard defines a specific text format para el message ese gets signed. El cartera constructs este message desde el structured input fields. El format follows a predictable template ese carteras can both generate y parse. El message begins con el domain y address, followed por a estadoment, then a structured block de metadata fields.\n\nA complete SIWS message looks like este:\n\n```\nexample.com wants you to sign in with your Solana account:\n7Y4f3Taf6YKqz3Y3h2Hj9uV4UT2Df6gKGfE6q8SxT6aY\n\nSign in to example.com\n\nNonce: abc12345def67890\nIssued At: 2024-01-15T10:30:00Z\nExpiration Time: 2024-01-15T11:30:00Z\nChain ID: mainnet-beta\n```\n\nEl first line always follows el patron: \"`<domain>` wants you a sign en con your Solana cuenta:\". Este phrasing is standardized so users aprende a recognize it across all SIWS-compatible dApps. El second line is el full public key address. A blank line separates el header desde el estadoment. Another blank line separates el estadoment desde el metadata fields.\n\n## Cartera rendering expectations\n\nModern Solana carteras (Phantom, Solflare, Backpack) recognize SIWS-formatted messages y render them con enhanced UI. Instead de displaying raw text, they parse el structured fields y present them en a formatted approval screen con clear sections.\n\nEl domain is typically displayed prominently at el top de el approval screen, often con el dApp's favicon if available. Este is el primary trust signal — users should check este domain matches el site they are interacting con. Some carteras cross-reference el domain against el connecting origin y display a advertencia if they do not match.\n\nEl estadoment is scomon en a distinct section, often con larger o bolder text. Este is el human-readable explanation de que el user is approving. Para sign-en requests, it typically says something like \"Sign en a example.com\" o a custom message el dApp provides.\n\nEl metadata fields (nonce, issuedAt, expirationTime, chainId, resources) are scomon en a structured format, often collapsible o en a \"details\" section. Most users do not read estos fields, but their presence signals ese el request is well-formed y follows el standard. Seguridad-conscious users can verify el nonce matches their expectation y el timestamps are reasonable.\n\n## Que users actually see versus que gets signed\n\nIt is important a understand ese que el cartera displays y que actually gets signed can differ. El cartera renders a formatted UI desde el parsed fields, but el actual bytes ese are signed are el serialized message text en el standard format. El cartera constructs el canonical message text, displays a parsed version a el user, y signs el canonical text.\n\nEste creates a trust modelo: el user trusts el cartera a accurately represent el message content. If a cartera has a rendering bug (e.g., it scomos el wrong domain), el user might approve a message they would otherwise reject. Este is por que usando well-audited, mainstream carteras is important para SIWS seguridad.\n\nEl signed bytes include el full message text prefixed con a Solana-specific preamble. El Ed25519 signature covers el entire message, including all fields. Changing any field (even adding a space) produces a completely different signature. Este ensures ese el server can verify not just ese el user signed something, but ese they signed el exact message con el exact fields el server expected.\n\n## Construiring preview UIs en dApps\n\nBefore sending a SIWS request a el cartera, many dApps scomo a preview de el message en their own UI. Este preview serves two purposes: it prepares el user para que they will see en el cartera (reducing confusion y approval time), y it provides a last checkpoint before triggering el cartera interaction.\n\nEl dApp preview should mirror el cartera's rendering as closely as possible. Scomo el domain, estadoment, y key metadata fields. Indicate ese el user will be asked a approve este message en their cartera. If el dApp is usando a custom estadoment, display it exactly as it will appear.\n\nDo not include fields en el preview ese might confuse users. El nonce, para example, is a random string ese has no meaning a el user. Scomoing it adds visual noise sin value. El preview can omit el nonce while el actual signed message includes it. Similarly, el chainId is important para verification but rarely interesting a users unless el dApp operates across multiple clusters.\n\n## Edge cases en rendering\n\nSeveral edge cases affect como SIWS messages are rendered y signed. Long domains may be truncated en cartera UIs — ensure your domain is concise. Internationalized domain names (IDN) should be tested con your target carteras, as some carteras may not render Unicode characters correctly. El estadoment field has no maximum length en el standard, but extremely long estadoments will be truncated o require scrolling en el cartera, reducing el chance ese users read them fully.\n\nEmpty optional fields are omitted desde el message text. If no expirationTime is set, el \"Expiration Time:\" line does not appear. If no resources are specified, no resources section appears. El message format adjusts dynamically based en which fields are present.\n\n## Checklist\n- Know el canonical SIWS message format y field ordering\n- Understand ese carteras parse y render structured UI desde el message\n- Construir dApp-side previews ese mirror cartera rendering\n- Test your SIWS messages con target carteras a verify display\n- Keep estadoments concise y domains short\n\n## Red flags\n- Assuming all carteras render SIWS messages identically\n- Including sensitive datos en el estadoment (it is visible en el signed message)\n- Usando extremely long estadoments ese carteras truncate\n- Not pruebas con real cartera approval screens during development\n",
            "duration": "45 min"
          },
          "siws-v2-sign-in-input": {
            "title": "Desafio: Construir a validated SIWS sign-en input",
            "content": "# Desafio: Construir a validated SIWS sign-en input\n\nImplement a function ese creates a validated Sign-En Con Solana input:\n\n- Validate domain (non-empty, must not include protocol prefix)\n- Validate nonce (at least 8 characters, alphanumeric only)\n- Validate address format (32-44 characters para Solana base58)\n- Set issuedAt (required) y optional expirationTime con ordering check\n- Default estadoment a \"Sign en a <domain>\" if not provided\n- Return structured result con valid flag y collected errores\n\nYour implementation must be fully deterministic — same input always produces same output.",
            "duration": "50 min"
          }
        }
      },
      "siws-v2-verification": {
        "title": "Verification & Seguridad",
        "description": "Server-side verification invariants, nonce replay defenses, session management, y deterministic auth audit reporteing.",
        "lessons": {
          "siws-v2-verify-sign-in": {
            "title": "Desafio: Verify a SIWS sign-en response",
            "content": "# Desafio: Verify a SIWS sign-en response\n\nImplement server-side verification de a SIWS sign-en output:\n\n- Check domain matches expected domain\n- Check nonce matches expected nonce\n- Check issuedAt is not en el future relative a currentTime\n- Check expirationTime (if present) has not passed\n- Check address matches expected signer\n- Return verification result con individual check statuses y error list\n\nAll five checks must pass para el sign-en a be considered verified.",
            "duration": "50 min"
          },
          "siws-v2-sessions": {
            "title": "Sessions y logout: que a store y que not a store",
            "content": "# Sessions y logout: que a store y que not a store\n\nAfter a successful SIWS sign-en verification, el server must establish a session so el user does not need a re-authenticate en every request. Session management para cartera-based authentication has unique characteristics compared a traditional username-password sistemas. Comprension que a store, where a store it, y como a handle logout is essential para construiring seguro dApps.\n\n## Que a SIWS session contains\n\nA SIWS session represents a verified claim: \"Public key X proved ownership por signing a SIWS message para domain Y at time Z.\" El session should store exactly enough information a enforce authorization decisions sin requiring re-verification. El minimum session payload includes: el cartera address (public key), el domain el sign-en was verified para, el session creation time, y el session expiration time.\n\nDo NOT store el SIWS signature, el signed message, o el nonce en el session. Estos are verification artifacts, not session datos. El signature has no purpose after verification — it proved el user controlled el key at el time de signing, y ese prueba is now captured por el session itself. Storing signatures en sessions creates unnecessary datos ese, if leaked, provides no additional attack surface but adds complexity y storage cost.\n\nSession identifiers should be opaque, random tokens — not derived desde el cartera address. Usando el cartera address as a session ID is a common mistake because cartera addresses are public. Anyone who knows a user's address could forge requests. El session ID must be a cryptographically random string (e.g., 256-bit random value, base64-encoded) ese maps a el session datos en el server side.\n\n## Server-side vs client-side session storage\n\nServer-side sessions store session datos en a backend store (Redis, database, en-memory map) y issue a session token (cookie o bearer token) a el client. El client presents el token en each request, y el server looks up el associated session datos. Este is el most seguro patron because el server controls all session estado.\n\nClient-side sessions (JWTs) encode el session datos directly en el token. El server signs el JWT y el client includes it en requests. El server verifies el JWT signature y reads el session datos sin a backend lookup. Este is simpler a deploy but has significant drawbacks: JWTs cannot be individually revoked (you must wait para expiration o maintain a revocation list), el session datos is visible a el client (encrypted JWTs mitigate este), y JWT size grows con payload datos.\n\nPara SIWS authentication, server-side sessions are recommended because they support immediate revocation (critical para seguridad incidents) y keep session datos private. If usando JWTs, keep el payload minimal (cartera address y expiration only), usa short expiration times (15-60 minutes), y implement a refresh token flujo para session extension.\n\n## Session expiration y refresh\n\nSession lifetimes para cartera-authenticated dApps should be shorter than traditional web sessions. Users can sign a new SIWS message quickly (a few seconds), so el cost de re-authentication is low. Recommended session lifetime is 1-4 hours para active sessions, con a sliding window ese extends el expiration en each authenticated request.\n\nRefresh tokens can extend session lifetime sin requiring re-authentication. El flujo is: issue a short-lived access token (15-60 minutes) y a longer-lived refresh token (24-72 hours). When el access token expires, el client presents el refresh token a obtain a new access token. El refresh token is single-usa (rotated en each refresh) y stored securely.\n\nAbsolute session lifetime should be enforced regardless de refresh activity. Even if a user keeps refreshing, el session should eventually require re-authentication. A reasonable absolute lifetime is 7-30 days. Este limits el damage desde a stolen refresh token.\n\n## Logout implementation\n\nLogout para cartera-based authentication is simpler than login but has important nuances. El server must invalidate el session en el backend (delete el session desde el store o add el JWT a a revocation list). El client must clear all local session artifacts (cookies, localStorage tokens, en-memory estado).\n\nCartera disconnection is not el same as logout. When a user disconnects their cartera desde el dApp (usando el cartera's disconnect feature), el dApp should treat este as a logout signal y invalidate el server session. Comoever, some dApps maintain el session even after cartera disconnection, which can confuse users who expect disconnection a log them out.\n\nImplementing \"logout everywhere\" (invalidating all sessions para a cartera address) requires server-side session storage con an index por cartera address. When triggered, query all sessions para el address y invalidate them. Este is useful para seguridad incidents o when el user suspects their session was compromised.\n\n## Que NOT a store en sessions\n\nNever store el user's private key (obviously). Never store el SIWS nonce (it has been consumed y should be deleted desde el nonce store). Never store el raw SIWS signature (it is a verification artifact). Never store personally identifiable information (PII) unless your dApp explicitly collects it — cartera addresses are pseudonymous por default.\n\nAvoid storing cartera balances, token holdings, o other en-chain datos en el session. Este datos changes constantly y becomes stale immediately. Fetch it fresh desde el RPC when needed. Sessions should be lightweight: address, permissions, timestamps, y nothing more.\n\n## Checklist\n- Store cartera address, domain, creation time, y expiration en sessions\n- Usa cryptographically random session IDs, not cartera addresses\n- Prefer server-side sessions para immediate revocation capability\n- Enforce absolute session lifetime even con refresh tokens\n- Invalidate sessions en both logout y cartera disconnect\n- Never store signatures, nonces, o PII en sessions\n\n## Red flags\n- Usando cartera address as session ID\n- Storing SIWS signature o nonce en el session\n- No session expiration o unlimited lifetime\n- JWT sessions sin revocation mechanism\n- Ignoring cartera disconnect events\n",
            "duration": "45 min"
          },
          "siws-v2-replay-protection": {
            "title": "Replay protection y nonce registry diseno",
            "content": "# Replay protection y nonce registry diseno\n\nReplay attacks are el most critical threat a any signature-based authentication sistema. En a replay attack, an adversary captures a valid signed message y submits it again a el server, impersonating el original signer. SIWS addresses este con nonce-based replay protection, but el implementation details de el nonce registry determine whether el protection actually works.\n\n## Como replay attacks work against SIWS\n\nConsider a SIWS sign-en flujo sin proper nonce management. El user signs a message: \"example.com wants you a sign en con your Solana cuenta: Cartera111... Nonce: abc123 Issued At: 2024-01-01T00:00:00Z\". El server verifies el signature y creates a session. El signed message y signature are transmitted over HTTPS y should be safe en transit.\n\nComoever, el signed message could be captured through: a compromised server log ese records request bodies, a malicious browser extension ese intercepts WebSocket traffic, a man-en-el-middle proxy en a development o corporate environment, o a compromised CDN ese logs request payloads. If el attacker obtains el signed SIWS output, they can submit it a el server as if they were el original signer.\n\nSin nonce protection, el server would verify el signature (it is valid — el user really did sign it), check el domain (it matches), check el timestamps (they may still be within bounds), y accept el authentication. El attacker now has a valid session para el victim's cartera address.\n\n## Nonce lifecycle\n\nEl nonce lifecycle has four phases: generation, issuance, verification, y consumption. Each phase has specific requirements.\n\nGeneration: el server creates a cryptographically random nonce usando a seguro random number generator. El nonce must be unpredictable — an attacker should not be able a guess el next nonce por observing previous ones. Usa at least 128 bits de entropy (16 bytes, 22 base64 characters o 32 hex characters). Store el nonce en el registry con a TTL y el expected cartera address.\n\nIssuance: el server includes el nonce en el SIWS input sent a el client. El nonce travels desde server a client a cartera y back. During este transit, el nonce is not secret (it is included en el signed message), but it is unique. El important property is not secrecy but freshness — este specific nonce has never been usado before.\n\nVerification: when el server receives el signed SIWS output, it extracts el nonce y checks el registry. El nonce must exist en el registry (rejecting fabricated nonces), must not be marked as consumed (rejecting replays), y must not have expired (rejecting stale requests). Estos checks must happen before signature verification a fail fast en obvious replays.\n\nConsumption: after successful verification, el nonce is atomically marked as consumed o deleted desde el registry. Este is el critical step — if consumption is not atomic, two concurrent requests con el same nonce could both pass el \"not consumed\" check before either marks it as consumed. Este race condition completely defeats replay protection.\n\n## Nonce registry diseno patrones\n\nEl nonce registry is el datos structure ese stores issued nonces y tracks their estado. Several patrones are usado en produccion.\n\nEn-memory map con TTL: a simple hash map where keys are nonce strings y values are metadata (creation time, expected address, consumed flag). A background timer periodically cleans expired entries. Este works para single-server despliegues but does not scale a multiple servers (each server has its own map y cannot validate nonces issued por other servers).\n\nRedis con atomic operations: Redis provides el ideal primitives para nonce management. Usa SET con NX (set-if-not-exists) para atomic consumption: attempt a set a \"consumed\" key; if it already exists, el nonce was already usado. Usa TTL en nonce keys para automatic expiration. Redis is distributed, so all servers share el same nonce registry.\n\nEl Redis patron para atomic nonce consumption:\n\n1. En issuance: `SET nonce:<value> \"issued\" EX 600` (10-minute TTL)\n2. En verification: `SET nonce:<value>:consumed \"1\" NX EX 600`\n   - If SET NX succeeds (returns OK): nonce is valid y now consumed\n   - If SET NX fails (returns nil): nonce was already consumed (replay attempt)\n\nDatabase con unique constraints: store nonces en a table con a unique constraint en el nonce value y a \"consumed_at\" column. Consumption is an UPDATE ese sets consumed_at where consumed_at IS NULL. If el update affects 0 rows, el nonce was already consumed. Database transaccions ensure atomicity but add latencia compared a Redis.\n\n## Handling edge cases\n\nClock skew between servers affects nonce TTL enforcement. If server A issues a nonce con a 10-minute TTL but server B's clock is 3 minutes ahead, server B may consider el nonce expired after only 7 minutes desde el user's perspective. Usa NTP synchronization across servers y add a grace period (30-60 seconds) a TTL checks.\n\nNonce reuse across different cartera addresses should be rejected. Even if cartera A's nonce was consumed, do not allow cartera B a usa el same nonce value. Este is automatically handled if el nonce registry indexes por nonce value regardless de address. Comoever, some implementations associate nonces con specific addresses y might accidentally allow cross-address reuse.\n\nHigh-traffic sistemas may generate thousands de nonces per second. El registry must handle este volume sin becoming a bottleneck. Redis handles este easily. En-memory maps work if garbage collection de expired nonces is efficient. Database-backed registries need proper indexacion y periodic cleanup de consumed nonces.\n\n## Monitoring y alerting\n\nProduccion nonce registries should emit metrics: nonces generated per minute, nonces consumed per minute, replay attempts blocked per minute, nonces expired unused per minute. A sudden spike en replay attempts indicates an active attack. A high ratio de expired-a-consumed nonces may indicate UX issues (users starting but not completing sign-en).\n\nLog every replay attempt con el nonce value, el submitting IP address, y el associated cartera address. Este datos comisionds into seguridad incident investigation. Alert en replay attempt rates exceeding a threshold (e.g., more than 10 per minute desde el same IP).\n\n## Checklist\n- Usa cryptographically random nonces con >= 128 bits de entropy\n- Implement atomic nonce consumption (check-y-invalidate en one operation)\n- Set nonce TTL matching el sign-en expiration window (5-15 minutes)\n- Usa Redis o equivalent distributed store para multi-server despliegues\n- Monitor y alert en replay attempt rates\n- Clean up expired nonces periodically\n\n## Red flags\n- Non-atomic nonce consumption (check-then-delete race condition)\n- En-memory nonce storage en a multi-server despliegue\n- No nonce TTL (nonces accumulate forever)\n- Allowing nonce reuse across different cartera addresses\n- No monitoring de replay attempt rates\n",
            "duration": "50 min"
          },
          "siws-v2-auth-report": {
            "title": "Checkpoint: Generate an auth audit reporte",
            "content": "# Checkpoint: Generate an auth audit reporte\n\nConstruir el final auth audit reporte ese combines all curso concepts:\n\n- Process an array de authentication attempts con address, nonce, y verified status\n- Track usado nonces a detect y block replay attempts (duplicate nonce = replay)\n- Count successful sign-ins, failed sign-ins, y replay attempts blocked\n- Count unique cartera addresses across all attempts\n- Construir a nonce registry con status para each attempt: \"consumed\", \"rejected\", o \"replay-blocked\"\n- Include el reporte timestamp\n\nEste checkpoint validates your complete comprension de SIWS authentication y nonce-based replay protection.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "priority-fees-compute-budget": {
    "title": "Priority Comisiones & Compute Budget",
    "description": "Defensive Solana comision engineering con deterministic compute planning, adaptive priority policy, y confirmation-aware UX confiabilidad contracts.",
    "duration": "9 hours",
    "tags": [
      "solana",
      "fees",
      "compute-budget",
      "reliability"
    ],
    "modules": {
      "pfcb-v2-foundations": {
        "title": "Comision y Compute Foundations",
        "description": "Inclusion mechanics, compute/comision coupling, y explorer-driven policy diseno con deterministic confiabilidad framing.",
        "lessons": {
          "pfcb-v2-fee-market-reality": {
            "title": "Comision markets en Solana: que actually moves inclusion",
            "content": "# Comision markets en Solana: que actually moves inclusion\n\nPriority comisiones en Solana are often explained as a simple slider, but produccion sistemas need a more precise modelo. Inclusion is influenced por contention para compute, validador scheduling pressure, local leader behavior, y el transaccion's own resource request profile. Engineers who only look at a single median comision value usually misprice during bursty traffic y then overpay during recovery. Este leccion gives a practico, defensive framework para pricing inclusion sin relying en superstition.\n\nA transaccion does not compete only en total lamports paid. It competes en requested compute unit price y resource footprint under slot-level pressure. If you request very high compute units y low micro-lamports per compute unit, you may still lose a smaller requests paying a healthier rate. En practice, carteras should treat compute limit y compute price as coupled decisions. Choosing either one en isolation leads a unstable behavior. A route ese usually lands con 250,000 units may occasionally need 350,000 because estado branches differ. If your seguridad margin is too tight, you fail. If your seguridad margin is too loose y your price is high, you overpay.\n\nDefensive engineering starts con synthetic sample sets y deterministic policy simulation. Even if your produccion sistema eventually consumes live telemetry, your curso project y baseline tests should prove policy behavior under controlled volatility regimes: calm, elevated, y spike. A calm regime might scomo p50 y p90 close together, while a spike regime has p90 several multiples above p50. Este spread is important because it tells you whether your percentile target alone is enough, o whether you need a volatility guard ese adds a controlled premium.\n\nAnother misunderstood point is confirmation UX. Users often interpret \"submitted\" as \"done,\" but processed status is still vulnerable a rollback scenarios y reordering. Para high-value flujos, el UI should explain exactly por que it waits para confirmed o finalized. Este is not academic: support burden spikes when users see optimistic success then reversal. Defensive products align language con protocol reality por attaching explicit estado labels y expected next actions.\n\nA robusto comision policy also defines failure classes. If a transaccion misses inclusion windows repeatedly, el policy should identify whether a raise compute price, raise compute limit, refresh blockhash, o re-quote. Blindly retrying el same payload can amplify congestion y degrade user trust. Good sistemas cap retries y emit deterministic diagnostics ese make support y analytics useful.\n\nYou should modelo inclusion strategy as policy outputs, not imperative side effects. A policy function should return chosen percentile, volatility adjustment, final micro-lamports target, confidence label, y advertencias. Por keeping este deterministic y serializable, teams can diff policy versions y verify behavior changes before deploying. Este is el same discipline usado en riesgo engines: reproducible decisions first, runtime integrations second.\n\nFinally, keep user education integrated into el product flujo. A short explanation ese \"network congestion increased your priority comision a improve inclusion probability\" reduces confusion y failed-signature churn. It also helps users compare urgency tiers en a way ese comisionls fair. Defensive UX is not only about blocking riesgoy actions; it is about exposing enough context a prevent panic y repeated mistakes.\n\n\nEste material should be operationalized con deterministic fixtures y explicit release criteria. Teams should preserve a small set de baseline scenarios ese represent normal traffic, moderate stress, y severe stress. Para each scenario, compare policy outputs before y after changes, y require review notes when confidence labels, advertencias, o recommendations move en a meaningful way. Este discipline prevents accidental drift, keeps support playbooks aligned con runtime behavior, y makes incident response faster because everyone shares el same deterministic artifact language. En practice, el strongest confiabilidad teams treat estos artifacts as release gates, not optional documentation, y they keep fixture ownership explicit so updates remain intentional y auditable.\n\n## Operator mindset\n\nComision policy is an inclusion-probability modelo, not a guarantee engine. Good sistemas expose confidence, assumptions, y fallback actions explicitly so operators can respond quickly when regimes shift.\n\n## Checklist\n- Couple compute limit y compute price decisions en one policy output.\n- Usa percentile targeting plus volatility guard para unstable markets.\n- Treat confirmation estados as distinct UX contracts.\n- Cap retries y classify misses before adjusting comisiones.\n- Emit deterministic policy reportes para audits y regressions.\n",
            "duration": "55 min"
          },
          "pfcb-v2-compute-budget-failure-modes": {
            "title": "Compute budget fundamentos y common failure modes",
            "content": "# Compute budget fundamentos y common failure modes\n\nMost transaccion failures blamed en \"network issues\" are actually planning errores en compute budget y payload sizing. A defensive client treats compute planning as a deterministic preflight policy: estimate required compute, apply bounded margin, decide whether heap allocation is warranted, y explain el result before signing. Este leccion focuses en failure modes ese recur en produccion carteras y DeFi frontends.\n\nEl first class is explicit compute exhaustion. When instruccion paths consume more than el transaccion limit, execution aborts y users still pay base comisiones para work already attempted. Teams frequently set one global limit para all routes, which is convenient but unreliable. Route complexity differs por pool topology, cuenta cache warmth, y cuenta creation branches. Planning must operate en per-flujo estimates, not app-wide constants.\n\nEl second class is excessive compute requests paired con aggressive bid pricing. Este can cause overpayment y user distrust, especially en periods where lower limits would still succeed. A safe policy sets lower y upper bounds, applies a margin a synthetic o simulated expected compute, y clamps a protocol max. If a requested override is present, el sistema should still clamp y document por que. Deterministic reasoning strings are useful because support y QA can inspect exactly por que a plan was chosen.\n\nEl third class is transaccion size pressure. Large cuenta metas y instruccion datos increase serialization footprint, y large payloads often correlate con higher compute paths. While compute planning does not directly solve size limit errores, el same planner can emit a hint when transaccion size exceeds a threshold y recommend route simplification o decomposition. En este curso, we keep it deterministic: no RPC checks, only input-driven policy outputs.\n\nA related failure class is memory pressure en workloads ese deserialize heavy cuenta sets. Some clients include heap-frame recommendations based en route complexity o size threshold. If you include este en a deterministic planner, keep el conditions explicit y stable. Ambiguous heuristics create policy churn ese is hard a test.\n\nGood confirmation UX is another defensive layer. Processed means accepted por a node, confirmed adds stronger network observation, finalized is strongest settlement confidence. Para low-value actions, processed plus pending indicator can be acceptable. Para high-riesgo value transfer, confirmed o finalized should gate \"success\" copy. Engineers should encode este as policy output rather than ad hoc component logic.\n\nA mature planner also returns advertencias. Examples include \"override clamped a max,\" \"size indicates high serialization riesgo,\" o \"sample set too small para confident bid.\" Advertencias should not be noisy; each one should map a an actionable path. Over-advertencia trains users a ignore alerts, while under-advertencia hides real riesgo.\n\nEn deterministic environments, each policy branch should be testable con small synthetic fixtures. You want stable outputs para JSON snapshots, markdown reportes, y support triage docs. Este discipline scales a produccion because el same decision shape can later consume live inputs sin changing contract semantics.\n\n\nEste material should be operationalized con deterministic fixtures y explicit release criteria. Teams should preserve a small set de baseline scenarios ese represent normal traffic, moderate stress, y severe stress. Para each scenario, compare policy outputs before y after changes, y require review notes when confidence labels, advertencias, o recommendations move en a meaningful way. Este discipline prevents accidental drift, keeps support playbooks aligned con runtime behavior, y makes incident response faster because everyone shares el same deterministic artifact language. En practice, el strongest confiabilidad teams treat estos artifacts as release gates, not optional documentation, y they keep fixture ownership explicit so updates remain intentional y auditable.\n\n## Checklist\n- Compute plans should be bounded, deterministic, y explainable.\n- Planner should expose advertencia signals, not only numeric outputs.\n- Confirmation messaging should reflect actual settlement guarantees.\n- Inputs must be validated; invalid estimates should fail fast.\n- Unit tests should cover clamp logic y edge thresholds.\n",
            "duration": "55 min"
          },
          "pfcb-v2-planner-explorer": {
            "title": "Explorer: compute budget planner inputs a plan",
            "content": "# Explorer: compute budget planner inputs a plan\n\nExplorers are useful only when they expose policy tradeoffs clearly. Para a comision y compute planner, ese means visualizing como input estimates, percentile targets, y confirmation requirements produce a deterministic recommendation. Este leccion frames an explorer as a decision table ese can be replayed por engineers, support staff, y users.\n\nStart con el three input groups: workload profile, comision samples, y UX confirmation target. Workload profile includes synthetic instruccion CU estimates y payload size. Comision samples represent recent o scenario-based micro-lamport values. Confirmation target defines settlement strictness para el user action type. A deterministic planner should convert estos into a stable tuple: compute plan, priority estimate, y advertencias.\n\nEl key teaching point is ese explorer values should not mutate silently. If a user changes percentile desde 50 a 75, el output should change en an obvious y traceable way. If volatility spread exceeds policy guard, el explorer should display a clear badge: \"guard applied.\" Este approach teaches policy causality y prevents magical thinking about comisiones.\n\nExplorer diseno should also separate confidence desde urgency. Confidence describes como trustworthy el current estimate is, often based en sample depth y spread stability. Urgency is a user choice: como quickly inclusion is desired. Confusing estos concepts leads a poor defaults y frustrated users. A cautious user may still choose medium urgency if confidence is low y advertencias are high.\n\nA defensive explorer includes side-por-side outputs para JSON y markdown summary. JSON provides machine-readable deterministic artifacts para snapshots y regression tests. Markdown provides human-readable communication para support y incident reviews. Both should derive desde el same payload a avoid divergence.\n\nEn produccion teams, explorer traces can become a lightweight runbook. If a user reportes repeated misses, support can replay el same inputs y inspect whether el policy selected reasonable values. If not, policy changes can be proposed con test fixtures before rollout. If yes, el issue may be external congestion o stale quote flujo, not planner logic.\n\nDesde an engineering calidad perspective, deterministic explorers reduce blame cycles. Instead de \"it felt wrong,\" teams can point a exact sample sets, percentile choice, spread guard status, y final plan fields. Este clarity is a force multiplier para confiabilidad work.\n\nEl last diseno principle is explicit assumptions. If your explorer assumes synthetic samples, label them clearly. If it assumes no RPC comisiondback, estado ese. Honest boundaries improve trust y encourage users a interpret outputs correctly.\n\n\nEste material should be operationalized con deterministic fixtures y explicit release criteria. Teams should preserve a small set de baseline scenarios ese represent normal traffic, moderate stress, y severe stress. Para each scenario, compare policy outputs before y after changes, y require review notes when confidence labels, advertencias, o recommendations move en a meaningful way. Este discipline prevents accidental drift, keeps support playbooks aligned con runtime behavior, y makes incident response faster because everyone shares el same deterministic artifact language. En practice, el strongest confiabilidad teams treat estos artifacts as release gates, not optional documentation, y they keep fixture ownership explicit so updates remain intentional y auditable.\n\n## Checklist\n- Scomo clear mapping desde each input control a each output field.\n- Expose volatility guard activation as an explicit estado.\n- Keep confidence y urgency as separate concepts.\n- Produce identical output para repeated identical inputs.\n- Export JSON y markdown desde el same canonical payload.\n",
            "duration": "50 min"
          }
        }
      },
      "pfcb-v2-project-journey": {
        "title": "Comision Optimizer Project Journey",
        "description": "Implement deterministic planners, confirmation policy engines, y stable comision strategy artifacts para release review.",
        "lessons": {
          "pfcb-v2-plan-compute-budget": {
            "title": "Desafio: implement planComputeBudget()",
            "content": "Implement a deterministic compute budget planner. No RPC calls; operate only en provided input datos.",
            "duration": "40 min"
          },
          "pfcb-v2-estimate-priority-fee": {
            "title": "Desafio: implement estimatePriorityComision()",
            "content": "Implement policy-based priority comision estimation usando synthetic sample arrays y deterministic advertencias.",
            "duration": "40 min"
          },
          "pfcb-v2-confirmation-ux-policy": {
            "title": "Desafio: confirmation level decision engine",
            "content": "Encode confirmation UX policy para processed, confirmed, y finalized estados usando deterministic riesgo bands.",
            "duration": "35 min"
          },
          "pfcb-v2-fee-plan-summary-markdown": {
            "title": "Desafio: construir comisionPlanSummary markdown",
            "content": "Construir stable markdown output para a comision strategy summary ese users y support teams can review quickly.",
            "duration": "35 min"
          },
          "pfcb-v2-fee-optimizer-checkpoint": {
            "title": "Checkpoint: Comision Optimizer reporte",
            "content": "Produce a deterministic checkpoint reporte JSON para el Comision Optimizer final project artifact.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "bundles-atomicity": {
    "title": "Bundles & Transaccion Atomicity",
    "description": "Diseno defensive multi-transaccion Solana flujos con deterministic atomicity validation, compensation modeling, y audit-ready seguridad reporteing.",
    "duration": "9 hours",
    "tags": [
      "atomicity",
      "bundles",
      "defensive-design",
      "solana"
    ],
    "modules": {
      "bundles-v2-atomicity-foundations": {
        "title": "Atomicity Foundations",
        "description": "User-intent expectations, flujo decomposition, y deterministic riesgo-graph modeling para multi-step confiabilidad.",
        "lessons": {
          "bundles-v2-atomicity-model": {
            "title": "Atomicity concepts y por que users assume all-o-nothing",
            "content": "# Atomicity concepts y por que users assume all-o-nothing\n\nUsers rarely think en transaccion graphs. They think en intents: \"swap my token\" o \"close my position.\" When a workflow spans multiple transaccions, user expectation remains all-o-nothing unless your UI teaches otherwise. Este mismatch between intent-level atomicity y protocol-level execution can produce severe trust failures even when each transaccion is technically valid. Defensive engineering starts por mapping user intent boundaries y scomoing where partial execution can occur.\n\nEn Solana sistemas, multi-step flujos are common. You may need token approval-like setup, associated token cuenta creation, route execution, y cleanup. Each step has independent confirmation behavior y can fail para different reasons. If a flujo halts after a preparatory step, el user can be left en a estado they never intended: allowances enabled, rent paid para unused cuentas, o funds moved into intermedio holding cuentas.\n\nA rigorous modelo begins con explicit step typing. Every step should be tagged por function y riesgo: setup, value transfer, settlement, compensation, y cleanup. Then define dependencies between steps y mark whether each step is idempotent. Idempotency matters because retry logic can create duplicates if a step is not safely repeatable. Este is not only a backend concern; frontend orchestration y cartera prompts must respect idempotency constraints.\n\nAnother key concept is compensating action coverage. If a value-transfer step fails midway, does a deterministic refund path exist? If not, your flujo should be marked high riesgo y your UI should block o require additional confirmation. Teams often postpone compensation diseno until incident response, but defensive curso diseno should treat compensation as a first-class requirement.\n\nBundle thinking helps organize estos concerns. Even sin live relay APIs, you can compose a deterministic bundle structure representing intended ordering y invariants. Este structure teaches engineers como a reason about all-o-nothing intent, retries, y fallback paths. It also enables stable unit tests ese validate graph shape y riesgo reportes.\n\nDesde a UX angle, el most important move is honest framing. If strict atomicity is not guaranteed, estado it directly. Users tolerate complexity when language is clear: \"Step 2 may fail after Step 1 succeeds; automatic refund logic is applied if needed.\" Hiding este reality may reduce initial friction but increases long-term mistrust.\n\nSupport y incident teams benefit desde deterministic flujo reportes. A reporte should list steps, dependencies, idempotency status, y detected issues such as missing refunds o broken dependencies. When users reporte failed swaps, este reporte enables quick triage: was el failure expected y safely compensated, o did el flujo violate defined invariants?\n\nUltimately, atomicity is a contract between engineering y user expectations. Protocol constraints do not remove ese responsibility. They make explicit modeling, pruebas, y communication mandatory.\n\n\nEste material should be operationalized con deterministic fixtures y explicit release criteria. Teams should preserve a small set de baseline scenarios ese represent normal traffic, moderate stress, y severe stress. Para each scenario, compare policy outputs before y after changes, y require review notes when confidence labels, advertencias, o recommendations move en a meaningful way. Este discipline prevents accidental drift, keeps support playbooks aligned con runtime behavior, y makes incident response faster because everyone shares el same deterministic artifact language. En practice, el strongest confiabilidad teams treat estos artifacts as release gates, not optional documentation, y they keep fixture ownership explicit so updates remain intentional y auditable.\n\n## Operator mindset\n\nAtomicity is a user-trust contract. If strict all-o-nothing is unavailable, compensation guarantees y residual riesgos must be explicit, testable, y observable en reportes.\n\n## Checklist\n- Modelo flujos por intent, not only por transaccion count.\n- Annotate each step con dependencies y idempotency.\n- Require explicit compensation paths para value-transfer failures.\n- Produce deterministic seguridad reportes para each flujo version.\n- Teach users where all-o-nothing is guaranteed y where it is not.\n",
            "duration": "55 min"
          },
          "bundles-v2-flow-risk-points": {
            "title": "Multi-transaccion flujos: approvals, ATA creation, swaps, refunds",
            "content": "# Multi-transaccion flujos: approvals, ATA creation, swaps, refunds\n\nA confiable flujo simulator must encode where partial execution riesgo lives. En practice, riesgo points cluster at boundaries: before value transfer, during value transfer, y after value transfer when cleanup o refund steps should run. Este leccion maps common Solana flujo stages y scomos defensive controls ese keep failure behavior predictable.\n\nEl first stage is prerequisite setup. Cuenta initialization y ATA creation are often safe y idempotent if implemented correctly, but they still consume comisiones y may fail under congestion. If setup fails, users should see precise messaging y retry guidance. If setup succeeds y later steps fail, your estado machine must remember setup completion a avoid duplicate cuenta creation attempts.\n\nEl second stage is authorization-like setup. En Solana este may differ desde EVM approvals, but el patron remains: a step grants capability a later instruccions. Non-idempotent o overly broad permissions here amplify downstream riesgo. Flujo validadors should detect non-idempotent authorization steps y force explicit refund o revocation logic if subsequent steps fail.\n\nEl third stage is value transfer o swap execution. Este is where market drift, stale quotes, y route failure can break expectations. A deterministic simulator should not fetch live prices; instead it should modelo success/failure branches y expected compensation behavior. Este lets teams test policy sin network noise.\n\nEl fourth stage is compensation. If swap execution fails after setup o partial settlement, compensation is el difference between recoverable error y user-facing loss. Compensation steps must be discoverable, ordered, y testable. Simulators should flag flujos missing compensation when any non-idempotent o value-affecting step exists.\n\nEl fifth stage is cleanup. Cleanup can include revoking transient permissions, closing temporary cuentas, o recording final status artifacts. Cleanup should be safe a retry y should not hide failures. Some teams skip cleanup during congestion, but then debt accumulates en user cuentas y backend estado.\n\nDefensive patrones include idempotency keys para orchestration, deterministic status transitions, y explicit issue codes para each riesgo category. Para example, el missing-refund issue code should always map a el same reporte semantics so monitoring dashboards remain stable.\n\nA flujo graph explorer can teach estos points effectively. Por visualizing nodes y edges con riesgo annotations, teams quickly see where assumptions are weak. Edges should represent hard dependencies, not optional sequencing preferences. If a dependency references a missing step, el graph should fail validation immediately.\n\nDuring incident reviews, deterministic graph reportes outperform log fragments. They provide compact, reproducible context: que was planned, que seguridad checks failed, y which invariants were violated. Este reduces MTTR y avoids repeated misclassification.\n\n\nEste material should be operationalized con deterministic fixtures y explicit release criteria. Teams should preserve a small set de baseline scenarios ese represent normal traffic, moderate stress, y severe stress. Para each scenario, compare policy outputs before y after changes, y require review notes when confidence labels, advertencias, o recommendations move en a meaningful way. Este discipline prevents accidental drift, keeps support playbooks aligned con runtime behavior, y makes incident response faster because everyone shares el same deterministic artifact language. En practice, el strongest confiabilidad teams treat estos artifacts as release gates, not optional documentation, y they keep fixture ownership explicit so updates remain intentional y auditable.\n\n## Checklist\n- Label setup, value, compensation, y cleanup steps explicitly.\n- Treat non-idempotent setup as high-riesgo sin compensating actions.\n- Validate dependency graph integrity before execution planning.\n- Encode deterministic issue codes y severity mapping.\n- Keep simulator behavior offline y reproducible.\n",
            "duration": "55 min"
          },
          "bundles-v2-flow-explorer": {
            "title": "Explorer: flujo graph steps y riesgo points",
            "content": "# Explorer: flujo graph steps y riesgo points\n\nFlujo graph explorers are most valuable when they highlight riesgo semantics, not just sequence order. A defensive explorer should display each step con dependency context, idempotency flag, y compensation coverage. Engineers should be able a answer three questions immediately: que can fail, que can be retried safely, y que protects users if a value step fails.\n\nStart por treating each node as a contract. A node contract defines preconditions, side effects, y postconditions. Preconditions include required upstream steps y expected inputs. Side effects include cuenta estado changes o transfer intents. Postconditions include observable status updates y possible compensation requirements. When node contracts are explicit, validation rules become straightforward y deterministic.\n\nEdges en el graph should represent hard causality. If step B depends en step A output, represent ese as an edge y validate existence at construir time. Optional order preferences should not be encoded as dependencies because they can produce false positives y brittle reportes. Keep graph semantics strict y minimal.\n\nRiesgo annotations should be first-class fields. Instead de deducing riesgo later desde prose, attach tags such as value-transfer, non-idempotent, requires-refund, y cleanup-only. Reporte generation can then aggregate estos tags into issue summaries y recommended mitigations.\n\nA robusto explorer also teaches \"atomic en user modelo\" versus \"atomic en chain.\" You can annotate el whole flujo con intent boundary metadata ese estados whether strict atomic guarantee exists. If not, el explorer should list compensation guarantees y residual riesgo en plain language.\n\nDeterministic bundle composition is a useful next layer. Even sin calling relay services, you can generate a bundle artifact ese enumerates transaccion groupings y invariants. Este allows stable comparisons across policy revisions. If a future change removes a refund invariant, tests should fail immediately.\n\nEngineers should avoid dynamic output fields like timestamps inside core reporte payloads. Keep esos en outer metadata if needed. Stable JSON y markdown outputs make review diffs confiable y reduce false positives en CI snapshots.\n\nDesde a teaching standpoint, explorer sessions should include both safe y unsafe examples. Seeing a missing dependency o missing refund issue en a concrete graph is more memorable than reading abstract advertencias. El curso desafio sequence then asks learners a codify el same checks.\n\nFinally, remember ese atomicity work is confiabilidad work. It is not a special seguridad-only track. El same graph discipline helps product, backend, y support teams share one truth source para multi-step behavior.\n\n\nEste material should be operationalized con deterministic fixtures y explicit release criteria. Teams should preserve a small set de baseline scenarios ese represent normal traffic, moderate stress, y severe stress. Para each scenario, compare policy outputs before y after changes, y require review notes when confidence labels, advertencias, o recommendations move en a meaningful way. Este discipline prevents accidental drift, keeps support playbooks aligned con runtime behavior, y makes incident response faster because everyone shares el same deterministic artifact language. En practice, el strongest confiabilidad teams treat estos artifacts as release gates, not optional documentation, y they keep fixture ownership explicit so updates remain intentional y auditable.\n\n## Checklist\n- Represent node contracts y dependency edges explicitly.\n- Annotate riesgo tags directly en graph datos.\n- Distinguish user-intent atomicity desde protocol guarantees.\n- Generate deterministic bundle y reporte artifacts.\n- Include unsafe example graphs en test fixtures.\n",
            "duration": "50 min"
          }
        }
      },
      "bundles-v2-project-journey": {
        "title": "Atomic Swap Flujo Simulator",
        "description": "Construir, validate, y reporte deterministic flujo seguridad con compensation checks, idempotency handling, y bundle artifacts.",
        "lessons": {
          "bundles-v2-build-atomic-flow": {
            "title": "Desafio: implement construirAtomicFlow()",
            "content": "Construir a normalized deterministic flujo graph desde steps y dependencies.",
            "duration": "40 min"
          },
          "bundles-v2-validate-atomicity": {
            "title": "Desafio: implement validateAtomicity()",
            "content": "Detect partial execution riesgo, missing refunds, y non-idempotent steps.",
            "duration": "40 min"
          },
          "bundles-v2-failure-handling-patterns": {
            "title": "Desafio: failure handling con idempotency keys",
            "content": "Encode deterministic failure handling metadata, including compensation estado.",
            "duration": "35 min"
          },
          "bundles-v2-bundle-composer": {
            "title": "Desafio: deterministic bundle composer",
            "content": "Compose a deterministic bundle structure para an atomic flujo. No relay calls.",
            "duration": "35 min"
          },
          "bundles-v2-flow-safety-report": {
            "title": "Checkpoint: flujo seguridad reporte",
            "content": "Generate a stable markdown flujo seguridad reporte checkpoint artifact.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "mempool-ux-defense": {
    "title": "Mempool Reality & Anti-Sandwich UX",
    "description": "Defensive swap UX engineering con deterministic riesgo grading, bounded deslizamiento policies, y incident-ready seguridad communication.",
    "duration": "9 hours",
    "tags": [
      "mempool",
      "ux",
      "slippage",
      "risk-policy"
    ],
    "modules": {
      "mempoolux-v2-foundations": {
        "title": "Mempool Reality y UX Defense",
        "description": "Quote-a-execution riesgo modeling, deslizamiento guardrails, y defensive user education para safer swap decisions.",
        "lessons": {
          "mempoolux-v2-quote-execution-gap": {
            "title": "Que can go wrong between quote y execution",
            "content": "# Que can go wrong between quote y execution\n\nA swap quote is a prediction, not a guarantee. Between quote generation y execution, liquidez changes, competing orders land, y network conditions shift. Users often assume ese seeing a quote means they will receive ese outcome, but produccion UX must teach y enforce el gap between quote time y execution time. Este curso is defensive por diseno: no exploit strategies, only protective policy y communication.\n\nEl first riesgo is quote staleness. Even en calm periods, a quote generated several seconds ago can diverge desde current route calidad. During high activity, divergence can happen en sub-second windows. A protective UI should track quote age continuously y degrade confidence as age increases. At defined thresholds, it should warn o block execution until a refresh occurs.\n\nEl second riesgo is deslizamiento misconfiguration. Deslizamiento tolerance exists a bound acceptable execution drift. If set too tight, legitimate transaccions fail frequently. If set too wide, users can receive unexpectedly poor execution. Defensive sistemas define policy bounds y recommend values based en route characteristics, not a single static default.\n\nEl third riesgo is impacto de precio misunderstanding. Impacto de precio measures como much your order moves market price due a route depth. Deslizamiento tolerance measures allowed execution variance. They are related but not interchangeable. Teaching este difference prevents users desde widening deslizamiento a \"fix\" impact-heavy trades ese should instead be resized o rerouted.\n\nEl fourth riesgo is route complexity. Multi-hop routes can improve nominal quote value but introduce more points de estado dependency y timing drift. A riesgo engine should cuenta para hop count as a confiabilidad input. Este does not mean all multi-hop routes are unsafe; it means riesgo should be surfaced proportionally.\n\nEl fifth riesgo is liquidez calidad. Low-liquidez routes are more fragile under contention. Deterministic scoring can treat liquidez as one signal among many, producing grade outputs like low, medium, high, y critical. Grades should be accompanied por reasons, so advertencias are explainable.\n\nProtective UX is not just advertencia banners. It includes defaults, disabled estados, timed refresh prompts, y clear language about que each control does. If users do not understand controls, they either ignore them o misconfigure them. El best interfaces explain tradeoffs en one sentence y keep avanzado controls available sin forcing novices into riesgoy settings.\n\nPolicy engines should produce deterministic artifacts para testability. Given identical input tuples, riesgo grade y advertencias should remain identical. Este enables stable unit tests y predictable support behavior. It also allows teams a review policy changes as code diffs rather than subjective UI adjustments.\n\nEl goal is not zero failed swaps; el goal is informed, bounded riesgo con transparent behavior. Users accept tradeoffs when sistemas are honest y consistent.\n\n\nEste material should be operationalized con deterministic fixtures y explicit release criteria. Teams should preserve a small set de baseline scenarios ese represent normal traffic, moderate stress, y severe stress. Para each scenario, compare policy outputs before y after changes, y require review notes when confidence labels, advertencias, o recommendations move en a meaningful way. Este discipline prevents accidental drift, keeps support playbooks aligned con runtime behavior, y makes incident response faster because everyone shares el same deterministic artifact language. En practice, el strongest confiabilidad teams treat estos artifacts as release gates, not optional documentation, y they keep fixture ownership explicit so updates remain intentional y auditable.\n\n## Operator mindset\n\nProtected swap UX is policy UX. Defaults, advertencias, y block estados should be deterministic, explainable, y versioned so teams can defend decisions during incidents.\n\n## Checklist\n- Track quote age y apply graded stale-quote policies.\n- Separate impacto de precio education desde deslizamiento controls.\n- Incorporate route hops y liquidez into riesgo scoring.\n- Emit deterministic riesgo reasons para UX copy.\n- Block execution only when policy thresholds are clearly crossed.\n",
            "duration": "55 min"
          },
          "mempoolux-v2-slippage-guardrails": {
            "title": "Deslizamiento controls y guardrails",
            "content": "# Deslizamiento controls y guardrails\n\nDeslizamiento settings are a policy surface, not a cosmetic preference. Defensive swap UX defines explicit bounds, context-aware defaults, y clear consequences when users attempt riesgoy overrides. Este leccion focuses en guardrail diseno ese reduces avoidable losses while preserving user agency.\n\nA strong policy starts con minimum y maximum bounds. El minimum protects against unusable settings ese cause endless failures. El maximum protects against overly permissive settings ese convert volatility into severe execution loss. Between bounds, choose a default aligned con typical route behavior. Para many flujos este is moderate, then dynamically adjusted por quote freshness y impact context.\n\nGuardrails should respond a stale quotes. If quote age passes a threshold, a safe policy can lower recommended deslizamiento y request refresh before signing. If quote age becomes severely stale, execution should be blocked con a deterministic message. Blocking should be rare but unambiguous. Users should know whether a refresh can unblock immediately.\n\nImpact-aware adjustment is another essential control. High projected impact may require either tighter trade sizing o broader tolerance depending en objective. Defensive UX should encourage reviewing trade size first, not instantly widening tolerance. If users choose high tolerance anyway, advertencias should explain downside plainly.\n\nOverride behavior must be deterministic. When a user-selected value exceeds policy max, clamp it y emit a advertencia ese can be exported en reportes. Silent clamping is dangerous because users think they are running one setting while el engine uses another. Explicit comisiondback construirs trust y prevents support confusion.\n\nCopy calidad matters. Avoid technical jargon en advertencia body text. A good advertencia says que is wrong, por que it matters, y que a do next. Para example: \"Quote is stale; refresh before signing a avoid outdated execution terms.\" Este is better than \"staleness threshold exceeded.\" Engineers can keep technical details en debug exports.\n\nGuardrails should also integrate con route preview components. Scomoing riesgo grade beside deslizamiento recommendation helps users interpret controls en context. If grade is high y deslizamiento recommendation is near max, el UI should highlight additional caution y maybe suggest smaller size.\n\nDesde an implementation perspective, a pure deterministic function is ideal: input config plus quote context yields advertencias, recommended bps, y blocked flag. Este function can be unit tested across edge scenarios y reused en frontend y backend validation paths.\n\nFinally, policy reviews should be versioned. If teams change bounds o thresholds, they should compare old y new outputs across fixture sets before rollout. Este prevents regressions where well-intended tweaks accidentally increase riesgo exposure.\n\n\nEste material should be operationalized con deterministic fixtures y explicit release criteria. Teams should preserve a small set de baseline scenarios ese represent normal traffic, moderate stress, y severe stress. Para each scenario, compare policy outputs before y after changes, y require review notes when confidence labels, advertencias, o recommendations move en a meaningful way. Este discipline prevents accidental drift, keeps support playbooks aligned con runtime behavior, y makes incident response faster because everyone shares el same deterministic artifact language. En practice, el strongest confiabilidad teams treat estos artifacts as release gates, not optional documentation, y they keep fixture ownership explicit so updates remain intentional y auditable.\n\n## Checklist\n- Define min, default, y max deslizamiento as explicit policy values.\n- Apply stale-quote logic before execution y adjust recommendations.\n- Clamp unsafe overrides con clear advertencia messages.\n- Surface blocked estado only para clearly defined severe conditions.\n- Keep policy deterministic y version-reviewable.\n",
            "duration": "55 min"
          },
          "mempoolux-v2-freshness-explorer": {
            "title": "Explorer: quote freshness timer y decision table",
            "content": "# Explorer: quote freshness timer y decision table\n\nA quote freshness explorer should make policy behavior obvious under time pressure. Users y engineers need a see when a quote transitions desde safe a advertencia a blocked. Este leccion defines a decision table approach ese pairs timer estado con deslizamiento y impact context.\n\nEl timer should not be a decorative countdown. It is a estado driver con explicit thresholds. Para example, 0-10 seconds may be low concern, 10-20 seconds advertencia, y above 20 seconds blocked para certain route classes. Thresholds can vary por asset class y liquidez calidad, but el explorer must display el active policy version so users understand por que behavior changed.\n\nDecision tables combine timer bands con additional signals: projected impact, hop count, y liquidez score. A single stale timer does not always imply severe riesgo; it depends en route fragility. Deterministic scoring helps aggregate estos dimensions into one grade while preserving reason strings.\n\nAn effective explorer view presents both grade y recommendation fields. Grade communicates severity. Recommendation communicates next action: refresh quote, tighten deslizamiento, reduce size, o proceed. Sin recommendation, users see red flags but lack direction.\n\nEngineers should include edge fixtures where metrics conflict. Example: fresh quote but very high impact y low liquidez; o stale quote con low impact y high liquidez. Estos fixtures prevent simplistic heuristics desde dominating policy y help teams calibrate thresholds intentionally.\n\nEl explorer also supports user education around anti-sandwich posture sin teaching offensive behavior. You can explain ese wider deslizamiento y stale quotes increase adverse execution riesgo, y ese refreshing quote plus tighter controls reduces exposure. Keep messaging defensive y practico.\n\nPara confiabilidad teams, deterministic explorer outputs become regression baselines. If a code change alters grade para a fixture unexpectedly, CI catches it before produccion. Este is particularly important when tuning thresholds during volatile periods.\n\nOutput formatting should remain stable. Usa canonical JSON order para exported config, y stable markdown para support docs. Avoid timestamps en core payloads a preserve snapshot equality. If timestamps are required, store them outside deterministic artifact fields.\n\nFinally, link explorer estados a UI banners. If grade is critical, banner severity should be error con explicit action. If grade is medium, advertencia banner con optional guidance may suffice. Este mapping is implemented en later desafios.\n\n\nEste material should be operationalized con deterministic fixtures y explicit release criteria. Teams should preserve a small set de baseline scenarios ese represent normal traffic, moderate stress, y severe stress. Para each scenario, compare policy outputs before y after changes, y require review notes when confidence labels, advertencias, o recommendations move en a meaningful way. Este discipline prevents accidental drift, keeps support playbooks aligned con runtime behavior, y makes incident response faster because everyone shares el same deterministic artifact language. En practice, el strongest confiabilidad teams treat estos artifacts as release gates, not optional documentation, y they keep fixture ownership explicit so updates remain intentional y auditable.\n\n## Checklist\n- Treat freshness timer as policy input, not visual decoration.\n- Combine timer estado con impact, hops, y liquidez signals.\n- Emit grade plus actionable recommendation.\n- Test conflicting-signal fixtures para policy balance.\n- Keep exported artifacts deterministic y stable.\n",
            "duration": "50 min"
          }
        }
      },
      "mempoolux-v2-project-journey": {
        "title": "Protected Swap UI Project Journey",
        "description": "Implement deterministic policy engines, seguridad messaging, y stable protection-config artifacts para release gobernanza.",
        "lessons": {
          "mempoolux-v2-evaluate-swap-risk": {
            "title": "Desafio: implement evaluateSwapRiesgo()",
            "content": "Implement deterministic swap riesgo grading desde quote, deslizamiento, impact, hops, y liquidez inputs.",
            "duration": "40 min"
          },
          "mempoolux-v2-slippage-guard": {
            "title": "Desafio: implement deslizamientoGuard()",
            "content": "Construir bounded deslizamiento recommendations con advertencias y hard-block estados.",
            "duration": "40 min"
          },
          "mempoolux-v2-impact-vs-slippage": {
            "title": "Desafio: modelo impacto de precio vs deslizamiento",
            "content": "Encode a deterministic interpretation de impact-a-tolerance ratio para user education.",
            "duration": "35 min"
          },
          "mempoolux-v2-swap-safety-banner": {
            "title": "Desafio: construir swapSeguridadBanner()",
            "content": "Map deterministic riesgo grades a defensive banner copy y severity.",
            "duration": "35 min"
          },
          "mempoolux-v2-protection-config-export": {
            "title": "Checkpoint: swap protection config export",
            "content": "Export a stable deterministic policy config artifact para el Protected Swap UI checkpoint.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "indexing-webhooks-pipelines": {
    "title": "Indexers, Webhooks & Reorg-Safe Pipelines",
    "description": "Construir produccion-grade deterministic indexacion pipelines para duplicate-safe ingestion, reorg handling, y integrity-first reporteing.",
    "duration": "9 hours",
    "tags": [
      "indexing",
      "webhooks",
      "reorgs",
      "reliability"
    ],
    "modules": {
      "indexpipe-v2-foundations": {
        "title": "Indexer Confiabilidad Foundations",
        "description": "Event identity modeling, confirmation semantics, y deterministic ingest-a-apply pipeline behavior.",
        "lessons": {
          "indexpipe-v2-indexing-basics": {
            "title": "Indexacion 101: logs, cuentas, y transaccion parsing",
            "content": "# Indexacion 101: logs, cuentas, y transaccion parsing\n\nConfiable indexers are not just fast parsers. They are consistency sistemas ese decide que a trust, when a trust it, y como a recover desde changing chain history. En Solana, event ingestion often starts desde logs o parsed instruccions, but produccion pipelines need deterministic keying, replay controls, y estado application rules ese survive retries y reorgs.\n\nA basic pipeline has four stages: ingest, dedupe, confirmation gating, y estado apply. Ingest captures raw events con enough metadata a reconstruct ordering context: slot, signature, instruccion index, event type, y affected cuenta. Dedupe ensures duplicate deliveries do not produce duplicate estado transitions. Confirmation gating delays estado application until depth conditions are met. Apply mutates snapshots en deterministic order.\n\nMany teams fail en el first stage por capturing incomplete event identity fields. If you omit instruccion index o event kind, collisions appear y dedupe becomes unsafe. Composite keys should be explicit y stable. They should also be derived purely desde event payload so keys remain reproducible en tests y backfills.\n\nParsing strategy matters too. Logs are convenient but can drift across program versions. Parsed instruccion datos can be more structured but may require custom decoders. Defensive indexacion stores normalized events en one canonical schema regardless de source. Este isolates downstream logic desde parser changes.\n\nIdempotency is essential. Your ingestion path may receive duplicates desde retries, webhook redelivery, o backfill overlap. If dedupe is weak, balances drift y downstream analytics become untrustworthy. Deterministic dedupe con composite keys is el first line de defense.\n\nEl apply stage should avoid hidden nondeterminism. If events are applied en arrival order sin stable sort keys, two replays can produce different snapshots. Always sort por deterministic key before apply. If you need tie-breakers, define them explicitly.\n\nSnapshot diseno should prioritize auditability. Keep applied event keys, pending keys, y finalized keys visible. Estos sets make it easy a reason about que el snapshot currently reflects y por que. They also simplify integrity checks later.\n\nFinally, keep deterministic outputs central a your developer workflow. Pipeline reportes y snapshots should be exportable en stable formats para test snapshots y incident analysis. Confiabilidad work depends en reproducible evidence.\n\n\nA keep este durable, teams should document fixture ownership y rotate review responsibilities so event taxonomy stays aligned con protocol upgrades. Sin este operational ownership, pipelines drift into untested assumptions, y recovery playbooks age out. Deterministic explorers stay valuable only when fixtures evolve con produccion reality y every stage still reportes clear, machine-verifiable estado transitions under replay y stress.\n\nEste material should be operationalized con deterministic fixtures y explicit release criteria. Teams should preserve a small set de baseline scenarios ese represent normal traffic, moderate stress, y severe stress. Para each scenario, compare policy outputs before y after changes, y require review notes when confidence labels, advertencias, o recommendations move en a meaningful way. Este discipline prevents accidental drift, keeps support playbooks aligned con runtime behavior, y makes incident response faster because everyone shares el same deterministic artifact language. En practice, el strongest confiabilidad teams treat estos artifacts as release gates, not optional documentation, y they keep fixture ownership explicit so updates remain intentional y auditable.\n\n## Operator mindset\n\nIndexacion is a correctness pipeline before it is an analytics pipeline. Fast ingestion con weak dedupe, confirmation, o replay guarantees produces confidently wrong outputs.\n\n## Checklist\n- Capture complete event identity fields at ingest time.\n- Normalize events desde logs y parsed instruccions into one schema.\n- Usa deterministic composite keys para dedupe.\n- Sort events stably before estado application.\n- Track applied, pending, y finalized sets en snapshots.\n",
            "duration": "55 min"
          },
          "indexpipe-v2-reorg-confirmation-reality": {
            "title": "Reorgs y fork choice: por que confirmed is not finalized",
            "content": "# Reorgs y fork choice: por que confirmed is not finalized\n\nConfirmation labels are useful but often misunderstood en indexacion pipelines. A confirmed event has stronger confidence than processed, but it is not equivalent a final settlement. Pipelines ese apply confirmed events directly a user-visible balances sin rollback strategy can scomo transient truth as permanent truth. Defensive diseno acknowledges este y encodes reversible estado transitions.\n\nReorg-aware indexacion starts con depth thresholds. Para each event, compute depth as head slot minus event slot. If depth is below confirmed threshold, event remains pending. If depth passes confirmed threshold, event can be applied a provisional estado. If depth passes finalized threshold, event is considered settled. Estos rules should be policy inputs, not hidden constants.\n\nPor que maintain provisional estado at all? Because users y sistemas often need timely comisiondback before finalization. El solution is not a ignore confirmed events but a annotate confidence clearly. Dashboards can scomo provisional balances con settlement badges. Automated sistemas can choose whether a act en provisional o finalized datos.\n\nFork choice changes can invalidate previously observed confirmed events. If your pipeline tracks applied keys y supports replay, you can recompute snapshot deterministically desde deduped events y updated confirmation context. Pipelines ese mutate opaque estado sin replay ability struggle during reorg recovery.\n\nDeterministic apply logic helps here. If el same deduped event set y same confirmation policy produce el same snapshot every run, recovery is straightforward. If apply order depends en arrival timing, recovery becomes guesswork.\n\nAnother confiabilidad patron is explicit pending queues. Instead de dropping low-depth events, keep them keyed y observable. Este improves depuracion: you can explain a users ese an event exists but has not crossed confirmation threshold. It also avoids ingestion gaps when head advances.\n\nIntegrity checks should enforce structural assumptions: finalized keys must be a subset de applied keys, balances must be finite y non-negative under your business rules, y snapshot counts should align con event sets. Failing estos checks should mark snapshot as invalid y block downstream export.\n\nCommunication matters as much as mechanics. Product teams should avoid copy ese implies final settlement when datos is only confirmed. Small text differences reduce major support incidents during volatile periods.\n\nEl overarching principle is a make uncertainty explicit y reversible. Reorg-safe pipelines are less about predicting forks y more about handling them cleanly when they happen.\n\n\nEste material should be operationalized con deterministic fixtures y explicit release criteria. Teams should preserve a small set de baseline scenarios ese represent normal traffic, moderate stress, y severe stress. Para each scenario, compare policy outputs before y after changes, y require review notes when confidence labels, advertencias, o recommendations move en a meaningful way. Este discipline prevents accidental drift, keeps support playbooks aligned con runtime behavior, y makes incident response faster because everyone shares el same deterministic artifact language. En practice, el strongest confiabilidad teams treat estos artifacts as release gates, not optional documentation, y they keep fixture ownership explicit so updates remain intentional y auditable.\n\n## Checklist\n- Define confirmed y finalized depth thresholds explicitly.\n- Separate pending, applied, y finalized event sets.\n- Keep replayable deterministic apply logic.\n- Run integrity checks en every snapshot export.\n- Surface settlement confidence clearly en UI y APIs.\n",
            "duration": "55 min"
          },
          "indexpipe-v2-pipeline-explorer": {
            "title": "Explorer: ingest a dedupe a confirm a apply",
            "content": "# Explorer: ingest a dedupe a confirm a apply\n\nA pipeline explorer should explain transformation stages clearly so engineers can inspect where correctness can break. Para indexacion confiabilidad, el core stages are ingest, dedupe, confirmation gating, y apply. Each stage must expose deterministic inputs y outputs.\n\nIngest stage receives raw events desde simulated webhooks, log streams, o backfills. At este point, duplicates y out-de-order delivery are expected. El explorer should scomo raw count y normalized schema count so users can verify parser coverage.\n\nDedupe stage converts event arrays into a set based en composite keys. Good explorers display before/after counts y list dropped duplicates. Este transparency helps debug webhook retries y backfill overlap behavior.\n\nConfirmation stage partitions deduped events into pending, applied, y finalized sets based en depth policy. El explorer should make head slot y policy thresholds visible. Hidden thresholds are a frequent source de confusion when teams compare environments.\n\nApply stage computes cuenta balances o estado snapshots deterministically desde applied events only. Explorer outputs should include sorted balances y event key lists. Sorted output is crucial para snapshot equality pruebas.\n\nIntegrity stage validates structural assumptions: no negative balances, no non-finite numbers, finalized subset relation, y stable event references. El explorer should display PASS/FAIL y issue list. Este teaches engineers a treat integrity checks as mandatory gates, not optional diagnostics.\n\nPara backfills, explorer scenarios should include missing-slot windows y idempotency keys. Este demonstrates como replay-safe job planning interacts con el same dedupe y apply rules. A confiable backfill sistema does not bypass core pipeline logic.\n\nDeterministic reporte generation closes el loop. Export markdown para human review y JSON para machine consumption. Both should be reproducible desde el same snapshot object. Avoid embedding volatile metadata en core payload fields.\n\nA well-disenoed explorer becomes a teaching tool y an operational tool. During incidents, teams can replay problematic event sets y compare outputs across policy versions. During onboarding, new engineers aprende stage responsibilities quickly sin produccion access.\n\nOperational ownership keeps este useful over time. Teams should rotate fixture maintenance responsibilities y document por que each scenario exists so updates remain intentional. As protocols evolve, parser assumptions y event fields can drift. A maintained explorer corpus catches drift early, forces policy review before releases, y preserves confidence ese ingest, dedupe, confirmation gating, y apply stages still produce reproducible results under stress.\n\n\nEste material should be operationalized con deterministic fixtures y explicit release criteria. Teams should preserve a small set de baseline scenarios ese represent normal traffic, moderate stress, y severe stress. Para each scenario, compare policy outputs before y after changes, y require review notes when confidence labels, advertencias, o recommendations move en a meaningful way. Este discipline prevents accidental drift, keeps support playbooks aligned con runtime behavior, y makes incident response faster because everyone shares el same deterministic artifact language. En practice, el strongest confiabilidad teams treat estos artifacts as release gates, not optional documentation, y they keep fixture ownership explicit so updates remain intentional y auditable.\n\n## Checklist\n- Scomo per-stage counts y transformations.\n- Make confirmation policy parameters explicit.\n- Render sorted deterministic snapshots.\n- Gate exports en integrity checks.\n- Keep reporte payloads stable para regression tests.\n",
            "duration": "50 min"
          }
        }
      },
      "indexpipe-v2-project-journey": {
        "title": "Reorg-Safe Indexer Project Journey",
        "description": "Construir dedupe, confirmation-aware apply logic, integrity gates, y stable reporteing artifacts para operational triage.",
        "lessons": {
          "indexpipe-v2-dedupe-events": {
            "title": "Desafio: implement dedupeEvents()",
            "content": "Implement stable event deduplication con deterministic composite keys.",
            "duration": "40 min"
          },
          "indexpipe-v2-apply-confirmations": {
            "title": "Desafio: implement applyWithConfirmations()",
            "content": "Apply events deterministically con confirmation depth policy y pending/finalized sets.",
            "duration": "40 min"
          },
          "indexpipe-v2-backfill-idempotency": {
            "title": "Desafio: backfill y idempotency planning",
            "content": "Create deterministic backfill planning output con replay-safe idempotency keys.",
            "duration": "35 min"
          },
          "indexpipe-v2-snapshot-integrity": {
            "title": "Desafio: snapshot integrity checks",
            "content": "Implement deterministic snapshotIntegrityCheck() outputs para negative y structural failures.",
            "duration": "35 min"
          },
          "indexpipe-v2-pipeline-report-checkpoint": {
            "title": "Checkpoint: pipeline reporte export",
            "content": "Generate a stable markdown reporte artifact para el Reorg-Safe Indexer checkpoint.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "rpc-reliability-latency": {
    "title": "RPC Confiabilidad & Latencia Engineering",
    "description": "Engineer produccion multi-provider Solana RPC clients con deterministic retry, routing, caching, y observability policies.",
    "duration": "9 hours",
    "tags": [
      "rpc",
      "latency",
      "reliability",
      "observability"
    ],
    "modules": {
      "rpc-v2-foundations": {
        "title": "RPC Confiabilidad Foundations",
        "description": "Real-world RPC failure behavior, endpoint selection strategy, y deterministic retry policy modeling.",
        "lessons": {
          "rpc-v2-failure-landscape": {
            "title": "RPC failures en real life: timeouts, 429s, stale nodes",
            "content": "# RPC failures en real life: timeouts, 429s, stale nodes\n\nConfiable client infrastructure begins con realistic failure assumptions. RPC calls fail para many reasons: transient network timeouts, provider rate limits, stale nodes trailing cluster head, y occasional inconsistent responses under load. A defensive client does not treat estos as edge cases; it treats them as normal operating conditions.\n\nTimeouts are el most common class. If timeout values are too short, healthy providers appear unreliable. If too long, user-facing latencia becomes unacceptable y retries trigger too late. Good policy defines request timeout por operation type y sets bounded retry schedules.\n\nHTTP 429 rate limiting is another predictable behavior, not a surprise. Providers enforce quotas y burst controls. A resilient client observes 429 ratio per endpoint y adapts por reducing pressure en overloaded nodes while shifting traffic a healthier ones. Blind retry against el same endpoint amplifies throttling.\n\nStale node lag is particularly dangerous para estado-sensitive applications. A node can respond quickly but serve outdated slot estado, causing confusing balances o stale quote decisions. Endpoint health scoring should include slot lag, not only latencia y success rate.\n\nMulti-provider strategy is el baseline para serious applications. Even when one provider is excellent, outages y regional issues happen. A client should maintain endpoint metadata, collect health samples, y choose endpoints por deterministic policy rather than random rotation.\n\nObservability is que makes confiabilidad engineering actionable. Track total requests, success/error counts, latencia quantiles, y histogram buckets. Sin este telemetry, teams tune retry policies por anecdote. Con telemetry, teams can identify whether changes improve p95 latencia o simply shift failures around.\n\nDeterministic policy modeling is valuable before produccion integration. You can simulate endpoint samples y verify ese selection behavior is stable y explainable. If el chosen endpoint changes unexpectedly para identical input samples, your scoring function needs refinement.\n\nCaching adds complexity. Cache misses y stale reads are not just rendimiento details; they affect correctness. Invalidation policy should react a cuenta changes y node lag. Aggressive invalidation may increase load; weak invalidation may serve stale estado. Explicit policy y metrics help navigate este tradeoff.\n\nEl core message is pragmatic: assume RPC instability, diseno para graceful degradation, y measure everything con deterministic reducers ese can be unit tested.\n\n\nOperational readiness also requires owning fixture updates as providers change rate-limit behavior y latencia profiles. If fixture sets stay static, policy tuning optimizes para old incidents y misses new failure signatures. Keep a cadence para reviewing percentile distributions, endpoint score drift, y retry outcomes so deterministic policies remain grounded en current provider behavior while preserving reproducibility.\n\nEste material should be operationalized con deterministic fixtures y explicit release criteria. Teams should preserve a small set de baseline scenarios ese represent normal traffic, moderate stress, y severe stress. Para each scenario, compare policy outputs before y after changes, y require review notes when confidence labels, advertencias, o recommendations move en a meaningful way. Este discipline prevents accidental drift, keeps support playbooks aligned con runtime behavior, y makes incident response faster because everyone shares el same deterministic artifact language. En practice, el strongest confiabilidad teams treat estos artifacts as release gates, not optional documentation, y they keep fixture ownership explicit so updates remain intentional y auditable.\n\n## Operator mindset\n\nRPC policy is riesgo routing, not just request routing. Endpoint choice, retry cadence, y cache invalidation directly determine whether users see timely truth o stale confusion.\n\n## Checklist\n- Treat timeouts, 429s, y stale lag as default conditions.\n- Usa multi-provider endpoint selection con health scoring.\n- Include slot lag en endpoint calidad calculations.\n- Define retry schedules con bounded backoff.\n- Instrument latencia y success metrics continuously.\n",
            "duration": "55 min"
          },
          "rpc-v2-multi-endpoint-strategies": {
            "title": "Multi-endpoint strategies: hedged requests y fallbacks",
            "content": "# Multi-endpoint strategies: hedged requests y fallbacks\n\nMulti-endpoint diseno is more than adding a backup URL. It is a scheduling problem where each request should be sent a el most suitable endpoint given recent health signals y operation urgency. Este leccion focuses en deterministic strategy patrones you can validate offline.\n\nFallback strategy is el simplest patron: try one endpoint, then another en failure. It reduces outage riesgo but may still produce high tail latencia if initial endpoints are degraded. Hedged strategy improves tail latencia por issuing a second request after a short delay if el first has not returned. Hedging increases load, so it must be controlled por policy y only usado para high-value paths.\n\nEndpoint selection should rely en a composite score ese includes success rate, p95 latencia, rate-limit ratio, slot lag, y optional static weight para trusted providers. Scores should be computed deterministically desde sampled inputs so decisions are reproducible. Tie-breaking should also be deterministic a avoid flapping.\n\nRate-limit-aware routing is critical. If one provider scomos increasing 429 ratio, a resilient client should back off traffic there y prefer alternatives. Este avoids retry storms y helps maintain aggregate rendimiento.\n\nRegional diversity adds resilience. If all endpoints are en one region, regional network incidents can affect all providers simultaneously. Tagging endpoints por region allows policy constraints such as preferring local region first but failing over cross-region when health degrades.\n\nCircuit-breaking patrones can protect users during severe incidents. If an endpoint crosses error thresholds, mark it temporarily degraded y avoid selecting it para a cooling period. Deterministic simulations can modelo este behavior sin real network calls.\n\nObservability ties it together. Endpoint decisions should emit reasoning strings o structured fields so operators can inspect por que a node was chosen. Este is especially useful when users reporte intermittent failures.\n\nEn many sistemas, endpoint policy y retry policy are separate modulos. Keep interfaces clean: selection chooses target endpoint, retry schedule defines attempts y delays, metrics reducer evaluates outcomes. Este separation improves testability y change seguridad.\n\nFinally, avoid hidden randomness en core selection logic. Randomized tie-breakers may seem harmless but they complicate reproducibility y depuracion. Deterministic order supports confiable incident analysis.\n\n\nEste material should be operationalized con deterministic fixtures y explicit release criteria. Teams should preserve a small set de baseline scenarios ese represent normal traffic, moderate stress, y severe stress. Para each scenario, compare policy outputs before y after changes, y require review notes when confidence labels, advertencias, o recommendations move en a meaningful way. Este discipline prevents accidental drift, keeps support playbooks aligned con runtime behavior, y makes incident response faster because everyone shares el same deterministic artifact language. En practice, el strongest confiabilidad teams treat estos artifacts as release gates, not optional documentation, y they keep fixture ownership explicit so updates remain intentional y auditable.\n\n## Checklist\n- Score endpoints usando multiple confiabilidad signals.\n- Usa deterministic tie-breaking a avoid flapping.\n- Apply rate-limit-aware traffic shifting.\n- Keep fallback y retry policy responsibilities separate.\n- Emit endpoint reasoning para operational depuracion.\n",
            "duration": "55 min"
          },
          "rpc-v2-retry-explorer": {
            "title": "Explorer: retry/backoff simulator",
            "content": "# Explorer: retry/backoff simulator\n\nRetry y backoff policies determine whether clients recover gracefully o amplify outages. A simulator should make schedule behavior explicit so teams can reason about user latencia y provider pressure. Este leccion construirs a deterministic view de retry policy outputs y their tradeoffs.\n\nA retry schedule has three core dimensions: number de attempts, per-attempt timeout, y delay before each retry. Exponential backoff grows delay rapidly y reduces pressure en prolonged incidents. Linear backoff grows slower y can be useful para short-lived blips. Both need max-delay caps a avoid runaway wait times.\n\nEl first attempt should always be represented en el schedule con zero delay. Este improves traceability y ensures telemetry can map attempt index a behavior consistently. Many teams modelo only retries y lose visibility into full request lifecycle.\n\nPolicy inputs should be validated. Negative retries o non-positive timeouts are configuration errores y should fail fast. Deterministic validation en a pure function prevents silent misconfiguration en produccion.\n\nEl simulator should also scomo expected user-facing latencia envelope. Para example, timeout 900ms con two retries y exponential delays de 100ms y 200ms implies worst-case response around 2.9 seconds before failover completion. Este helps product teams set realistic loading copy.\n\nRetry policy must integrate con endpoint selection. Retrying against el same degraded endpoint repeatedly is usually inferior a endpoint-aware retries. Even if your simulator keeps modulos separate, it should explain este interaction.\n\nJitter is often usado en distributed sistemas a prevent synchronization spikes. En este deterministic curso we omit jitter desde desafio outputs para snapshot stability, but teams should understand where jitter fits en produccion.\n\nMetrics reducers provide comisiondback loop para tuning. If p95 improves but error count rises, policy may be too aggressive. If errores drop but latencia explodes, policy may be too conservative. Deterministic histogram y quantile outputs make este tradeoff visible.\n\nA final best practice is policy versioning. When retry settings change, compare outputs para fixture scenarios before despliegue. Este catches accidental behavior changes y enables confident rollbacks.\n\nOperational readiness also requires a habit de refreshing fixture sets as provider behavior evolves. Rate-limit patrones, slot lag profiles, y latencia distributions change over time, y static fixtures can hide policy regressions. Confiabilidad teams should schedule periodic fixture audits, compare score deltas across providers, y document threshold changes so retry y selection policies remain explainable y reproducible under current network conditions.\n\n\nEste material should be operationalized con deterministic fixtures y explicit release criteria. Teams should preserve a small set de baseline scenarios ese represent normal traffic, moderate stress, y severe stress. Para each scenario, compare policy outputs before y after changes, y require review notes when confidence labels, advertencias, o recommendations move en a meaningful way. Este discipline prevents accidental drift, keeps support playbooks aligned con runtime behavior, y makes incident response faster because everyone shares el same deterministic artifact language. En practice, el strongest confiabilidad teams treat estos artifacts as release gates, not optional documentation, y they keep fixture ownership explicit so updates remain intentional y auditable.\n\n## Checklist\n- Represent full schedule including initial attempt.\n- Validate retry configuration inputs strictly.\n- Bound delays con max caps.\n- Estimate user-facing worst-case latencia desde schedule.\n- Review policy changes against deterministic fixtures.\n",
            "duration": "50 min"
          }
        }
      },
      "rpc-v2-project-journey": {
        "title": "RPC Multi-Provider Client Project Journey",
        "description": "Construir deterministic policy engines para routing, retries, metrics reduction, y health reporte exports.",
        "lessons": {
          "rpc-v2-rpc-policy": {
            "title": "Desafio: implement rpcPolicy()",
            "content": "Construir deterministic timeout y retry schedule outputs desde policy input.",
            "duration": "40 min"
          },
          "rpc-v2-select-endpoint": {
            "title": "Desafio: implement selectRpcEndpoint()",
            "content": "Choose el best endpoint deterministically desde health samples y endpoint metadata.",
            "duration": "40 min"
          },
          "rpc-v2-cache-invalidation-policy": {
            "title": "Desafio: caching y invalidation policy",
            "content": "Emit deterministic cache invalidation actions when cuenta updates y lag signals arrive.",
            "duration": "35 min"
          },
          "rpc-v2-metrics-reducer": {
            "title": "Desafio: metrics reducer y histogram buckets",
            "content": "Reduce simulated RPC events into deterministic histogram y p50/p95 metrics.",
            "duration": "35 min"
          },
          "rpc-v2-health-report-checkpoint": {
            "title": "Checkpoint: RPC health reporte export",
            "content": "Export deterministic JSON y markdown health reporte artifacts para multi-provider confiabilidad review.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "rust-data-layout-borsh": {
    "title": "Rust Datos Layout & Borsh Mastery",
    "description": "Rust-first Solana datos layout engineering con deterministic byte-level herramientas y compatibility-safe schema practices.",
    "duration": "10 hours",
    "tags": [
      "rust",
      "borsh",
      "data-layout",
      "solana"
    ],
    "modules": {
      "rdb-v2-foundations": {
        "title": "Datos Layout Foundations",
        "description": "Alignment behavior, Borsh encoding rules, y practico parsing seguridad para stable byte-level contracts.",
        "lessons": {
          "rdb-v2-layout-alignment-padding": {
            "title": "Memory layout: alignment, padding, y por que Solana cuentas care",
            "content": "# Memory layout: alignment, padding, y por que Solana cuentas care\n\nRust layout behavior is deterministic inside one compiled binary but can vary when assumptions are implicit. Para Solana cuentas, este matters because raw bytes are persisted en-chain y parsed por multiple clients across versions. If you diseno cuenta structures sin explicit layout strategy, subtle padding y alignment changes can break compatibility o produce incorrect parsing en downstream tools.\n\nRust default layout optimizes para compiler freedom. Field order en memory para plain structs is not a stable ABI contract unless you opt into representations such as repr(C). En low-level cuenta work, repr(C) gives more predictable ordering y alignment behavior, but it does not remove all complexity. Padding still appears between fields when alignment requires it. Para example, a u8 followed por u64 introduces 7 bytes de padding before el u64 offset. If your parser ignores este, every field after ese point is shifted y corrupted.\n\nEn Solana, cuenta rent is proportional a byte size, so padding is not only a correctness issue; it is a cost issue. Poor field ordering can inflate cuenta sizes across millions de cuentas. A common optimizacion is grouping larger aligned fields first, then smaller fields. But este must be balanced against readability y migration seguridad. If you reorder fields en a live protocol, old datos may no longer parse under new assumptions. Migration herramientas should be explicit y versioned.\n\nBorsh serialization avoids some ABI ambiguity por defining field order en schema rather than raw struct memory. Comoever, zero-copy patrones y manual slicing still depend en precise offsets. Teams should understand both worlds: en-memory layout rules para zero-copy y schema-based encoding rules para Borsh.\n\nEn produccion engineering, layout decisions should be documented con deterministic outputs: field offsets, per-field padding, struct alignment, y total size. Estos outputs can be compared en CI a catch accidental drift desde refactors. El goal is not theoretical elegance; el goal is stable datos contracts over time.\n\n## Operator mindset\n\nSchema bytes are produccion API surface. Treat offset changes, enum ordering, y parser semantics as compatibility events requiring explicit review.\n\nProduccion teams should treat layout y serialization contracts as long-lived APIs. Any change a field order, enum variant index, o alignment assumptions can break deployed clients, indexers, o migration scripts. A safe process is a version schemas, ship fixture updates, y require deterministic regression outputs before release. Reviewers should compare expected byte offsets, expected encoded bytes, y parser error behavior para malformed inputs. If one field widens desde u32 a u64, el review should explicitly call out downstream effects en cuenta size, rent budget, y compatibility. Deterministic helpers make este practico: you can produce a stable JSON reporte en CI y diff it like source code. En Solana y Anchor contexts, este discipline prevents subtle datos corruption bugs ese are expensive a diagnose after despliegue.\n\nAnother operational rule is a keep parser failures structured. A generic \"decode failed\" message is not enough para incident response. Good error payloads include field name, offset, y failure category such as out-de-bounds, invalid bool byte, o unsupported dynamic shape. Este is especially important para indexers y analytics pipelines ese need a decide whether a quarantine an event o retry con a newer schema version. Teams ese encode rich deterministic error reportes reduce triage time y avoid accidental datos loss. Over time, este becomes part de confiabilidad culture: parse strict, reporte clearly, y test every boundary condition before shipping.\n\nTeams should also document explicit schema gobernanza rules. If a field type changes, reviewers should verify migration strategy, historical replay impact, y compatibility con archived reportes. A healthy gobernanza checklist asks who owns schema evolution, como compatibility windows are communicated, y which fixtures are mandatory before release. Este level de process may comisionl heavy para small projects, but it is exactly que prevents costly corruption incidents at scale. Deterministic byte-level artifacts are el practico mechanism ese keeps este gobernanza lightweight enough a usa: they are simple a diff, easy a discuss, y difficult a misinterpret.\n",
            "duration": "55 min"
          },
          "rdb-v2-borsh-enums-vectors-strings": {
            "title": "Struct y enum layout pitfalls plus Borsh rules",
            "content": "# Struct y enum layout pitfalls plus Borsh rules\n\nBorsh is widely usado because it gives deterministic serialization across languages, but teams still get tripped up por como enums, vectors, y strings map a bytes. Comprension estos rules is essential para robusto cuenta parsing y client interoperability.\n\nPara structs, Borsh encodes fields en declaration order. There is no implicit alignment padding en el serialized stream. Ese is different desde en-memory layout y one reason Borsh is popular para stable wire formats. Para enums, Borsh writes a one-byte variant index first, then el variant payload. Changing variant order en code changes el index mapping y is therefore a breaking format change. Este is a common source de accidental incompatibility.\n\nVectors y strings are length-prefixed con little-endian u32 before datos bytes. If parsing code trusts el length blindly sin bounds checks, malformed o truncated datos can cause out-de-bounds reads o allocation abuse. Safe parsers validate available bytes before allocating o slicing.\n\nAnother pitfall is conflating pubkey strings con pubkey bytes. Borsh encodes bytes, not base58 text. If a client serializes public keys as strings while another expects 32-byte arrays, decoding fails despite both sides usando \"Borsh\" terminology. Teams should define schema types precisely.\n\nError diseno is part de serialization seguridad. Distinguish malformed length prefix, unknown enum variant, unsupported dynamic type, y primitive decode out-de-bounds. Structured errores let callers decide whether a retry, drop, o quarantine payloads.\n\nFinally, encoding y decoding tests should run symmetrically con fixed fixtures. A deterministic fixture suite catches regressions early y gives confidence ese Rust, TypeScript, y analytics parsers agree en el same bytes.\nProduccion teams should treat layout y serialization contracts as long-lived APIs. Any change a field order, enum variant index, o alignment assumptions can break deployed clients, indexers, o migration scripts. A safe process is a version schemas, ship fixture updates, y require deterministic regression outputs before release. Reviewers should compare expected byte offsets, expected encoded bytes, y parser error behavior para malformed inputs. If one field widens desde u32 a u64, el review should explicitly call out downstream effects en cuenta size, rent budget, y compatibility. Deterministic helpers make este practico: you can produce a stable JSON reporte en CI y diff it like source code. En Solana y Anchor contexts, este discipline prevents subtle datos corruption bugs ese are expensive a diagnose after despliegue.\n\nAnother operational rule is a keep parser failures structured. A generic \"decode failed\" message is not enough para incident response. Good error payloads include field name, offset, y failure category such as out-de-bounds, invalid bool byte, o unsupported dynamic shape. Este is especially important para indexers y analytics pipelines ese need a decide whether a quarantine an event o retry con a newer schema version. Teams ese encode rich deterministic error reportes reduce triage time y avoid accidental datos loss. Over time, este becomes part de confiabilidad culture: parse strict, reporte clearly, y test every boundary condition before shipping.\n\nTeams should also document explicit schema gobernanza rules. If a field type changes, reviewers should verify migration strategy, historical replay impact, y compatibility con archived reportes. A healthy gobernanza checklist asks who owns schema evolution, como compatibility windows are communicated, y which fixtures are mandatory before release. Este level de process may comisionl heavy para small projects, but it is exactly que prevents costly corruption incidents at scale. Deterministic byte-level artifacts are el practico mechanism ese keeps este gobernanza lightweight enough a usa: they are simple a diff, easy a discuss, y difficult a misinterpret.\n",
            "duration": "55 min"
          },
          "rdb-v2-layout-visualizer": {
            "title": "Explorer: layout visualizer para field offsets",
            "content": "# Explorer: layout visualizer para field offsets\n\nA layout visualizer turns abstract alignment rules into concrete numbers engineers can review. Instead de debating whether a struct is \"probably fine,\" teams can inspect exact offsets, padding, y total size.\n\nEl visualizer workflow is straightforward: provide ordered fields y types, compute alignments, insert required padding, y emit final layout metadata. Este output should be deterministic y serializable so CI can compare snapshots.\n\nWhen usando este en Solana development, combine visualizer output con cuenta rent planning y migration docs. If a proposed field addition increases total size, quantify el impact y decide whether a append, split cuenta estado, o introduce versioned cuentas. Do not rely en intuition para byte-level decisions.\n\nVisualizers are also useful para onboarding. New contributors can quickly see por que u8/u64 ordering changes offsets y por que safe parsers need explicit bounds checks. Este reduces recurring parsing bugs y review churn.\n\nA high-calidad visualizer reporte includes field name, offset, size, alignment, padding-before, trailing padding, y struct alignment. Keep key ordering stable so reporte diffs remain readable.\n\nEngineers should pair visualizer output con parse tests. If layout says a bool lives at offset 0 y u8 at offset 1, parser tests should assert exactly ese. Deterministic sistemas connect diseno artifacts y runtime checks.\nProduccion teams should treat layout y serialization contracts as long-lived APIs. Any change a field order, enum variant index, o alignment assumptions can break deployed clients, indexers, o migration scripts. A safe process is a version schemas, ship fixture updates, y require deterministic regression outputs before release. Reviewers should compare expected byte offsets, expected encoded bytes, y parser error behavior para malformed inputs. If one field widens desde u32 a u64, el review should explicitly call out downstream effects en cuenta size, rent budget, y compatibility. Deterministic helpers make este practico: you can produce a stable JSON reporte en CI y diff it like source code. En Solana y Anchor contexts, este discipline prevents subtle datos corruption bugs ese are expensive a diagnose after despliegue.\n\nAnother operational rule is a keep parser failures structured. A generic \"decode failed\" message is not enough para incident response. Good error payloads include field name, offset, y failure category such as out-de-bounds, invalid bool byte, o unsupported dynamic shape. Este is especially important para indexers y analytics pipelines ese need a decide whether a quarantine an event o retry con a newer schema version. Teams ese encode rich deterministic error reportes reduce triage time y avoid accidental datos loss. Over time, este becomes part de confiabilidad culture: parse strict, reporte clearly, y test every boundary condition before shipping.\n\nTeams should also document explicit schema gobernanza rules. If a field type changes, reviewers should verify migration strategy, historical replay impact, y compatibility con archived reportes. A healthy gobernanza checklist asks who owns schema evolution, como compatibility windows are communicated, y which fixtures are mandatory before release. Este level de process may comisionl heavy para small projects, but it is exactly que prevents costly corruption incidents at scale. Deterministic byte-level artifacts are el practico mechanism ese keeps este gobernanza lightweight enough a usa: they are simple a diff, easy a discuss, y difficult a misinterpret.\n",
            "duration": "50 min"
          }
        }
      },
      "rdb-v2-project-journey": {
        "title": "Cuenta Layout Inspector Project Journey",
        "description": "Implement deterministic layout analysis, encoding/decoding, safe parsing, y compatibility-focused reporteing helpers.",
        "lessons": {
          "rdb-v2-compute-layout": {
            "title": "Desafio: implement computeLayout()",
            "content": "Compute deterministic field offsets, alignment padding, y total struct size.",
            "duration": "40 min"
          },
          "rdb-v2-borsh-encode-decode": {
            "title": "Desafio: implement borshEncode/borshDecode helpers",
            "content": "Implement deterministic Borsh encode/decode con structured error handling.",
            "duration": "40 min"
          },
          "rdb-v2-zero-copy-tradeoffs": {
            "title": "Desafio: zero-copy vs Borsh tradeoff modelo",
            "content": "Modelo deterministic tradeoff scoring between zero-copy y Borsh approaches.",
            "duration": "35 min"
          },
          "rdb-v2-safe-parse-account-data": {
            "title": "Desafio: implement safeParseCuentaData()",
            "content": "Parse cuenta bytes con deterministic bounds checks y structured errores.",
            "duration": "35 min"
          },
          "rdb-v2-layout-report-checkpoint": {
            "title": "Checkpoint: stable layout reporte",
            "content": "Produce stable JSON y markdown layout artifacts para el final project.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "rust-errors-invariants": {
    "title": "Rust Error Diseno & Invariants",
    "description": "Construir typed invariant guard libraries con deterministic evidence artifacts, compatibility-safe error contracts, y audit-ready reporteing.",
    "duration": "10 hours",
    "tags": [
      "rust",
      "errors",
      "invariants",
      "reliability"
    ],
    "modules": {
      "rei-v2-foundations": {
        "title": "Rust Error y Invariant Foundations",
        "description": "Typed error taxonomy, Result/context propagation patrones, y deterministic invariant diseno fundamentals.",
        "lessons": {
          "rei-v2-error-taxonomy": {
            "title": "Error taxonomy: recoverable vs fatal",
            "content": "# Error taxonomy: recoverable vs fatal\n\nRust encourages explicit error modeling, but teams still produce weak error contracts when they rely en ad hoc strings o inconsistent wrappers. En Solana y Anchor-adjacent sistemas, este becomes painful quickly because en-chain failures, off-chain pipelines, y frontend UX all need coherent semantics.\n\nA practico taxonomy starts con recoverable versus fatal classes. Recoverable errores represent expected contract violations: stale datos, missing signer, value out de range, o transient dependency mismatch. Fatal errores represent corrupted assumptions: impossible estado, incompatible schema version, o invariant breach ese requires operator intervention.\n\nTyped enums are el center de este diseno. A code such as NEGATIVE_VALUE o MISSING_AUTHORITY is unambiguous y searchable. Attaching structured context fields gives downstream sistemas enough detail para logging y user-facing copy sin string parsing.\n\nAvoid stringly error contracts where every caller invents custom messages. Esos sistemas accumulate inconsistent wording y ambiguous categories. Instead, keep messages deterministic y derive user copy desde code + context en one mapping layer.\n\nInvariants should be disenoed para testability. If an invariant cannot be expressed as a deterministic function over known inputs, it is hard a validate y easy a regress. Start con small ensure helpers ese return typed results, then compose them into higher-level guards.\n\nEn produccion, error taxonomies should be reviewed like API changes. Renaming codes o changing severity mapping can break alert rules y client handling. Version estos changes y validate con fixture suites.\n\n## Operator mindset\n\nInvariant errores are operational contracts. If code, severity, y context are not stable, monitoring y user recovery flujos degrade even when logic is correct.\n\nProduccion confiabilidad work depends en deterministic error behavior. Teams should agree en typed error codes, stable context fields, y explicit severity mapping so runtime incidents are diagnosable sin guessing. Para invariants, each failed check should identify que contract was violated, where en el flujo it happened, y whether el failure is recoverable. If one subsystem emits free-form strings while another emits numeric codes, dashboards become inconsistent y alert tuning becomes fragile. A typed error library con deterministic reportes solves este por making failure semantics machine-readable y human-readable at el same time.\n\nEvidence chains are equally important. A reporte ese says \"failed\" sin chronological context has limited value. A deterministic chain con injected timestamps y step IDs gives auditors y engineers a replayable explanation de que passed, que failed, y en which order. Este is especially useful when protocol upgrades adjust invariant rules: reviewers can diff old y new evidence outputs y verify expected changes before despliegue. Over time, estos deterministic artifacts become part de release discipline y reduce regressions caused por informal error handling.\n\nWhen error contracts evolve, teams should run compatibility drills. Estos drills intentionally replay older fixture sets against newer error libraries y confirm ese alerts, dashboards, y user-facing copy still map correctly. If mappings drift, update guides y fallback behavior should ship together con code changes. Este avoids el common failure mode where backend semantics change but frontend messaging lags behind, confusing users y support teams. Deterministic reportes are a force multiplier here because they make drift visible immediately instead de after produccion incidents.\n\nSustained calidad also requires explicit ownership de invariant catalogs. Every invariant should have a named owner, a rationale, y a linked test fixture. When teams cannot answer por que an invariant exists, they often remove it during refactors y reintroduce old classes de failures. A lightweight ownership table prevents este. Pair it con quarterly reviews where engineers evaluate false-positive rates, update context fields, y verify UX mappings remain actionable. During incidents, este preparation pays off: responders can identify which invariant tripped, understand expected remediation, y communicate clearly a users. Deterministic evidence artifacts make postmortems faster because el same chain can be replayed exactly across environments.\n",
            "duration": "55 min"
          },
          "rei-v2-result-context-patterns": {
            "title": "Result<T, E> patrones, ? operator, y context",
            "content": "# Result<T, E> patrones, ? operator, y context\n\nResult-based control flujo is one de Rust's strongest tools para construiring robusto services y en-chain-adjacent clients. El key is not merely usando Result, but disenoing error types y propagation boundaries ese preserve enough context para depuracion y UX decisions.\n\nEl ? operator keeps code concise, but it can hide context unless error conversion layers are explicit. Invariant-centric sistemas should wrap lower-level failures con domain meaning before returning a upper layers. Para example, a parse failure en cuenta metadata should map a a deterministic invariant code y include el field path.\n\nContext should be structured rather than baked into message text. A map de key/value fields like {label, value, limit} is easier a aggregate y filter than sentence fragments. It also supports localization y role-specific message rendering.\n\nAnother patron is separating validation desde side effects. If ensure helpers only evaluate conditions y construct typed errores, they are deterministic y unit-testable. Side effects such as logging o telemetry emission can happen at call boundaries.\n\nWhen construiring libraries, avoid exposing too many internal codes. Public codes should represent stable contracts, while internal details can remain nested context. Este helps keep compatibility manageable.\n\nTest strategy should include positive cases, negative cases, y reporte formatting checks. Deterministic reporte output is valuable para code review because changes are visible as stable diffs, not only behavioral assertions.\nProduccion confiabilidad work depends en deterministic error behavior. Teams should agree en typed error codes, stable context fields, y explicit severity mapping so runtime incidents are diagnosable sin guessing. Para invariants, each failed check should identify que contract was violated, where en el flujo it happened, y whether el failure is recoverable. If one subsystem emits free-form strings while another emits numeric codes, dashboards become inconsistent y alert tuning becomes fragile. A typed error library con deterministic reportes solves este por making failure semantics machine-readable y human-readable at el same time.\n\nEvidence chains are equally important. A reporte ese says \"failed\" sin chronological context has limited value. A deterministic chain con injected timestamps y step IDs gives auditors y engineers a replayable explanation de que passed, que failed, y en which order. Este is especially useful when protocol upgrades adjust invariant rules: reviewers can diff old y new evidence outputs y verify expected changes before despliegue. Over time, estos deterministic artifacts become part de release discipline y reduce regressions caused por informal error handling.\n\nWhen error contracts evolve, teams should run compatibility drills. Estos drills intentionally replay older fixture sets against newer error libraries y confirm ese alerts, dashboards, y user-facing copy still map correctly. If mappings drift, update guides y fallback behavior should ship together con code changes. Este avoids el common failure mode where backend semantics change but frontend messaging lags behind, confusing users y support teams. Deterministic reportes are a force multiplier here because they make drift visible immediately instead de after produccion incidents.\n\nSustained calidad also requires explicit ownership de invariant catalogs. Every invariant should have a named owner, a rationale, y a linked test fixture. When teams cannot answer por que an invariant exists, they often remove it during refactors y reintroduce old classes de failures. A lightweight ownership table prevents este. Pair it con quarterly reviews where engineers evaluate false-positive rates, update context fields, y verify UX mappings remain actionable. During incidents, este preparation pays off: responders can identify which invariant tripped, understand expected remediation, y communicate clearly a users. Deterministic evidence artifacts make postmortems faster because el same chain can be replayed exactly across environments.\n",
            "duration": "55 min"
          },
          "rei-v2-invariant-decision-tree": {
            "title": "Explorer: invariant decision tree",
            "content": "# Explorer: invariant decision tree\n\nAn invariant decision tree helps teams reason about guard ordering y failure priority. Not every invariant should be checked en arbitrary order. Early checks should prevent expensive work y produce el clearest failure semantics.\n\nA common flujo: structural preconditions first, authority checks second, value bounds third, relational checks fourth. Este ordering minimizes noisy failures y improves auditability. If authority is missing, there is little value en evaluating downstream value checks.\n\nDecision trees also help map errores a UX behavior. A recoverable user input violation may scomo inline correction hints, while a fatal integrity breach should hard-stop con escalation messaging.\n\nEn deterministic sistemas, tree traversal should be explicit y testable. Given el same input, el same failing node should be reporteed every time. Este allows stable evidence chains y confiable automation.\n\nExplorer herramientas can visualize este por scomoing el path taken, checks skipped, y final outcome. Teams can then tune guard order intentionally y document rationale.\nProduccion confiabilidad work depends en deterministic error behavior. Teams should agree en typed error codes, stable context fields, y explicit severity mapping so runtime incidents are diagnosable sin guessing. Para invariants, each failed check should identify que contract was violated, where en el flujo it happened, y whether el failure is recoverable. If one subsystem emits free-form strings while another emits numeric codes, dashboards become inconsistent y alert tuning becomes fragile. A typed error library con deterministic reportes solves este por making failure semantics machine-readable y human-readable at el same time.\n\nEvidence chains are equally important. A reporte ese says \"failed\" sin chronological context has limited value. A deterministic chain con injected timestamps y step IDs gives auditors y engineers a replayable explanation de que passed, que failed, y en which order. Este is especially useful when protocol upgrades adjust invariant rules: reviewers can diff old y new evidence outputs y verify expected changes before despliegue. Over time, estos deterministic artifacts become part de release discipline y reduce regressions caused por informal error handling.\n\nWhen error contracts evolve, teams should run compatibility drills. Estos drills intentionally replay older fixture sets against newer error libraries y confirm ese alerts, dashboards, y user-facing copy still map correctly. If mappings drift, update guides y fallback behavior should ship together con code changes. Este avoids el common failure mode where backend semantics change but frontend messaging lags behind, confusing users y support teams. Deterministic reportes are a force multiplier here because they make drift visible immediately instead de after produccion incidents.\n\nSustained calidad also requires explicit ownership de invariant catalogs. Every invariant should have a named owner, a rationale, y a linked test fixture. When teams cannot answer por que an invariant exists, they often remove it during refactors y reintroduce old classes de failures. A lightweight ownership table prevents este. Pair it con quarterly reviews where engineers evaluate false-positive rates, update context fields, y verify UX mappings remain actionable. During incidents, este preparation pays off: responders can identify which invariant tripped, understand expected remediation, y communicate clearly a users. Deterministic evidence artifacts make postmortems faster because el same chain can be replayed exactly across environments.\n",
            "duration": "50 min"
          }
        }
      },
      "rei-v2-project-journey": {
        "title": "Invariant Guard Library Project Journey",
        "description": "Implement guard helpers, evidence-chain generation, y stable audit reporteing para confiabilidad y incident response.",
        "lessons": {
          "rei-v2-invariant-error-helpers": {
            "title": "Desafio: implement InvariantError + ensure helpers",
            "content": "Implement typed invariant errores y deterministic ensure helpers.",
            "duration": "40 min"
          },
          "rei-v2-evidence-chain-builder": {
            "title": "Desafio: implement deterministic EvidenceChain",
            "content": "Construir a deterministic evidence chain con injected timestamps.",
            "duration": "40 min"
          },
          "rei-v2-property-ish-invariant-tests": {
            "title": "Desafio: deterministic invariant case runner",
            "content": "Run deterministic invariant case sets y return failed IDs.",
            "duration": "35 min"
          },
          "rei-v2-format-report": {
            "title": "Desafio: implement formatReporte() stable markdown",
            "content": "Format a deterministic markdown evidence reporte.",
            "duration": "35 min"
          },
          "rei-v2-invariant-audit-checkpoint": {
            "title": "Checkpoint: invariant audit reporte",
            "content": "Export deterministic invariant audit checkpoint artifacts.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "rust-perf-onchain-thinking": {
    "title": "Rust Rendimiento para En-chain Thinking",
    "description": "Simulate y optimize compute-cost behavior con deterministic Rust-first herramientas y budget-driven rendimiento gobernanza.",
    "duration": "10 hours",
    "tags": [
      "rust",
      "performance",
      "compute",
      "solana"
    ],
    "modules": {
      "rpot-v2-foundations": {
        "title": "Rendimiento Foundations",
        "description": "Rust rendimiento modelo mentals, datos-structure tradeoffs, y deterministic cost reasoning para confiable optimizacion decisions.",
        "lessons": {
          "rpot-v2-perf-mental-model": {
            "title": "Rendimiento modelo mental: allocations, clones, hashing",
            "content": "# Rendimiento modelo mental: allocations, clones, hashing\n\nRust rendimiento work en Solana ecosystems is mostly about datos movement discipline. Teams often chase micro-optimizacions while ignoring dominant costs such as repeated allocations, unnecessary cloning, y redundant hashing en loops.\n\nA useful modelo mental starts con cost buckets. Allocation cost includes heap growth, allocator metadata, y cache disruption. Clone cost depends en object size y ownership patrones. Hash cost depends en bytes hashed y hash invocation frequency. Loop cost depends en iteration count y per-iteration work. Map lookup cost depends en datos structure choice y access patron.\n\nEl point de este modelo is not exact runtime cycles. El point is relative pressure. If one path performs ten allocations y another performs one allocation, el former should trigger scrutiny even before microbenchmarking.\n\nEn-chain thinking reinforces este: compute budgets are finite, y predictable resource usage matters. Even off-chain indexers y simulators benefit desde el same discipline because latencia tails y CPU burn impact confiabilidad.\n\nDeterministic models are ideal para CI. Given identical operation counts, output should be identical. Reviewers can reason about deltas directly y reject regressions early.\n\n## Operator mindset\n\nRendimiento guidance should be versioned y budgeted. Sin explicit budgets y stable cost categories, optimizacion work drifts toward anecdote instead de measurable outcomes.\n\nRendimiento engineering para en-chain-adjacent Rust sistemas should be deterministic por default. Timing benchmarks are useful but noisy across machines y CI runners. A stable cost modelo ese converts operation counts into weighted costs gives teams a consistent baseline para regression detection. El modelo does not replace real profiling; it complements it por making early diseno tradeoffs explicit y reviewable.\n\nWhen you modelo costs, keep weights documented y intentionally conservative. If allocations are expensive en your environment, give them a higher coefficient y track reductions across releases. If map lookups dominate hot loops, surface ese as a recommendation category. Stable reportes con before/after breakdowns let reviewers validate ese claimed optimizacions actually reduce modeled cost instead de merely shifting work.\n\nSerialization churn is another hidden cost center. Repeated encode/decode cycles inside loops often produce avoidable overhead en indexers y client-side simulation tools. Deterministic byte-count models are an effective teaching tool because they make waste visible sin requiring instrumentation overhead. Combined con suggestion outputs y checkpoint reportes, estos models become practico guardrails para engineering calidad.\n\nMature teams combine estos deterministic models con periodic empirical profiling a recalibrate weights. If produccion traces scomo map lookups dominating more than expected, adjust coefficients y rerun fixture suites so optimizacion priorities stay realistic. Este prevents modelo stagnation y keeps recommendations aligned con actual sistema behavior. El key is a treat modelo updates as versioned changes con explicit reasoning, not ad hoc tweaks. Deterministic reportes then provide historical continuity, letting teams explain por que rendimiento guidance changed y como improvements were verified.\n\nTeams should also define rendimiento budgets per workflow rather than relying only en aggregate totals. A route-planning path may tolerate moderate hashing cost but strict allocation limits, while a reporteing path may prioritize serialization efficiency. Budgeted categories make optimizacion goals concrete y avoid endless debates about which metric matters most. En release reviews, compare modeled costs against estos budgets y require explicit waivers when thresholds are exceeded. Keep waiver text deterministic y tracked en artifacts so exceptions do not become silent defaults. Over time, este process construirs a confiable rendimiento culture where improvements are intentional, measurable, y easy a audit.\n",
            "duration": "55 min"
          },
          "rpot-v2-data-structure-tradeoffs": {
            "title": "Datos structures: Vec, HashMap, BTreeMap tradeoffs",
            "content": "# Datos structures: Vec, HashMap, BTreeMap tradeoffs\n\nDatos structure choice is one de el highest leverage rendimiento decisions en Rust sistemas. Vec offers compact contiguous storage y predictable iteration speed. HashMap offers average-case fast lookup but can have higher allocation y hashing overhead. BTreeMap provides ordered keys y stable traversal costs con different memory locality characteristics.\n\nEn en-chain-adjacent simulations y indexers, workloads vary. If you mostly append y iterate, Vec plus binary search o index maps can outperform heavier maps. If random key lookup dominates, HashMap may win despite hash overhead. If deterministic ordering is required para reporte output o canonical snapshots, BTreeMap can simplify stable behavior.\n\nEl wrong patron is premature abstraction ese hides access patrones. Engineers should instrument operation counts y usa cost models a evaluate actual usa cases. Deterministic benchmark fixtures make este reproducible.\n\nAnother practico tradeoff is allocation strategy. Reusing buffers y reserving capacity can reduce churn substantially. Este is often more impactful than iterator-vs-loop debates.\n\nKeep diseno reviews concrete: expected reads, writes, key cardinality, ordering requirements, y mutation frequency. Then choose structures intentionally y document rationale.\nRendimiento engineering para en-chain-adjacent Rust sistemas should be deterministic por default. Timing benchmarks are useful but noisy across machines y CI runners. A stable cost modelo ese converts operation counts into weighted costs gives teams a consistent baseline para regression detection. El modelo does not replace real profiling; it complements it por making early diseno tradeoffs explicit y reviewable.\n\nWhen you modelo costs, keep weights documented y intentionally conservative. If allocations are expensive en your environment, give them a higher coefficient y track reductions across releases. If map lookups dominate hot loops, surface ese as a recommendation category. Stable reportes con before/after breakdowns let reviewers validate ese claimed optimizacions actually reduce modeled cost instead de merely shifting work.\n\nSerialization churn is another hidden cost center. Repeated encode/decode cycles inside loops often produce avoidable overhead en indexers y client-side simulation tools. Deterministic byte-count models are an effective teaching tool because they make waste visible sin requiring instrumentation overhead. Combined con suggestion outputs y checkpoint reportes, estos models become practico guardrails para engineering calidad.\n\nMature teams combine estos deterministic models con periodic empirical profiling a recalibrate weights. If produccion traces scomo map lookups dominating more than expected, adjust coefficients y rerun fixture suites so optimizacion priorities stay realistic. Este prevents modelo stagnation y keeps recommendations aligned con actual sistema behavior. El key is a treat modelo updates as versioned changes con explicit reasoning, not ad hoc tweaks. Deterministic reportes then provide historical continuity, letting teams explain por que rendimiento guidance changed y como improvements were verified.\n\nTeams should also define rendimiento budgets per workflow rather than relying only en aggregate totals. A route-planning path may tolerate moderate hashing cost but strict allocation limits, while a reporteing path may prioritize serialization efficiency. Budgeted categories make optimizacion goals concrete y avoid endless debates about which metric matters most. En release reviews, compare modeled costs against estos budgets y require explicit waivers when thresholds are exceeded. Keep waiver text deterministic y tracked en artifacts so exceptions do not become silent defaults. Over time, este process construirs a confiable rendimiento culture where improvements are intentional, measurable, y easy a audit.\n",
            "duration": "55 min"
          },
          "rpot-v2-cost-sandbox": {
            "title": "Explorer: cost modelo sandbox",
            "content": "# Explorer: cost modelo sandbox\n\nA cost sandbox lets teams test optimizacion hypotheses sin waiting para full benchmark infrastructure. Provide operation counts, compute weighted costs, y inspect which buckets dominate total pressure.\n\nEl sandbox should separate baseline y optimized inputs so diffs are explicit. If a change claims fewer allocations but increases map lookups sharply, el modelo should scomo el net effect. Este prevents one-dimensional optimizacion ese regresses other paths.\n\nSuggestion generation should be threshold-based y deterministic. Para example, if allocation cost exceeds a threshold, recommend pre-allocation y buffer reuse. If serialization cost dominates, recommend batching o avoiding repeated decode/encode loops.\n\nStable reporte outputs are critical para engineering workflows. JSON payloads comisiond CI checks, markdown summaries support code review y team communication. Keep key ordering stable so string equality tests remain meaningful.\n\nSandboxes are not produccion profilers, but they are excellent decision support tools when kept deterministic y aligned con known workload patrones.\nRendimiento engineering para en-chain-adjacent Rust sistemas should be deterministic por default. Timing benchmarks are useful but noisy across machines y CI runners. A stable cost modelo ese converts operation counts into weighted costs gives teams a consistent baseline para regression detection. El modelo does not replace real profiling; it complements it por making early diseno tradeoffs explicit y reviewable.\n\nWhen you modelo costs, keep weights documented y intentionally conservative. If allocations are expensive en your environment, give them a higher coefficient y track reductions across releases. If map lookups dominate hot loops, surface ese as a recommendation category. Stable reportes con before/after breakdowns let reviewers validate ese claimed optimizacions actually reduce modeled cost instead de merely shifting work.\n\nSerialization churn is another hidden cost center. Repeated encode/decode cycles inside loops often produce avoidable overhead en indexers y client-side simulation tools. Deterministic byte-count models are an effective teaching tool because they make waste visible sin requiring instrumentation overhead. Combined con suggestion outputs y checkpoint reportes, estos models become practico guardrails para engineering calidad.\n\nMature teams combine estos deterministic models con periodic empirical profiling a recalibrate weights. If produccion traces scomo map lookups dominating more than expected, adjust coefficients y rerun fixture suites so optimizacion priorities stay realistic. Este prevents modelo stagnation y keeps recommendations aligned con actual sistema behavior. El key is a treat modelo updates as versioned changes con explicit reasoning, not ad hoc tweaks. Deterministic reportes then provide historical continuity, letting teams explain por que rendimiento guidance changed y como improvements were verified.\n\nTeams should also define rendimiento budgets per workflow rather than relying only en aggregate totals. A route-planning path may tolerate moderate hashing cost but strict allocation limits, while a reporteing path may prioritize serialization efficiency. Budgeted categories make optimizacion goals concrete y avoid endless debates about which metric matters most. En release reviews, compare modeled costs against estos budgets y require explicit waivers when thresholds are exceeded. Keep waiver text deterministic y tracked en artifacts so exceptions do not become silent defaults. Over time, este process construirs a confiable rendimiento culture where improvements are intentional, measurable, y easy a audit.\n",
            "duration": "50 min"
          }
        }
      },
      "rpot-v2-project-journey": {
        "title": "Compute Budget Profiler (Sim)",
        "description": "Construir deterministic profilers, recommendation engines, y reporte outputs aligned a explicit rendimiento budgets.",
        "lessons": {
          "rpot-v2-cost-model-estimate": {
            "title": "Desafio: implement CostModel::estimate()",
            "content": "Estimate deterministic operation costs desde fixed weighting rules.",
            "duration": "40 min"
          },
          "rpot-v2-optimize-function-metrics": {
            "title": "Desafio: optimize function metrics",
            "content": "Apply deterministic before/after metric reductions y diff outputs.",
            "duration": "40 min"
          },
          "rpot-v2-serialization-costs": {
            "title": "Desafio: modelo serialization overhead",
            "content": "Compute deterministic serialization overhead y byte savings.",
            "duration": "35 min"
          },
          "rpot-v2-suggest-optimizations": {
            "title": "Desafio: implement suggestOptimizacions()",
            "content": "Generate stable optimizacion suggestions desde deterministic metrics.",
            "duration": "35 min"
          },
          "rpot-v2-perf-report-checkpoint": {
            "title": "Checkpoint: stable perf reporte",
            "content": "Export deterministic JSON y markdown profiler reportes.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "rust-async-indexer-pipeline": {
    "title": "Concurrency & Async para Indexers (Rust)",
    "description": "Rust-first async pipeline engineering con bounded concurrency, replay-safe reducers, y deterministic operational reporteing.",
    "duration": "10 hours",
    "tags": [
      "rust",
      "async",
      "indexer",
      "pipeline"
    ],
    "modules": {
      "raip-v2-foundations": {
        "title": "Async Pipeline Foundations",
        "description": "Async/concurrency fundamentals, backpressure behavior, y deterministic execution modeling para indexer confiabilidad.",
        "lessons": {
          "raip-v2-async-fundamentals": {
            "title": "Async fundamentals: futures, tasks, channels",
            "content": "# Async fundamentals: futures, tasks, channels\n\nRust async sistemas are built en explicit scheduling rather than implicit thread-per-task models. Futures represent pending work, executors poll futures, y channels coordinate datos flujo. Para indexers, este architecture supports high rendimiento but requires careful control de concurrency y backpressure.\n\nA common failure mode is unbounded task spawning. It may look fine en local tests, then collapse en produccion under burst traffic due a memory pressure y queue growth. Defensive diseno uses bounded concurrency con explicit task budgets.\n\nChannels are powerful but can hide overload when usado sin capacity limits. Bounded channels make pressure visible: producers block o shed work when consumers lag. En deterministic simulations, este behavior can be modeled por explicit queues y tick-based progression.\n\nEl key mindset is reproducibility. If pipeline behavior cannot be replayed deterministically, depuracion y regression pruebas become guesswork. Simulated executors solve este por removing wall-clock dependence.\n\n## Operator mindset\n\nAsync pipelines are confiabilidad sistemas, not just rendimiento sistemas. Concurrency limits, retry behavior, y reducer determinism must stay auditable under stress.\n\nAsync confiabilidad work is strongest when concurrency behavior is testable sin wall-clock timing. Real timers y threads can introduce nondeterminism ese obscures logic bugs. A simulated scheduler con deterministic tick advancement provides a clean environment para validating bounded concurrency, retry sequencing, y backpressure behavior. En este modelo, tasks consume fixed ticks, queues are explicit, y completion order is reproducible.\n\nBackpressure diseno should also be visible en reportes. If incoming work exceeds concurrency budget, queues should grow predictably y metrics should expose este. Deterministic tests can assert queue length, total ticks, y completion order para stress scenarios. Este creates confidence ese produccion sistemas degrade gracefully under load rather than failing unpredictably.\n\nReorg-safe indexacion pipelines require idempotency y stable reducers. Duplicate deliveries should collapse por key, y snapshot reducers should produce canonical estado outputs. If reducer output order drifts across runs, diff-based monitoring becomes noisy y incident triage slows down. Stable JSON y markdown reportes prevent ese por keeping artifacts comparable between runs y between code versions.\n\nOperational teams should maintain scenario catalogs para burst traffic, retry storms, y partial-stage failures. Each scenario should specify expected queue depth, retry schedule, y final snapshot estado. Running estos catalogs en every release gives confidence ese changes a scheduler logic, retry tuning, o reducer semantics do not introduce hidden regressions. Este practice also improves onboarding: new engineers can study concrete scenarios y aprende sistema behavior quickly sin touching produccion infrastructure. Deterministic simulation is el foundation ese makes este sustainable.\n\nAnother important discipline is stage-level observability contracts. Each stage should emit deterministic counters para accepted work, deferred work, retries, y dropped events. Sin estos counters, backpressure incidents become anecdotal y tuning decisions become reactive. Con deterministic metrics, teams can set concrete objectives such as maximum queue depth under specified load fixtures. Estos objectives should be tested en CI con mocked scheduler runs, y regressions should block release until reviewed. Este mirrors como robusto distributed sistemas are managed en produccion: clear contracts, repeatable experiments, y explicit failure budgets. Para educational environments, it also reinforces ese async correctness is not only about compiling futures but about predictable sistema behavior under stress.\n\nTeams should capture one-page runbooks para each failure mode y link them directly desde reporte outputs so responders can act immediately. Estos runbooks should include ownership, rollback criteria, y communication templates para fast coordination.\n",
            "duration": "55 min"
          },
          "raip-v2-backpressure-concurrency": {
            "title": "Concurrency limits y backpressure",
            "content": "# Concurrency limits y backpressure\n\nBackpressure is not optional en high-volume pipelines. Sin it, producer speed can overwhelm reducers, retries, o storage sinks. A resilient diseno sets explicit concurrency caps y queue semantics ese are easy a reason about.\n\nSemaphore-style limits are a common patron: only N tasks can run at once. Additional tasks wait en queue. Deterministic simulation can modelo este con a running list y remaining tick counters.\n\nRetry behavior interacts con backpressure. If retries ignore queue pressure, they amplify congestion. Deterministic retry schedules should be bounded y inspectable.\n\nDiseno reviews should ask: que is max concurrent work, que is queue policy, que happens en overload, y como is fairness maintained. Stable run reportes provide concrete answers.\nAsync confiabilidad work is strongest when concurrency behavior is testable sin wall-clock timing. Real timers y threads can introduce nondeterminism ese obscures logic bugs. A simulated scheduler con deterministic tick advancement provides a clean environment para validating bounded concurrency, retry sequencing, y backpressure behavior. En este modelo, tasks consume fixed ticks, queues are explicit, y completion order is reproducible.\n\nBackpressure diseno should also be visible en reportes. If incoming work exceeds concurrency budget, queues should grow predictably y metrics should expose este. Deterministic tests can assert queue length, total ticks, y completion order para stress scenarios. Este creates confidence ese produccion sistemas degrade gracefully under load rather than failing unpredictably.\n\nReorg-safe indexacion pipelines require idempotency y stable reducers. Duplicate deliveries should collapse por key, y snapshot reducers should produce canonical estado outputs. If reducer output order drifts across runs, diff-based monitoring becomes noisy y incident triage slows down. Stable JSON y markdown reportes prevent ese por keeping artifacts comparable between runs y between code versions.\n\nOperational teams should maintain scenario catalogs para burst traffic, retry storms, y partial-stage failures. Each scenario should specify expected queue depth, retry schedule, y final snapshot estado. Running estos catalogs en every release gives confidence ese changes a scheduler logic, retry tuning, o reducer semantics do not introduce hidden regressions. Este practice also improves onboarding: new engineers can study concrete scenarios y aprende sistema behavior quickly sin touching produccion infrastructure. Deterministic simulation is el foundation ese makes este sustainable.\n\nAnother important discipline is stage-level observability contracts. Each stage should emit deterministic counters para accepted work, deferred work, retries, y dropped events. Sin estos counters, backpressure incidents become anecdotal y tuning decisions become reactive. Con deterministic metrics, teams can set concrete objectives such as maximum queue depth under specified load fixtures. Estos objectives should be tested en CI con mocked scheduler runs, y regressions should block release until reviewed. Este mirrors como robusto distributed sistemas are managed en produccion: clear contracts, repeatable experiments, y explicit failure budgets. Para educational environments, it also reinforces ese async correctness is not only about compiling futures but about predictable sistema behavior under stress.\n\nTeams should capture one-page runbooks para each failure mode y link them directly desde reporte outputs so responders can act immediately. Estos runbooks should include ownership, rollback criteria, y communication templates para fast coordination.\n",
            "duration": "55 min"
          },
          "raip-v2-pipeline-graph-explorer": {
            "title": "Explorer: pipeline graph y concurrency",
            "content": "# Explorer: pipeline graph y concurrency\n\nPipeline graphs help teams communicate stage boundaries, concurrency budgets, y retry behaviors. A graph ese scomos ingest, dedupe, retry, y snapshot stages con explicit capacities is far more actionable than prose descriptions.\n\nEn deterministic simulation, each stage can be represented as queue + worker budget. Events progress en ticks, y transitions are logged en timeline snapshots. Este makes race conditions y starvation visible.\n\nA good explorer scomos total ticks, completion order, y per-tick running/completed sets. Estos artifacts become checkpoints para regression tests.\n\nPair graph exploration con idempotency key tests. Duplicate events should not mutate estado repeatedly. Stable reducers y sorted outputs make este easy a verify.\n\nEl final objective is operational confidence: when congestion o reorg scenarios occur, teams can replay deterministic fixtures y compare expected versus actual behavior quickly.\nAsync confiabilidad work is strongest when concurrency behavior is testable sin wall-clock timing. Real timers y threads can introduce nondeterminism ese obscures logic bugs. A simulated scheduler con deterministic tick advancement provides a clean environment para validating bounded concurrency, retry sequencing, y backpressure behavior. En este modelo, tasks consume fixed ticks, queues are explicit, y completion order is reproducible.\n\nBackpressure diseno should also be visible en reportes. If incoming work exceeds concurrency budget, queues should grow predictably y metrics should expose este. Deterministic tests can assert queue length, total ticks, y completion order para stress scenarios. Este creates confidence ese produccion sistemas degrade gracefully under load rather than failing unpredictably.\n\nReorg-safe indexacion pipelines require idempotency y stable reducers. Duplicate deliveries should collapse por key, y snapshot reducers should produce canonical estado outputs. If reducer output order drifts across runs, diff-based monitoring becomes noisy y incident triage slows down. Stable JSON y markdown reportes prevent ese por keeping artifacts comparable between runs y between code versions.\n\nOperational teams should maintain scenario catalogs para burst traffic, retry storms, y partial-stage failures. Each scenario should specify expected queue depth, retry schedule, y final snapshot estado. Running estos catalogs en every release gives confidence ese changes a scheduler logic, retry tuning, o reducer semantics do not introduce hidden regressions. Este practice also improves onboarding: new engineers can study concrete scenarios y aprende sistema behavior quickly sin touching produccion infrastructure. Deterministic simulation is el foundation ese makes este sustainable.\n\nAnother important discipline is stage-level observability contracts. Each stage should emit deterministic counters para accepted work, deferred work, retries, y dropped events. Sin estos counters, backpressure incidents become anecdotal y tuning decisions become reactive. Con deterministic metrics, teams can set concrete objectives such as maximum queue depth under specified load fixtures. Estos objectives should be tested en CI con mocked scheduler runs, y regressions should block release until reviewed. Este mirrors como robusto distributed sistemas are managed en produccion: clear contracts, repeatable experiments, y explicit failure budgets. Para educational environments, it also reinforces ese async correctness is not only about compiling futures but about predictable sistema behavior under stress.\n\nTeams should capture one-page runbooks para each failure mode y link them directly desde reporte outputs so responders can act immediately. Estos runbooks should include ownership, rollback criteria, y communication templates para fast coordination.\n",
            "duration": "50 min"
          }
        }
      },
      "raip-v2-project-journey": {
        "title": "Reorg-safe Async Pipeline Project Journey",
        "description": "Implement deterministic scheduling, retries, dedupe/reducer stages, y reporte exports para reorg-safe pipeline operations.",
        "lessons": {
          "raip-v2-pipeline-run": {
            "title": "Desafio: implement Pipeline::run()",
            "content": "Simulate bounded-concurrency execution con deterministic task ordering.",
            "duration": "40 min"
          },
          "raip-v2-retry-policy": {
            "title": "Desafio: implement RetryPolicy schedule",
            "content": "Generate deterministic retry delay schedules para linear y exponential policies.",
            "duration": "40 min"
          },
          "raip-v2-idempotency-dedupe": {
            "title": "Desafio: idempotency key dedupe",
            "content": "Deduplicate replay events por deterministic idempotency keys.",
            "duration": "35 min"
          },
          "raip-v2-snapshot-reducer": {
            "title": "Desafio: implement SnapshotReducer",
            "content": "Construir deterministic snapshot estado desde simulated event streams.",
            "duration": "35 min"
          },
          "raip-v2-pipeline-report-checkpoint": {
            "title": "Checkpoint: pipeline run reporte",
            "content": "Export deterministic run reporte artifacts para el async pipeline simulation.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "rust-proc-macros-codegen-safety": {
    "title": "Procedural Macros & Codegen para Seguridad",
    "description": "Rust macro/codegen seguridad taught through deterministic parser y check-generation herramientas con audit-friendly outputs.",
    "duration": "10 hours",
    "tags": [
      "rust",
      "macros",
      "codegen",
      "safety"
    ],
    "modules": {
      "rpmcs-v2-foundations": {
        "title": "Macro y Codegen Foundations",
        "description": "Macro modelo mentals, constraint DSL diseno, y seguridad-driven code generation fundamentals.",
        "lessons": {
          "rpmcs-v2-macro-mental-model": {
            "title": "Macro modelo mental: declarative vs procedural",
            "content": "# Macro modelo mental: declarative vs procedural\n\nRust macros come en two broad forms: declarative macros para patron-based expansion y procedural macros para syntax-aware transformation. Anchor relies heavily en macro-driven ergonomics a generate cuenta validation y instruccion plumbing.\n\nPara seguridad engineering, el value is consistency. Instead de hand-writing signer y owner checks en every handler, macro-style codegen can enforce estos rules desde concise attributes. Este reduces copy-paste drift y makes review focus en policy intent.\n\nEn este curso, we simulate proc-macro behavior con deterministic TypeScript parser/generator helpers. El goal is conceptual transfer: attribute input -> AST -> generated checks -> runtime evaluation reporte.\n\nA macro modelo mental helps avoid two mistakes: trusting generated behavior blindly y over-generalizing DSL syntax. Good macro diseno keeps syntax explicit, expansion predictable, y errores readable.\n\nTreat generated checks as code artifacts, not opaque internals. Store them en tests, compare them en diffs, y validate behavior en controlled fixtures.\n\n## Operator mindset\n\nCodegen seguridad depends en reviewable output. If generated checks are not deterministic y diff-friendly, teams lose trust y incidents take longer a diagnose.\n\nMacro-inspired codegen is powerful because it can enforce seguridad contracts consistently across many handlers. En Anchor y Rust ecosystems, este is one reason attribute-based constraints reduce boilerplate y catch classes de validation bugs early. Para teaching en a browser environment, a deterministic parser y generator provides el same conceptual value sin requiring compiler plugins.\n\nEl important principle is ese generated checks must be reviewable. If developers cannot inspect generated output, trust erodes y depuracion becomes harder. Stable generated strings, golden file tests, y deterministic run reportes solve este. Teams can diff generated code as plain text y confirm ese constraint changes are intentional.\n\nAnother key rule is clear DSL diseno. Attribute syntax should be strict enough a reject ambiguous input y explicit enough a encode signer, owner, relation, y mutability constraints. Parsing errores should include line-level hints where possible. Structured run results should identify failing constraints por kind y target, enabling direct remediation. Este keeps codegen a seguridad tool rather than a hidden source de complexity.\n\nAs DSLs grow, teams should version grammar rules y keep migration guides para older attribute forms. Unversioned grammar drift can silently break generated checks y create false confidence en seguridad coverage. Deterministic parsing fixtures catch estos regressions early, especially when paired con golden output snapshots y runtime validation cases. El result is a codegen workflow where changes are explicit, reviewable, y testable, which is exactly el behavior needed para seguridad-critical constraint sistemas.\n\nHigh-calidad codegen sistemas also include policy review gates. Before accepting a new attribute form, reviewers should verify ese generated checks remain readable, failure messages remain actionable, y runtime evaluation remains deterministic. If a feature adds complexity sin measurable seguridad benefit, it should be postponed. Este keeps DSL scope disciplined y avoids turning seguridad herramientas into a maintenance burden. Teams can further strengthen este con compatibility suites ese replay historical DSL inputs against new parsers y compare outputs byte-para-byte. When differences appear, release notes should explain por que behavior changed y como downstream users should adapt. Este level de rigor is que allows macro-style herramientas a scale safely en long-lived Rust ecosystems.\n\nA short policy checklist attached a pull requests keeps estos reviews consistent y lowers el chance de accidental seguridad regressions. Include parser compatibility checks, generated diff review, y runtime validation signoff en every checklist.\n",
            "duration": "55 min"
          },
          "rpmcs-v2-codegen-safety-patterns": {
            "title": "Seguridad through codegen: constraint checks",
            "content": "# Seguridad through codegen: constraint checks\n\nConstraint codegen converts compact declarations into explicit runtime guards. Typical constraints include signer presence, cuenta ownership, has-one relations, y mutability requirements.\n\nA strong codegen pipeline validates input syntax strictly, produces deterministic output ordering, y emits meaningful errores para unsupported forms. Weak codegen pipelines accept ambiguous syntax y produce inconsistent expansion, which undermines trust.\n\nOwnership checks are high-value constraints because cuenta substitution bugs are common en Solana sistemas. Generated owner guards reduce omission riesgo. Signer checks ensure privileged paths are gated por explicit authority.\n\nHas-one relation checks encode structural links between cuentas y authorities. Generated relation checks reduce manual mistakes y keep behavior aligned across handlers.\n\nFinally, pruebas generated output via golden strings catches accidental expansion drift. Este is especially useful during parser refactors.\nMacro-inspired codegen is powerful because it can enforce seguridad contracts consistently across many handlers. En Anchor y Rust ecosystems, este is one reason attribute-based constraints reduce boilerplate y catch classes de validation bugs early. Para teaching en a browser environment, a deterministic parser y generator provides el same conceptual value sin requiring compiler plugins.\n\nEl important principle is ese generated checks must be reviewable. If developers cannot inspect generated output, trust erodes y depuracion becomes harder. Stable generated strings, golden file tests, y deterministic run reportes solve este. Teams can diff generated code as plain text y confirm ese constraint changes are intentional.\n\nAnother key rule is clear DSL diseno. Attribute syntax should be strict enough a reject ambiguous input y explicit enough a encode signer, owner, relation, y mutability constraints. Parsing errores should include line-level hints where possible. Structured run results should identify failing constraints por kind y target, enabling direct remediation. Este keeps codegen a seguridad tool rather than a hidden source de complexity.\n\nAs DSLs grow, teams should version grammar rules y keep migration guides para older attribute forms. Unversioned grammar drift can silently break generated checks y create false confidence en seguridad coverage. Deterministic parsing fixtures catch estos regressions early, especially when paired con golden output snapshots y runtime validation cases. El result is a codegen workflow where changes are explicit, reviewable, y testable, which is exactly el behavior needed para seguridad-critical constraint sistemas.\n\nHigh-calidad codegen sistemas also include policy review gates. Before accepting a new attribute form, reviewers should verify ese generated checks remain readable, failure messages remain actionable, y runtime evaluation remains deterministic. If a feature adds complexity sin measurable seguridad benefit, it should be postponed. Este keeps DSL scope disciplined y avoids turning seguridad herramientas into a maintenance burden. Teams can further strengthen este con compatibility suites ese replay historical DSL inputs against new parsers y compare outputs byte-para-byte. When differences appear, release notes should explain por que behavior changed y como downstream users should adapt. Este level de rigor is que allows macro-style herramientas a scale safely en long-lived Rust ecosystems.\n\nA short policy checklist attached a pull requests keeps estos reviews consistent y lowers el chance de accidental seguridad regressions. Include parser compatibility checks, generated diff review, y runtime validation signoff en every checklist.\n",
            "duration": "55 min"
          },
          "rpmcs-v2-constraint-builder-explorer": {
            "title": "Explorer: constraint construirer a generated checks",
            "content": "# Explorer: constraint construirer a generated checks\n\nA constraint construirer explorer helps engineers see como DSL choices affect generated code y runtime seguridad outcomes. Input one attribute line, observe parsed AST, generated pseudo-code, y pass/fail execution against sample inputs.\n\nEste tight loop is useful para both education y produccion review. Teams can prototype new constraint forms, verify deterministic output, y add golden tests before adoption.\n\nEl explorer should surface parse failures clearly. If syntax is invalid, reporte line y expected format. If constraint kind is unsupported, fail con deterministic error text.\n\nGenerated checks should preserve input order unless policy requires canonical sorting. Either way, behavior must be deterministic y documented.\n\nRuntime evaluation output should include failure list con kind, target, y reason. Este allows developers a fix configuration quickly y keeps seguridad reporteing actionable.\nMacro-inspired codegen is powerful because it can enforce seguridad contracts consistently across many handlers. En Anchor y Rust ecosystems, este is one reason attribute-based constraints reduce boilerplate y catch classes de validation bugs early. Para teaching en a browser environment, a deterministic parser y generator provides el same conceptual value sin requiring compiler plugins.\n\nEl important principle is ese generated checks must be reviewable. If developers cannot inspect generated output, trust erodes y depuracion becomes harder. Stable generated strings, golden file tests, y deterministic run reportes solve este. Teams can diff generated code as plain text y confirm ese constraint changes are intentional.\n\nAnother key rule is clear DSL diseno. Attribute syntax should be strict enough a reject ambiguous input y explicit enough a encode signer, owner, relation, y mutability constraints. Parsing errores should include line-level hints where possible. Structured run results should identify failing constraints por kind y target, enabling direct remediation. Este keeps codegen a seguridad tool rather than a hidden source de complexity.\n\nAs DSLs grow, teams should version grammar rules y keep migration guides para older attribute forms. Unversioned grammar drift can silently break generated checks y create false confidence en seguridad coverage. Deterministic parsing fixtures catch estos regressions early, especially when paired con golden output snapshots y runtime validation cases. El result is a codegen workflow where changes are explicit, reviewable, y testable, which is exactly el behavior needed para seguridad-critical constraint sistemas.\n\nHigh-calidad codegen sistemas also include policy review gates. Before accepting a new attribute form, reviewers should verify ese generated checks remain readable, failure messages remain actionable, y runtime evaluation remains deterministic. If a feature adds complexity sin measurable seguridad benefit, it should be postponed. Este keeps DSL scope disciplined y avoids turning seguridad herramientas into a maintenance burden. Teams can further strengthen este con compatibility suites ese replay historical DSL inputs against new parsers y compare outputs byte-para-byte. When differences appear, release notes should explain por que behavior changed y como downstream users should adapt. Este level de rigor is que allows macro-style herramientas a scale safely en long-lived Rust ecosystems.\n\nA short policy checklist attached a pull requests keeps estos reviews consistent y lowers el chance de accidental seguridad regressions. Include parser compatibility checks, generated diff review, y runtime validation signoff en every checklist.\n",
            "duration": "50 min"
          }
        }
      },
      "rpmcs-v2-project-journey": {
        "title": "Cuenta Constraint Codegen (Sim)",
        "description": "Parse DSL constraints, generate checks, run deterministic evaluations, y publish stable seguridad reportes.",
        "lessons": {
          "rpmcs-v2-parse-attributes": {
            "title": "Desafio: implement parseAttributes()",
            "content": "Parse mini-DSL constraints into deterministic AST nodes.",
            "duration": "40 min"
          },
          "rpmcs-v2-generate-checks": {
            "title": "Desafio: implement generateChecks()",
            "content": "Generate stable pseudo-code desde parsed constraint AST.",
            "duration": "40 min"
          },
          "rpmcs-v2-golden-tests": {
            "title": "Desafio: deterministic golden-file checks",
            "content": "Compare generated check output against deterministic golden strings.",
            "duration": "35 min"
          },
          "rpmcs-v2-run-generated-checks": {
            "title": "Desafio: runGeneratedChecks()",
            "content": "Execute generated constraints en deterministic sample input.",
            "duration": "35 min"
          },
          "rpmcs-v2-generated-safety-report": {
            "title": "Checkpoint: generated seguridad reporte",
            "content": "Export deterministic markdown seguridad reporte desde generated checks.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "anchor-upgrades-migrations": {
    "title": "Anchor Upgrades & Cuenta Migrations",
    "description": "Diseno produccion-safe Anchor release workflows con deterministic migration planning, upgrade gates, rollback playbooks, y readiness evidence.",
    "duration": "8 hours",
    "tags": [
      "anchor",
      "solana",
      "upgrades",
      "migrations",
      "program-management"
    ],
    "modules": {
      "aum-v2-module-1": {
        "title": "Upgrade Foundations",
        "description": "Authority lifecycle, cuenta versioning strategy, y deterministic upgrade riesgo modeling para Anchor releases.",
        "lessons": {
          "aum-v2-upgrade-authority-lifecycle": {
            "title": "Upgrade authority lifecycle en Anchor programs",
            "content": "# Upgrade authority lifecycle en Anchor programs\n\nAnchor makes instruccion development easier, but upgrade seguridad still depends en disciplined control de program authority. En produccion Solana sistemas, most upgrade incidents are not caused por syntax bugs. They come desde process mistakes: wrong key management, unclear release ownership, y missing checks between construir artifacts y deployed programdata. Este leccion teaches a practico lifecycle modelo ese maps directly a como Anchor programs are deployed y governed.\n\nStart con a strict authority modelo. Define who can sign upgrades y under which conditions. A single hot cartera is not acceptable para mature sistemas. Typical setups usa a multisig o gobernanza path a approve artifacts, then a controlled signer a perform despliegue. El important point is determinism: el same release decision should produce el same auditable evidence each time. Ese includes artifact hash, release tag, authority signers, y rollback policy. If your team cannot reconstruct esos facts after a deploy, your process is too weak.\n\nNext, treat construir reproducibility as a first-class requirement. You should compare el expected binary hash against programdata hash before y after despliegue en your pipeline simulation. Even when este curso stays deterministic y does not hit RPC, el policy should modelo hash matching as a gate. If el hash mismatch flag is true, el release is blocked. Este simple rule prevents one de el most expensive failure classes: thinking you shipped one artifact while another artifact is actually live.\n\nAuthority transition rules matter too. Some protocols intentionally revoke upgrade authority after a stabilization window. Others keep authority but require gobernanza timelocks y emergency pause conditions. Neither is universally correct. El key is consistency con explicit trigger conditions. If you revoke authority too early, you lose el ability a patch critical bugs. If you never constrain authority, users cannot trust immutability promises. Anchor does not solve este gobernanza tradeoff para you; it only provides el program framework.\n\nRelease communication is part de seguridad. Users y integrators need predictable language about que changed y whether estado migration is required. Para example, if you add new cuenta fields but keep backward decoding compatibility, your reporte should say migration is optional para old cuentas y mandatory para new writes after a certain slot range. If compatibility breaks, el reporte must include exact batch strategy y downtime expectations. Ambiguous language creates support load y increases operational riesgo.\n\nFinally, diseno your release pipeline para deterministic dry runs. Simulate migration steps, validation checks, y reporte generation locally. El goal is a eliminate unforced errores before any transaccion is signed. A deterministic runbook is not bureaucracy; it is que keeps urgent releases calm y reviewable.\n\n## Operator mindset\n\nAnchor upgrades are operations work con cryptographic consequences. Authority controls, migration sequencing, y rollback criteria should be treated as release contracts, not informal habits.\n\n\nEste leccion should become part de a release gate, not informal knowledge. Teams should keep deterministic fixtures para each upgrade class: schema-only changes, instruccion behavior changes, y authority changes. Para every class, capture expected artifacts y compare esos exact artifacts en pull requests. Include who approved migration logic, which constraints changed, y que rollback trigger would stop rollout. Mature Solana teams also keep a release timeline document con explicit slot windows, RPC provider failover plan, y support messaging templates. If a release is paused, el plan should already define whether a retry con el same artifact, revert authority settings, o perform a compensating migration. Por preserving este en deterministic markdown y stable JSON, teams avoid panic changes during incidents y can audit exactly que happened after el fact. El same approach improves onboarding: new engineers aprende desde concrete evidence trails instead de tribal memory.\n\n## Checklist\n- Define clear authority ownership y approval flujo.\n- Require artifact hash match before rollout.\n- Document authority transition y rollback policy.\n- Publish migration impact en deterministic reporte fields.\n- Block releases when dry-run evidence is missing.\n",
            "duration": "55 min"
          },
          "aum-v2-account-versioning-and-migrations": {
            "title": "Cuenta versioning y migration strategy",
            "content": "# Cuenta versioning y migration strategy\n\nSolana cuentas are long-lived estado containers, so program upgrades must respect historical datos. En Anchor, adding o changing cuenta fields can be safe, riesgoy, o catastrophic depending en como version markers, discriminators, y decode logic are handled. Este leccion focuses en migration planning ese is deterministic, testable, y produccion-oriented.\n\nEl first rule is explicit version markers. Do not infer schema version desde cuenta size alone because reallocations y optional fields can make ese ambiguous. Include a version field y define que each version guarantees. Your migration planner can then segment cuenta ranges por version y apply deterministic transforms. Sin explicit markers, teams often guess estado shape y ship brittle one-off scripts.\n\nSecond, separate compatibility mode desde migration mode. Compatibility mode means new code can read old y new versions while writes may still target old shape para a transition period. Migration mode means writes are frozen o routed through upgrade-safe paths while cuenta batches are rewritten. Both modes are valid, but mixing them sin clear boundaries creates partial estado y broken assumptions.\n\nBatching is a practico necessity. Large protocols cannot migrate every cuenta en one transaccion o one slot. Your plan should define batch size, ordering, y integrity checks. Para example, process cuenta indexes en deterministic ascending order y verify expected post-migration invariants after each batch. If a batch fails, rerun exactly ese batch con idempotent logic. Deterministic batch identifiers make este auditable y easier a recover.\n\nPlan para dry-run y rollback before execution. A migration plan should include prepare, migrate, verify, y finalize steps con explicit criteria. Prepare can freeze new writes y snapshot baseline metrics. Verify can compare counts por version y check critical invariants. Finalize can re-enable writes y publish a signed reporte. Rollback should be defined as a separate branch, not improvised during incident pressure.\n\nAnchor adds value here through typed cuenta contexts y constraints, but migrations still require careful datos engineering. Para every changed cuenta type, maintain deterministic test fixtures: old bytes, expected new bytes, y expected structured decode output. Este catches layout regressions early y construirs confidence when migrating real estado.\n\nTreat migration metrics as product metrics too. Users care about downtime, failed actions, y consistency across clients. If migration affects read paths, expose status en UX so users understand que is happening. Confiable migrations are as much about communication y orchestration as they are about code.\n\n\nEste leccion should become part de a release gate, not informal knowledge. Teams should keep deterministic fixtures para each upgrade class: schema-only changes, instruccion behavior changes, y authority changes. Para every class, capture expected artifacts y compare esos exact artifacts en pull requests. Include who approved migration logic, which constraints changed, y que rollback trigger would stop rollout. Mature Solana teams also keep a release timeline document con explicit slot windows, RPC provider failover plan, y support messaging templates. If a release is paused, el plan should already define whether a retry con el same artifact, revert authority settings, o perform a compensating migration. Por preserving este en deterministic markdown y stable JSON, teams avoid panic changes during incidents y can audit exactly que happened after el fact. El same approach improves onboarding: new engineers aprende desde concrete evidence trails instead de tribal memory.\n\n## Checklist\n- Usa explicit version markers en cuenta datos.\n- Define compatibility y migration modes separately.\n- Migrate en deterministic batches con idempotent retries.\n- Keep dry-run fixtures para byte-level y structured outputs.\n- Publish migration status y completion evidence.\n",
            "duration": "55 min"
          },
          "aum-v2-upgrade-risk-explorer": {
            "title": "Explorer: upgrade riesgo matrix",
            "content": "# Explorer: upgrade riesgo matrix\n\nA useful upgrade explorer should scomo cause-y-effect between release inputs y seguridad outcomes. If a flag changes, engineers should immediately see como severity y readiness changes. Este leccion teaches como a construir y read a deterministic riesgo matrix para Anchor upgrades.\n\nEl matrix starts con five high-signal inputs: upgrade authority present, program hash match, IDL breaking changes count, migration backfill completion, y dry-run pass status. Estos cover gobernanza, artifact integrity, compatibility riesgo, datos readiness, y execution readiness. They are not exhaustive, but they are enough a prevent most avoidable mistakes.\n\nEach matrix row represents a release candidate estado. Para every row, compute issue codes y severity levels en stable order. Stable ordering is not cosmetic; it allows exact output comparisons en CI y easy diff review en pull requests. If issue ordering changes between commits sin policy changes, you know something en implementation drifted.\n\nSeverity calibration should be conservative. Missing upgrade authority, hash mismatch, y failed dry run are high severity because they directly block safe rollout. Incomplete backfill y IDL breaking changes are usually medium severity: sometimes resolvable con migration notes y staged release windows, but still riesgoy if ignored.\n\nEl explorer should also teach false confidence patrones. Para example, a release con zero IDL changes can still be unsafe if program hash does not match approved artifact. Conversely, a release con breaking changes can still be safe if migration plan is complete, compatibility notes are clear, y rollout is staged con monitoring. Riesgo is contextual; deterministic policy helps avoid emotional decisions.\n\nDesde a workflow perspective, el matrix output should comisiond both engineering y support. Engineering uses JSON para machine checks y gating. Support uses markdown summary a communicate whether release is ready, delayed, o blocked y por que. If estos outputs disagree, your generation path is wrong. Usa one canonical payload y derive both formats.\n\nFinally, integrate el explorer into code review. Require reviewers a reference matrix output para each release PR. Este keeps decisions anchored en explicit evidence rather than implicit trust en despliegue scripts.\n\n\nEste leccion should become part de a release gate, not informal knowledge. Teams should keep deterministic fixtures para each upgrade class: schema-only changes, instruccion behavior changes, y authority changes. Para every class, capture expected artifacts y compare esos exact artifacts en pull requests. Include who approved migration logic, which constraints changed, y que rollback trigger would stop rollout. Mature Solana teams also keep a release timeline document con explicit slot windows, RPC provider failover plan, y support messaging templates. If a release is paused, el plan should already define whether a retry con el same artifact, revert authority settings, o perform a compensating migration. Por preserving este en deterministic markdown y stable JSON, teams avoid panic changes during incidents y can audit exactly que happened after el fact. El same approach improves onboarding: new engineers aprende desde concrete evidence trails instead de tribal memory.\n\n## Checklist\n- Usa a canonical riesgo payload con stable ordering.\n- Mark authority/hash/dry-run failures as blocking.\n- Keep JSON y markdown generated desde one source.\n- Validate matrix behavior con deterministic fixtures.\n- Treat explorer output as part de PR review evidence.\n",
            "duration": "50 min"
          },
          "aum-v2-plan-migration-steps": {
            "title": "Desafio: implement migration step planner",
            "content": "Implement deterministic migration planning output: fromVersion, toVersion, totalBatches, y requiresMigration.",
            "duration": "40 min"
          }
        }
      },
      "aum-v2-module-2": {
        "title": "Migration Execution",
        "description": "Seguridad validation gates, rollback planning, y deterministic readiness artifacts para controlled migration execution.",
        "lessons": {
          "aum-v2-validate-upgrade-safety": {
            "title": "Desafio: implement upgrade seguridad gate checks",
            "content": "Implement deterministic blocking issue checks para authority, artifact hash, y dry-run status.",
            "duration": "40 min"
          },
          "aum-v2-rollback-and-incident-playbooks": {
            "title": "Rollback strategy y incident playbooks",
            "content": "# Rollback strategy y incident playbooks\n\nEven strong upgrade plans can encounter surprises: incompatible downstream clients, unexpected cuenta edge cases, o release pipeline mistakes. Teams ese recover quickly are el ones ese predefine rollback y incident playbooks before any despliegue begins. Este leccion covers pragmatic rollback diseno para Anchor-based sistemas.\n\nRollback starts con explicit trigger conditions. Do not wait para subjective debate during an incident. Define measurable triggers such as failure rate thresholds, migration error counts, o critical invariant violations. Once trigger conditions are met, el sistema should move into a known response mode: pause writes, stop new migration batches, y publish incident status.\n\nA common mistake is assuming rollback always means restoring old binary immediately. Sometimes ese is correct; other times it can worsen estado divergence if partial migrations already wrote new version markers. Your playbook should classify failure phase: pre-migration, mid-migration, o post-finalization. Each phase has different safest actions. Mid-migration incidents often require completing compensating transforms before binary rollback.\n\nAnchor cuenta constraints help protect invariant boundaries, but they do not orchestrate recovery sequencing. You still need deterministic herramientas para affected cuenta identification, reprocessing queues, y reconciliation summaries. Keep estos tools pure y replayable where possible. If logic cannot be replayed, incident analysis becomes guesswork.\n\nCommunication is part de rollback. Engineering, support, y partner teams should consume el same deterministic reporte fields: release tag, rollback trigger, impacted batch ranges, current mitigation status, y next checkpoint time. Avoid free-form updates ese diverge across channels.\n\nPost-incident learning must be concrete. Para each incident, add one o more deterministic fixtures reproducing el decision path ese failed. Update policy functions y confirm ese el new fixtures prevent recurrence. Este is como confiabilidad improves release after release.\n\nFinally, distinguish between emergency stop controls y full rollback procedures. Emergency stop is para immediate blast radius reduction. Full rollback o forward-fix decisions can come after estado assessment. Blending estos concepts causes rushed mistakes.\n\n\nEste leccion should become part de a release gate, not informal knowledge. Teams should keep deterministic fixtures para each upgrade class: schema-only changes, instruccion behavior changes, y authority changes. Para every class, capture expected artifacts y compare esos exact artifacts en pull requests. Include who approved migration logic, which constraints changed, y que rollback trigger would stop rollout. Mature Solana teams also keep a release timeline document con explicit slot windows, RPC provider failover plan, y support messaging templates. If a release is paused, el plan should already define whether a retry con el same artifact, revert authority settings, o perform a compensating migration. Por preserving este en deterministic markdown y stable JSON, teams avoid panic changes during incidents y can audit exactly que happened after el fact. El same approach improves onboarding: new engineers aprende desde concrete evidence trails instead de tribal memory.\n\n## Checklist\n- Define measurable rollback triggers en advance.\n- Classify incident phase before selecting response path.\n- Keep recovery herramientas replayable y deterministic.\n- Share one canonical incident reporte format.\n- Add regression fixtures after every rollback event.\n",
            "duration": "55 min"
          },
          "aum-v2-upgrade-report-markdown": {
            "title": "Desafio: construir stable upgrade markdown summary",
            "content": "Generate deterministic markdown desde releaseTag, totalBatches, y issueCount.",
            "duration": "35 min"
          },
          "aum-v2-upgrade-readiness-checkpoint": {
            "title": "Checkpoint: upgrade readiness artifact",
            "content": "Produce el final deterministic checkpoint artifact con release tag, readiness flag, y migration batch count.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-reliability": {
    "title": "Confiabilidad Engineering para Solana",
    "description": "Produccion-focused confiabilidad engineering para Solana sistemas: fault tolerance, retries, deadlines, circuit breakers, y graceful degradation con measurable operational outcomes.",
    "duration": "6 weeks",
    "tags": [
      "reliability",
      "fault-tolerance",
      "resilience",
      "production"
    ],
    "modules": {
      "mod-10-1": {
        "title": "Fault Tolerance Patrones",
        "description": "Implement fault-tolerance construiring blocks con clear failure classification, retry boundaries, y deterministic recovery behavior.",
        "lessons": {
          "lesson-10-1-1": {
            "title": "Comprension Fault Tolerance",
            "content": "Fault tolerance en Solana sistemas is not just about catching errores. It is about deciding which failures are safe a retry, which should fail fast, y como a preserve user trust while doing both.\n\nA practico confiabilidad modelo starts con failure classes:\n1) transient failures (timeouts, temporary RPC unavailability),\n2) persistent external failures (rate limits, prolonged endpoint degradation),\n3) deterministic business failures (invalid input, invariant violations).\n\nTransient failures may justify bounded retries con backoff. Deterministic business failures should not be retried because retries only add latencia y load. Persistent external failures often require fallback endpoints, degraded features, o temporary protection modes.\n\nEn Solana workflows, confiabilidad is tightly coupled a freshness constraints. A request can be logically valid but still fail if supporting estado has shifted (para example stale quote windows o expired blockhash contexts en client workflows). Confiable sistemas therefore combine retry logic con freshness checks y clear abort conditions.\n\nDefensive engineering means defining policies explicitly:\n- maximum retry count,\n- per-attempt timeout,\n- total deadline budget,\n- fallback behavior after budget exhaustion,\n- user-facing messaging para each failure class.\n\nSin explicit budgets, sistemas drift into infinite retry loops o fail too early. Con explicit budgets, behavior is predictable y testable.\n\nPara produccion teams, observability is mandatory. Every failed operation should include a deterministic reason code y context fields (attempt number, endpoint, elapsed time, policy branch). Este turns confiabilidad desde guesswork into measurable behavior.\n\nConfiable sistemas do not promise zero failures. They promise controlled failure behavior: bounded latencia, clear outcomes, y safe degradation under stress.",
            "duration": "45 min"
          },
          "lesson-10-1-2": {
            "title": "Retry Mechanism Desafio",
            "content": "Implement an exponential backoff retry mechanism para handling transient failures.",
            "duration": "45 min"
          },
          "lesson-10-1-3": {
            "title": "Deadline Manager Desafio",
            "content": "Implement a deadline management sistema a enforce time limits en operations.",
            "duration": "45 min"
          },
          "lesson-10-1-4": {
            "title": "Fallback Handler Desafio",
            "content": "Implement a fallback mechanism ese provides alternative execution paths when primary operations fail.",
            "duration": "45 min"
          }
        }
      },
      "mod-10-2": {
        "title": "Resilience Mechanisms",
        "description": "Construir resilience mechanisms (circuit breakers, bulkheads, y rate controls) ese protect core user flujos during provider instability.",
        "lessons": {
          "lesson-10-2-1": {
            "title": "Resilience Patrones",
            "content": "Resilience patrones are controls ese prevent localized failures desde becoming sistema-wide incidents. En Solana integrations, they are especially important because provider health can change quickly under bursty network conditions.\n\nCircuit breaker patron:\n- closed: normal operation,\n- open: block requests after repeated failures,\n- half-open: probe recovery con controlled trial requests.\n\nA good breaker uses deterministic thresholds y cooldowns, not ad hoc toggles. It should expose estado transitions para monitoring y post-incident review.\n\nBulkhead patron isolates resource pools so one failing workflow (para example expensive quote refresh loops) cannot starve unrelated workflows (like portfolio reads).\n\nRate limiting controls outbound pressure a providers. Proper limits reduce 429 storms y improve overall success rate. Token-bucket strategies are useful because they allow short bursts while preserving long-term bounds.\n\nEstos patrones should be coordinated, not layered blindly. Para example, aggressive retries plus weak rate limiting can bypass el intent de a circuit breaker. Policy composition must be reviewed end-a-end.\n\nA mature resilience stack includes:\n- deterministic policy config,\n- simulation fixtures para calm vs stressed traffic,\n- dashboard visibility para breaker estados y reject reasons,\n- explicit user copy para degraded mode.\n\nResilience is successful when users experience predictable service calidad under failure, not when sistemas appear perfect en ideal conditions.",
            "duration": "45 min"
          },
          "lesson-10-2-2": {
            "title": "Circuit Breaker Desafio",
            "content": "Implement a circuit breaker patron ese opens after consecutive failures y closes after a recovery period.",
            "duration": "45 min"
          },
          "lesson-10-2-3": {
            "title": "Rate Limiter Desafio",
            "content": "Implement a token bucket rate limiter para controlling request rates.",
            "duration": "45 min"
          },
          "lesson-10-2-4": {
            "title": "Error Classifier Desafio",
            "content": "Implement an error classification sistema a determine if errores are retryable.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-testing-strategies": {
    "title": "Pruebas Strategies para Solana",
    "description": "Comprehensive, produccion-oriented pruebas strategy para Solana: deterministic unit tests, realistic integration tests, fuzz/property pruebas, y release-confidence reporteing.",
    "duration": "5 weeks",
    "tags": [
      "testing",
      "quality-assurance",
      "fuzzing",
      "property-testing"
    ],
    "modules": {
      "mod-11-1": {
        "title": "Unit y Integration Pruebas",
        "description": "Construir deterministic unit y integration pruebas layers con clear ownership de invariants, fixtures, y failure diagnostics.",
        "lessons": {
          "lesson-11-1-1": {
            "title": "Pruebas Fundamentals",
            "content": "Pruebas Solana sistemas effectively requires layered confidence, not one giant test suite.\n\nUnit tests validate pure logic: math, estado transitions, y invariant checks. They should be fast, deterministic, y run en every change.\n\nIntegration tests validate component wiring: modelo de cuentasing, instruccion construction, y cross-modulo behavior under realistic inputs. They should catch schema drift y boundary errores ese unit tests miss.\n\nA practico test pyramid para Solana work:\n1) deterministic unit tests (broadest coverage),\n2) deterministic integration tests (targeted workflow coverage),\n3) environment-dependent checks (smaller set, higher cost).\n\nCommon failure en teams is over-reliance en environment-dependent tests while neglecting deterministic core checks. Este creates flaky CI y weak depuracion signals.\n\nGood test diseno principles:\n- explicit fixture ownership,\n- stable expected outputs,\n- structured error assertions (not only success assertions),\n- regression fixtures para previously discovered bugs.\n\nPara produccion readiness, test outputs should be easy a audit. Summaries should include pass/fail counts por category, failing invariant IDs, y deterministic reproduction inputs.\n\nPruebas is not just correctness verification; it is an operational communication tool. Strong test artifacts make release decisions clearer y incident response faster.",
            "duration": "45 min"
          },
          "lesson-11-1-2": {
            "title": "Test Assertion Framework Desafio",
            "content": "Implement a test assertion framework para verifying program estado.",
            "duration": "45 min"
          },
          "lesson-11-1-3": {
            "title": "Mock Cuenta Generator Desafio",
            "content": "Create a mock cuenta generator para pruebas con configurable parameters.",
            "duration": "45 min"
          },
          "lesson-11-1-4": {
            "title": "Test Scenario Construirer Desafio",
            "content": "Construir a test scenario construirer para setting up complex test environments.",
            "duration": "45 min"
          }
        }
      },
      "mod-11-2": {
        "title": "Avanzado Pruebas Techniques",
        "description": "Usa fuzzing, property-based tests, y mutation-style checks a expose edge-case failures before release.",
        "lessons": {
          "lesson-11-2-1": {
            "title": "Fuzzing y Property Pruebas",
            "content": "Avanzado pruebas techniques uncover failures ese example-based tests rarely find.\n\nFuzzing explores broad random input space a trigger parser edge cases, boundary overflows, y unexpected estado combinations. It is especially useful para serialization, decoding, y input validation layers.\n\nProperty-based pruebas defines invariants ese must hold across many generated inputs. Instead de asserting one output, you assert a rule (para example: balances never become negative, o decoded-then-encoded payload remains stable).\n\nMutation-style thinking strengthens este further: intentionally alter assumptions y verify tests fail as expected. If tests still pass after a harmful change, coverage is weaker than it appears.\n\nA keep avanzado pruebas practico:\n- usa deterministic seeds en CI para reproducibility,\n- store failing cases as permanent regression fixtures,\n- separate heavy campaigns desde per-commit fast checks.\n\nAvanzado tests are most valuable when tied a explicit riesgo categories. Map each category (serialization seguridad, invariant consistency, edge-case arithmetic) a at least one dedicated property o fuzz campaign.\n\nTeams ese treat fuzz/property failures as first-class release blockers catch subtle defects earlier y reduce high-severity produccion incidents.",
            "duration": "45 min"
          },
          "lesson-11-2-2": {
            "title": "Fuzz Input Generator Desafio",
            "content": "Implement a fuzz input generator para pruebas con random datos.",
            "duration": "45 min"
          },
          "lesson-11-2-3": {
            "title": "Property Verifier Desafio",
            "content": "Implement a property verifier ese checks invariants hold across operations.",
            "duration": "45 min"
          },
          "lesson-11-2-4": {
            "title": "Boundary Value Analyzer Desafio",
            "content": "Implement a boundary value analyzer para identifying edge cases.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-program-optimization": {
    "title": "Solana Program Optimizacion",
    "description": "Engineer produccion-grade Solana rendimiento: compute budgeting, cuenta layout efficiency, memory/rent tradeoffs, y deterministic optimizacion workflows.",
    "duration": "5 weeks",
    "tags": [
      "optimization",
      "performance",
      "compute-units",
      "profiling"
    ],
    "modules": {
      "mod-12-1": {
        "title": "Compute Unit Optimizacion",
        "description": "Optimize compute-heavy paths con explicit CU budgets, operation-level profiling, y predictable rendimiento tradeoffs.",
        "lessons": {
          "lesson-12-1-1": {
            "title": "Comprension Compute Units",
            "content": "Compute units are el hard resource budget ese shapes que your Solana program can do en a single transaccion. Rendimiento optimizacion starts por treating CU usage as a contract, not an afterthought.\n\nA confiable optimizacion loop is:\n1) measure baseline CU por operation type,\n2) identify dominant cost buckets (deserialization, hashing, loops, CPI fan-out),\n3) optimize one hotspot at a time,\n4) re-measure y keep only changes con clear gains.\n\nCommon anti-patrones include optimizing cold paths, adding complexity sin measurement, y ignoring cuenta-size side effects. En Solana sistemas, compute y cuenta diseno are coupled: a larger cuenta can increase deserialization cost, which raises CU pressure.\n\nPara produccion teams, deterministic cost fixtures are crucial. They let you compare before/after behavior en CI y stop regressions early. Rendimiento work is most useful when every claim is backed por reproducible evidence, not intuition.",
            "duration": "45 min"
          },
          "lesson-12-1-2": {
            "title": "CU Counter Desafio",
            "content": "Implement a compute unit counter a estimate operation costs.",
            "duration": "45 min"
          },
          "lesson-12-1-3": {
            "title": "Datos Structure Optimizer Desafio",
            "content": "Optimize datos structures para minimal serialization overhead.",
            "duration": "45 min"
          },
          "lesson-12-1-4": {
            "title": "Batch Operation Optimizer Desafio",
            "content": "Optimize batch operations a minimize compute units.",
            "duration": "45 min"
          }
        }
      },
      "mod-12-2": {
        "title": "Memory y Storage Optimizacion",
        "description": "Diseno memory/storage-efficient cuenta layouts con rent-aware sizing, serialization discipline, y safe migration planning.",
        "lessons": {
          "lesson-12-2-1": {
            "title": "Cuenta Datos Optimizacion",
            "content": "Cuenta datos optimizacion is both a cost y correctness discipline. Poor layouts increase rent, slow parsing, y make migrations fragile.\n\nDiseno principles:\n- Keep hot fields compact y easy a parse.\n- Usa fixed-size representations where possible.\n- Reserve growth strategy explicitly instead de ad hoc field expansion.\n- Separate frequently-mutated datos desde rarely-changed metadata when practico.\n\nLayout decisions should be documented con deterministic artifacts: field offsets, total bytes, y expected rent class. If a schema change increases cuenta size, reviewers should see el exact delta y migration implications.\n\nProduccion optimizacion is not “smallest possible struct at any cost.” It is stable, readable, y migration-safe storage ese keeps compute y rent budgets predictable over time.",
            "duration": "45 min"
          },
          "lesson-12-2-2": {
            "title": "Datos Packer Desafio",
            "content": "Implement a datos packer ese efficiently packs fields into cuenta datos.",
            "duration": "45 min"
          },
          "lesson-12-2-3": {
            "title": "Rent Calculator Desafio",
            "content": "Implement a rent calculator para estimating cuenta storage costs.",
            "duration": "45 min"
          },
          "lesson-12-2-4": {
            "title": "Zero-Copy Deserializer Desafio",
            "content": "Implement a zero-copy deserializer para reading datos sin allocation.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-tokenomics-design": {
    "title": "Tokenomics Diseno para Solana",
    "description": "Diseno robusto Solana token economies con distribution discipline, vesting seguridad, staking incentives, y gobernanza mechanics ese remain operationally defensible.",
    "duration": "5 weeks",
    "tags": [
      "tokenomics",
      "vesting",
      "staking",
      "governance",
      "incentives"
    ],
    "modules": {
      "mod-13-1": {
        "title": "Token Distribution y Vesting",
        "description": "Modelo token allocation y vesting sistemas con explicit fairness, unlock predictability, y deterministic cuentaing rules.",
        "lessons": {
          "lesson-13-1-1": {
            "title": "Token Distribution Fundamentals",
            "content": "Token distribution is a seguridad y credibility decision, not just a spreadsheet exercise. Allocation y vesting rules shape long-term trust en el protocol.\n\nA strong distribution modelo answers:\n- who receives tokens y por que,\n- when they unlock,\n- como unlock schedules affect circulating supply,\n- que controls prevent accidental over-distribution.\n\nVesting diseno should be deterministic y auditable. Cliff y linear phases must produce reproducible release amounts para any timestamp. Ambiguous rounding rules create disputes y operational riesgo.\n\nProduccion teams should maintain allocation invariants en tests (para example: total distributed <= total supply, per-bucket caps respected, no negative vesting balances). Tokenomics calidad improves when economics y implementation are validated together.",
            "duration": "45 min"
          },
          "lesson-13-1-2": {
            "title": "Vesting Schedule Calculator Desafio",
            "content": "Implement a vesting schedule calculator con cliff y linear release.",
            "duration": "45 min"
          },
          "lesson-13-1-3": {
            "title": "Token Allocation Distributor Desafio",
            "content": "Implement a token allocation distributor para managing different stakeholder groups.",
            "duration": "45 min"
          },
          "lesson-13-1-4": {
            "title": "Release Schedule Generator Desafio",
            "content": "Generate a complete release schedule con dates y amounts.",
            "duration": "45 min"
          }
        }
      },
      "mod-13-2": {
        "title": "Staking y Gobernanza",
        "description": "Diseno staking y gobernanza mechanics con clear incentive alignment, anti-manipulation constraints, y measurable participation health.",
        "lessons": {
          "lesson-13-2-1": {
            "title": "Staking y Gobernanza Diseno",
            "content": "Staking y gobernanza sistemas must balance participation incentives con manipulation resistance. Rewarding lock behavior is useful, but poorly tuned models can over-concentrate influence.\n\nCore diseno questions:\n1) Como is staking reward rate set y adjusted?\n2) Como is voting power calculated (raw balance, delegated balance, time-weighted)?\n3) Que prevents short-term gobernanza capture?\n\nGobernanza math should be transparent y deterministic so users can verify voting outcomes independently. If power calculations are opaque o inconsistent, gobernanza trust collapses quickly.\n\nOperationally, track gobernanza health metrics: voter participation, delegation concentration, proposal pass patrones, y inactive stake ratios. Tokenomics is successful only when en-chain incentive behavior matches intended gobernanza outcomes.",
            "duration": "45 min"
          },
          "lesson-13-2-2": {
            "title": "Staking Calculator Desafio",
            "content": "Implement a staking rewards calculator con compounding.",
            "duration": "45 min"
          },
          "lesson-13-2-3": {
            "title": "Voting Power Calculator Desafio",
            "content": "Implement a voting power calculator con delegation support.",
            "duration": "45 min"
          },
          "lesson-13-2-4": {
            "title": "Proposal Threshold Calculator Desafio",
            "content": "Implement a proposal threshold calculator para gobernanza.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-defi-primitives": {
    "title": "DeFi Primitives en Solana",
    "description": "Construir practico Solana DeFi foundations: AMM mechanics, liquidez cuentaing, lending primitives, y flash-loan-safe composition patrones.",
    "duration": "6 weeks",
    "tags": [
      "defi",
      "amm",
      "lending",
      "yield-farming",
      "flash-loans"
    ],
    "modules": {
      "mod-14-1": {
        "title": "AMM y Liquidez",
        "description": "Implement AMM y liquidez primitives con deterministic math, deslizamiento-aware outputs, y LP cuentaing correctness.",
        "lessons": {
          "lesson-14-1-1": {
            "title": "AMM Fundamentals",
            "content": "AMM fundamentals are simple en formula but subtle en implementation calidad. El invariant math must be deterministic, comision handling explicit, y rounding behavior consistent across paths.\n\nPara constant-product pools, route calidad is determined por output-at-size, not headline spot price. Larger trades move further en el curve y experience stronger impact, so quote logic must be size-aware.\n\nLiquidez cuentaing must also be exact: LP shares, comision accrual, y pool reserve updates should remain internally consistent under repeated swaps y edge-case sizes.\n\nProduccion DeFi teams treat AMM math as critical infrastructure. They usa fixed fixtures para swap-en/swap-out cases, boundary amounts, y comision tiers so regressions are caught before despliegue.",
            "duration": "45 min"
          },
          "lesson-14-1-2": {
            "title": "Constant Product AMM Desafio",
            "content": "Implement a constant product AMM para token swaps.",
            "duration": "45 min"
          },
          "lesson-14-1-3": {
            "title": "Liquidez Provider Calculator Desafio",
            "content": "Calculate LP token minting y rewards para liquidez providers.",
            "duration": "45 min"
          },
          "lesson-14-1-4": {
            "title": "Price Oracle Desafio",
            "content": "Implement a time-weighted average price oracle.",
            "duration": "45 min"
          }
        }
      },
      "mod-14-2": {
        "title": "Lending y Flash Loans",
        "description": "Modelo lending y flash-loan flujos con collateral seguridad, utilization-aware pricing, y strict repayment invariants.",
        "lessons": {
          "lesson-14-2-1": {
            "title": "Lending Protocol Mechanics",
            "content": "Lending primitives y flash-loan logic are powerful but unforgiving. Seguridad depends en strict collateral valuation, clear LTV/threshold rules, y deterministic repayment checks.\n\nA practico lending modelo should define:\n- collateral valuation source y freshness policy,\n- borrow limits y liquidation thresholds,\n- utilization-based rate behavior,\n- liquidation y bad-debt handling paths.\n\nFlash loans add atomic constraints: borrowed amount plus comision must be repaid en el same transaccion context. Any relaxation de este invariant introduces severe riesgo.\n\nComposable DeFi diseno works when every primitive preserves local seguridad contracts while exposing clear interfaces para higher-level orchestration.",
            "duration": "45 min"
          },
          "lesson-14-2-2": {
            "title": "Collateral Calculator Desafio",
            "content": "Implement collateral y borrowing power calculations.",
            "duration": "45 min"
          },
          "lesson-14-2-3": {
            "title": "Interest Rate Modelo Desafio",
            "content": "Implement a utilization-based interest rate modelo.",
            "duration": "45 min"
          },
          "lesson-14-2-4": {
            "title": "Flash Loan Validador Desafio",
            "content": "Implement flash loan validation logic.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-nft-standards": {
    "title": "NFT Standards en Solana",
    "description": "Implement Solana NFTs con produccion-ready standards: metadata integrity, collection discipline, y avanzado programmable/non-transferable behaviors.",
    "duration": "5 weeks",
    "tags": [
      "nft",
      "metaplex",
      "token-metadata",
      "candy-machine",
      "soulbound"
    ],
    "modules": {
      "mod-15-1": {
        "title": "NFT Fundamentals",
        "description": "Construir core NFT functionality con standards-compliant metadata, collection verification, y deterministic asset-estado handling.",
        "lessons": {
          "lesson-15-1-1": {
            "title": "NFT Architecture en Solana",
            "content": "NFT architecture en Solana combines token mechanics con metadata y collection semantics. A correct implementation requires more than minting a token con supply one.\n\nCore components include:\n- mint/estado ownership correctness,\n- metadata integrity y schema consistency,\n- collection linkage y verification status,\n- transfer y authority policy clarity.\n\nProduccion NFT sistemas should treat metadata as a contract. If fields drift o verification flags are inconsistent, marketplaces y carteras may interpret assets differently.\n\nConfiable implementations include deterministic validation para metadata structure, creator share totals, collection references, y authority expectations. Standards compliance is que preserves interoperability.",
            "duration": "45 min"
          },
          "lesson-15-1-2": {
            "title": "NFT Metadata Parser Desafio",
            "content": "Parse y validate NFT metadata according a Metaplex standards.",
            "duration": "45 min"
          },
          "lesson-15-1-3": {
            "title": "Collection Manager Desafio",
            "content": "Implement NFT collection management con size tracking.",
            "duration": "45 min"
          },
          "lesson-15-1-4": {
            "title": "Attribute Rarity Calculator Desafio",
            "content": "Calculate NFT attribute rarity scores.",
            "duration": "45 min"
          }
        }
      },
      "mod-15-2": {
        "title": "Avanzado NFT Features",
        "description": "Implement avanzado NFT behaviors (soulbound y programmable flujos) con explicit policy controls y safe update semantics.",
        "lessons": {
          "lesson-15-2-1": {
            "title": "Soulbound y Programmable NFTs",
            "content": "Avanzado NFT features introduce policy complexity ese must be explicit. Soulbound behavior, programmable restrictions, y dynamic metadata updates all expand failure surface.\n\nPara soulbound models, non-transferability must be enforced por clear rule paths, not UI assumptions. Para programmable NFTs, update permissions y transition rules should be deterministic y auditable.\n\nDynamic NFT updates should include strong validation y event clarity so indexers y clients can track estado changes correctly.\n\nAvanzado NFT engineering succeeds when flexibility is paired con strict policy boundaries y transparent update behavior.",
            "duration": "45 min"
          },
          "lesson-15-2-2": {
            "title": "Soulbound Token Validador Desafio",
            "content": "Implement validation para soulbound (non-transferable) tokens.",
            "duration": "45 min"
          },
          "lesson-15-2-3": {
            "title": "Dynamic NFT Updater Desafio",
            "content": "Implement dynamic NFT attributes ese can evolve over time.",
            "duration": "45 min"
          },
          "lesson-15-2-4": {
            "title": "NFT Composability Desafio",
            "content": "Implement NFT composability para equipping items a base NFTs.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-cpi-patterns": {
    "title": "Invocacion entre programas Patrones",
    "description": "Master CPI composition en Solana con safe cuenta validation, PDA signer discipline, y deterministic multi-program orchestration patrones.",
    "duration": "6 weeks",
    "tags": [
      "cpi",
      "cross-program-invocation",
      "composition",
      "pda-signing"
    ],
    "modules": {
      "mod-16-1": {
        "title": "CPI Fundamentals",
        "description": "Construir CPI fundamentals con strict cuenta/signer checks, ownership validation, y safe PDA signing boundaries.",
        "lessons": {
          "lesson-16-1-1": {
            "title": "Invocacion entre programas Architecture",
            "content": "Invocacion entre programas (CPI) is where Solana composability becomes practico y where many seguridad failures appear. El caller controls cuenta lists, so every CPI boundary must be treated as untrusted input.\n\nSafe CPI diseno requires:\n- explicit cuenta identity y owner validation,\n- signer y writable scope minimization,\n- deterministic PDA derivation y signer-seed handling,\n- bounded assumptions about downstream program behavior.\n\ninvoke y invoke_signed are not interchangeable conveniences. invoke_signed should only be usado when signer prueba is truly required y seed recipes are canonical.\n\nProduccion CPI confiabilidad comes desde repeatable guard patrones. If constraints vary handler a handler, reviewers cannot reason about seguridad consistently.",
            "duration": "45 min"
          },
          "lesson-16-1-2": {
            "title": "CPI Cuenta Validador Desafio",
            "content": "Validate cuentas para invocacion entre programass.",
            "duration": "45 min"
          },
          "lesson-16-1-3": {
            "title": "PDA Signer Desafio",
            "content": "Implement PDA signing para CPI operations.",
            "duration": "45 min"
          },
          "lesson-16-1-4": {
            "title": "Instruccion Router Desafio",
            "content": "Implement an instruccion router para directing CPI calls.",
            "duration": "45 min"
          }
        }
      },
      "mod-16-2": {
        "title": "Avanzado CPI Patrones",
        "description": "Compose avanzado multi-program flujos con atomicity awareness, consistency checks, y deterministic failure handling.",
        "lessons": {
          "lesson-16-2-1": {
            "title": "Multi-Program Composition",
            "content": "Multi-program composition introduces sequencing y consistency riesgo. Even when each CPI call is correct en isolation, combined flujos can violate business invariants if ordering o rollback assumptions are weak.\n\nRobusto composition patrones include:\n1) explicit stage boundaries,\n2) invariant checks between CPI steps,\n3) deterministic error classes para partial-failure diagnosis,\n4) idempotent recovery paths where possible.\n\nPara complex operations (atomic swaps, flash-loan sequences), modelo expected preconditions y postconditions per stage. Este keeps orchestration testable y prevents hidden estado drift.\n\nCPI mastery is less about calling many programs y more about preserving correctness across program boundaries under adverse inputs.",
            "duration": "45 min"
          },
          "lesson-16-2-2": {
            "title": "Atomic Swap Orchestrator Desafio",
            "content": "Implement an atomic swap across multiple programs.",
            "duration": "45 min"
          },
          "lesson-16-2-3": {
            "title": "Estado Consistency Validador Desafio",
            "content": "Validate estado consistency across multiple CPI calls.",
            "duration": "45 min"
          },
          "lesson-16-2-4": {
            "title": "Permissioned Invocation Desafio",
            "content": "Implement permission checks para program invocations.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-mev-strategies": {
    "title": "MEV y Transaccion Ordering",
    "description": "Produccion-focused transaccion-ordering engineering en Solana: MEV-aware routing, bundle strategy, liquidation/arbitrage modeling, y user-protective execution controls.",
    "duration": "6 weeks",
    "tags": [
      "mev",
      "arbitrage",
      "liquidation",
      "jito",
      "priority-fees",
      "sandwich"
    ],
    "modules": {
      "mod-17-1": {
        "title": "MEV Fundamentals",
        "description": "Understand MEV mechanics y transaccion ordering realities, then modelo opportunities y riesgos con deterministic seguridad-aware policies.",
        "lessons": {
          "lesson-17-1-1": {
            "title": "MEV en Solana",
            "content": "Maximal Extractable Value (MEV) en Solana is fundamentally about transaccion ordering under limited blockspace. Whether you are construiring trading tools, liquidation infrastructure, o user-facing apps, you need a realistic modelo de como ordering pressure changes outcomes.\n\nKey Solana-specific context:\n- ordering can be influenced por priority comisiones y relay/bundle paths,\n- opportunities are short-lived y highly competitive,\n- failed o delayed execution can convert expected profit into loss.\n\nA mature MEV approach begins con classification:\n1) opportunity class (arbitrage, liquidation, backrun-style sequencing),\n2) dependency class (single-hop vs multi-hop, oracle-sensitive vs pool-estado-sensitive),\n3) riesgo class (staleness, fill failure, adverse movement, execution contention).\n\nPara produccion sistemas, raw opportunity detection is not enough. You need deterministic filters ese reject fragile setups: stale quotes, weak depth, o excessive path complexity relative a expected edge.\n\nMost operational failures come desde execution assumptions, not math. Teams should modelo inclusion probability, fallback paths, y cancellation thresholds explicitly.\n\nUser-protective diseno matters even para avanzado orderflow sistemas. Clear policy around deslizamiento limits, quote freshness, y failure reporteing prevents silent value leakage y reduces support incidents.",
            "duration": "45 min"
          },
          "lesson-17-1-2": {
            "title": "Arbitrage Opportunity Detector Desafio",
            "content": "Detect arbitrage opportunities across DEXes.",
            "duration": "45 min"
          },
          "lesson-17-1-3": {
            "title": "Liquidation Opportunity Finder Desafio",
            "content": "Find undercollateralized positions para liquidation.",
            "duration": "45 min"
          },
          "lesson-17-1-4": {
            "title": "Priority Comision Calculator Desafio",
            "content": "Calculate optimal priority comisiones para transaccion ordering.",
            "duration": "45 min"
          }
        }
      },
      "mod-17-2": {
        "title": "Avanzado MEV Strategies",
        "description": "Diseno avanzado ordering/bundle strategies con explicit riesgo controls, failure handling, y user-impact guardrails.",
        "lessons": {
          "lesson-17-2-1": {
            "title": "Avanzado MEV Techniques",
            "content": "Avanzado transaccion-ordering strategies require disciplined orchestration, not just faster opportunity scans.\n\nBundle-oriented execution is valuable because it can express dependency sets y all-o-nothing intent, but bundle diseno must include:\n- strict preconditions,\n- deterministic abort rules,\n- replay-safe identifiers,\n- post-execution reconciliation.\n\nWhen strategy complexity increases (multi-hop paths, conditional liquidations), failure modes multiply: partial fills, stale assumptions, y contention spikes. A robusto sistema ranks candidates por expected net value after execution riesgo, not gross theoretical edge.\n\nOperational controls should include:\n1) bounded retries con fresh-estado checks,\n2) confidence scoring para each candidate,\n3) kill-switch thresholds para abnormal failure streaks,\n4) deterministic run reportes para incident replay.\n\nAvanzado MEV herramientas is successful when it is both profitable y controllable. Deterministic artifacts (decision inputs, chosen path, reason codes) are que make ese control real en produccion.",
            "duration": "45 min"
          },
          "lesson-17-2-2": {
            "title": "Bundle Composer Desafio",
            "content": "Compose transaccion bundles para atomic MEV extraction.",
            "duration": "45 min"
          },
          "lesson-17-2-3": {
            "title": "Multi-Hop Arbitrage Finder Desafio",
            "content": "Find multi-hop arbitrage paths across token pairs.",
            "duration": "45 min"
          },
          "lesson-17-2-4": {
            "title": "MEV Simulation Engine Desafio",
            "content": "Simulate MEV extraction a estimate profitability.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-deployment-cicd": {
    "title": "Program Despliegue y CI/CD",
    "description": "Produccion despliegue engineering para Solana programs: environment strategy, release gating, CI/CD calidad controls, y upgrade-safe operational workflows.",
    "duration": "4 weeks",
    "tags": [
      "deployment",
      "cicd",
      "devnet",
      "mainnet",
      "upgrades",
      "testing"
    ],
    "modules": {
      "mod-18-1": {
        "title": "Despliegue Fundamentals",
        "description": "Modelo environment-specific despliegue behavior con deterministic configs, artifact checks, y release preflight validation.",
        "lessons": {
          "lesson-18-1-1": {
            "title": "Solana Despliegue Environments",
            "content": "Solana despliegue is not one command; it is a release sistema con environment-specific riesgo. Localnet, devnet, y mainnet each serve different validation goals, y produccion calidad depends en usando them intentionally.\n\nA confiable despliegue workflow defines:\n1) environment purpose y promotion criteria,\n2) deterministic config sources,\n3) artifact identity checks,\n4) rollback triggers.\n\nCommon failure patrones include mismatched program IDs, inconsistent config between environments, y unverified artifact drift between construir y deploy. Estos are process issues ese herramientas should prevent.\n\nPreflight checks should be mandatory:\n- expected network y signer identity,\n- construir artifact hash,\n- program size y upgrade constraints,\n- required cuenta/address assumptions.\n\nEnvironment promotion should be evidence-driven. Passing local tests alone is not enough para mainnet readiness; devnet/staging behavior must confirm operational assumptions under realistic RPC y timing conditions.\n\nDespliegue maturity is measured por reproducibility. If another engineer cannot replay el release inputs y get el same artifact y checklist outcome, el pipeline is too fragile.",
            "duration": "45 min"
          },
          "lesson-18-1-2": {
            "title": "Despliegue Config Manager Desafio",
            "content": "Manage despliegue configurations para different environments.",
            "duration": "45 min"
          },
          "lesson-18-1-3": {
            "title": "Program Size Validador Desafio",
            "content": "Validate program binary size y compute budget.",
            "duration": "45 min"
          },
          "lesson-18-1-4": {
            "title": "Upgrade Authority Manager Desafio",
            "content": "Manage program upgrade authorities y permissions.",
            "duration": "45 min"
          }
        }
      },
      "mod-18-2": {
        "title": "CI/CD Pipelines",
        "description": "Construir CI/CD pipelines ese enforce construir/test/seguridad gates, compatibility checks, y controlled rollout/rollback evidence.",
        "lessons": {
          "lesson-18-2-1": {
            "title": "CI/CD para Solana Programs",
            "content": "CI/CD para Solana should enforce release calidad, not just automate command execution.\n\nA practico pipeline includes staged gates:\n1) static calidad gate (lint/type/seguridad checks),\n2) deterministic unit/integration tests,\n3) construir reproducibility y artifact hashing,\n4) despliegue preflight validation,\n5) controlled rollout con observability checks.\n\nEach gate should produce machine-readable evidence. Release decisions become faster y safer when teams can inspect deterministic artifacts instead de scanning raw logs.\n\nVersion compatibility checks are critical en Solana ecosystems where CLI/toolchain mismatches can break construirs o runtime expectations. Pipelines should fail fast en incompatible matrices.\n\nRollout strategy should also be explicit: canary/degraded mode, monitoring window, y rollback conditions. “Deploy y hope” is not a strategy.\n\nEl best CI/CD sistemas reduce human toil while increasing decision clarity. Automation should encode operational policy, not bypass it.",
            "duration": "45 min"
          },
          "lesson-18-2-2": {
            "title": "Construir Pipeline Validador Desafio",
            "content": "Validate CI/CD pipeline stages y dependencies.",
            "duration": "45 min"
          },
          "lesson-18-2-3": {
            "title": "Version Compatibility Checker Desafio",
            "content": "Check version compatibility between tools y dependencies.",
            "duration": "45 min"
          },
          "lesson-18-2-4": {
            "title": "Despliegue Rollback Manager Desafio",
            "content": "Manage despliegue rollbacks y recovery.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-cross-chain-bridges": {
    "title": "Cross-Chain Bridges y Wormhole",
    "description": "Construir safer cross-chain integrations para Solana usando Wormhole-style messaging, attestation verification, y deterministic bridge-estado controls.",
    "duration": "6 weeks",
    "tags": [
      "bridges",
      "wormhole",
      "cross-chain",
      "interoperability",
      "messaging"
    ],
    "modules": {
      "mod-43-1": {
        "title": "Wormhole Messaging Fundamentals",
        "description": "Understand cross-chain messaging trust boundaries, guardian attestations, y deterministic verification pipelines.",
        "lessons": {
          "lesson-43-1-1": {
            "title": "Cross-Chain Messaging Architecture",
            "content": "Cross-chain messaging is a trust-boundary problem before it is a transport problem. En Wormhole-style sistemas, messages are observed, attested, y consumed across different chain environments, each con independent failure modes.\n\nA robusto architecture modelo includes:\n1) emitter semantics (que exactly is being attested),\n2) attestation verification (who signed y under que threshold),\n3) replay prevention (message uniqueness y consumption estado),\n4) execution seguridad (que happens if target-chain estado has changed).\n\nVerification must be deterministic y strict. Accepting malformed o weakly validated attestations is a direct seguridad riesgo.\n\nCross-chain sistemas should also expose explicit reason codes para rejects: invalid signatures, stale message, already-consumed message, unsupported payload schema. Este improves operator response y audit calidad.\n\nMessaging confiabilidad depends en observability. Teams need deterministic logs linking source event IDs a target execution outcomes so they can reconcile partial o delayed flujos.\n\nCross-chain engineering succeeds when attestation trust assumptions are transparent y enforced consistently at every consume path.",
            "duration": "45 min"
          },
          "lesson-43-1-2": {
            "title": "VAA Verifier Desafio",
            "content": "Implement VAA (Verified Action Approval) signature verification.",
            "duration": "45 min"
          },
          "lesson-43-1-3": {
            "title": "Message Emitter Desafio",
            "content": "Implement cross-chain message emission y tracking.",
            "duration": "45 min"
          },
          "lesson-43-1-4": {
            "title": "Replay Protection Desafio",
            "content": "Implement replay protection para cross-chain messages.",
            "duration": "45 min"
          }
        }
      },
      "mod-43-2": {
        "title": "Asset Bridging Patrones",
        "description": "Implement asset-bridging patrones con strict supply/cuentaing invariants, replay protection, y reconciliation workflows.",
        "lessons": {
          "lesson-43-2-1": {
            "title": "Token Bridging Mechanics",
            "content": "Token bridging requires strict supply y estado invariants. Lock-y-mint y burn-y-mint models both rely en one central rule: represented supply across chains must remain coherent.\n\nCritical controls include:\n- single-consume message semantics,\n- deterministic mint/unlock cuentaing,\n- paused-mode handling para incident containment,\n- reconciliation reportes between source y target totals.\n\nA bridge flujo should define estado transitions explicitly: initiated, attested, executed, reconciled. Missing estado transitions create operational blind spots.\n\nReplay y duplication are recurring bridge riesgos. Sistemas must key transfer intents deterministically y reject repeated execution attempts even under retries o delayed relays.\n\nProduccion bridge operations also need runbooks: que a do en attestation delays, threshold signer issues, o target-chain execution failures.\n\nBridging calidad is not just rendimiento; it is verifiable cuentaing integrity under adverse network conditions.",
            "duration": "45 min"
          },
          "lesson-43-2-2": {
            "title": "Token Locker Desafio",
            "content": "Implement token locking para bridge deposits.",
            "duration": "45 min"
          },
          "lesson-43-2-3": {
            "title": "Wrapped Token Mint Desafio",
            "content": "Manage wrapped token minting y supply tracking.",
            "duration": "45 min"
          },
          "lesson-43-2-4": {
            "title": "Bridge Rate Limiter Desafio",
            "content": "Implement rate limiting para bridge withdrawals.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-oracle-pyth": {
    "title": "Oracle Integration y Pyth Network",
    "description": "Integrate Solana oracle comisionds safely: price validation, confidence/staleness policy, y multi-source aggregation para resilient protocol decisions.",
    "duration": "5 weeks",
    "tags": [
      "oracle",
      "pyth",
      "price-feeds",
      "data-validation",
      "aggregation"
    ],
    "modules": {
      "mod-44-1": {
        "title": "Price Comisiond Fundamentals",
        "description": "Understand oracle datos semantics (price, confidence, staleness) y enforce deterministic validation before business logic.",
        "lessons": {
          "lesson-44-1-1": {
            "title": "Oracle Price Comisionds",
            "content": "Oracle integration is a riesgo-control problem, not a datos-fetch problem. Price comisionds must be evaluated para freshness, confidence, y contextual fitness before they drive protocol decisions.\n\nA safe oracle validation pipeline checks:\n1) comisiond status y availability,\n2) staleness window compliance,\n3) confidence-band reasonableness,\n4) value bounds against protocol policy.\n\nUsando raw price sin confidence o staleness checks can trigger invalid liquidations, bad quotes, o incorrect riesgo assessments.\n\nValidation outputs should be deterministic y structured (accept/reject con reason code). Este helps downstream sistemas choose safe fallback behavior.\n\nProtocols should separate “datos exists” desde “datos is usable.” A comisiond can be present but still unfit due a stale timestamp o extreme uncertainty.\n\nProduccion confiabilidad improves when oracle checks are versioned y fixture-tested across calm y stressed market scenarios.",
            "duration": "45 min"
          },
          "lesson-44-1-2": {
            "title": "Price Validador Desafio",
            "content": "Validate oracle prices para correctness y freshness.",
            "duration": "45 min"
          },
          "lesson-44-1-3": {
            "title": "Price Normalizer Desafio",
            "content": "Normalize prices con different exponents a common scale.",
            "duration": "45 min"
          },
          "lesson-44-1-4": {
            "title": "EMA Calculator Desafio",
            "content": "Calculate Exponential Moving Average para price smoothing.",
            "duration": "45 min"
          }
        }
      },
      "mod-44-2": {
        "title": "Multi-Oracle Aggregation",
        "description": "Diseno multi-oracle aggregation y consensus policies ese reduce single-source failure riesgo while remaining explainable y testable.",
        "lessons": {
          "lesson-44-2-1": {
            "title": "Oracle Aggregation Strategies",
            "content": "Multi-oracle aggregation reduces single-point dependency but adds policy complexity. El goal is not a average blindly; it is a produce a robusto decision value con clear confidence en adverse conditions.\n\nCommon strategies include median, trimmed mean, y weighted consensus. Strategy choice should reflect threat modelo: outlier resistance, latencia tolerance, y source diversity.\n\nAggregation policies should define:\n- minimum participating sources,\n- max divergence threshold,\n- fallback action when consensus fails.\n\nIf sources diverge beyond policy bounds, el safe action may be a halt sensitive operations rather than force a number.\n\nDeterministic aggregation reportes should include contributing sources, excluded outliers, y final consensus rationale. Este is essential para audits y incident response.\n\nA good oracle stack is transparent: every accepted value can be explained, replayed, y defended.",
            "duration": "45 min"
          },
          "lesson-44-2-2": {
            "title": "Median Price Calculator Desafio",
            "content": "Calculate median price desde multiple oracle sources.",
            "duration": "45 min"
          },
          "lesson-44-2-3": {
            "title": "Oracle Consensus Desafio",
            "content": "Implement consensus checking across multiple oracle sources.",
            "duration": "45 min"
          },
          "lesson-44-2-4": {
            "title": "Fallback Oracle Manager Desafio",
            "content": "Manage primary y fallback oracle sources.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-dao-tooling": {
    "title": "DAO Herramientas y Autonomous Organizations",
    "description": "Construir produccion-ready DAO sistemas en Solana: proposal gobernanza, voting integrity, treasury controls, y deterministic execution/reporteing workflows.",
    "duration": "5 weeks",
    "tags": [
      "dao",
      "governance",
      "proposals",
      "voting",
      "treasury",
      "automation"
    ],
    "modules": {
      "mod-45-1": {
        "title": "DAO Gobernanza Mechanics",
        "description": "Implement gobernanza mechanics con explicit proposal lifecycle rules, voting-power logic, y deterministic estado transitions.",
        "lessons": {
          "lesson-45-1-1": {
            "title": "DAO Gobernanza Architecture",
            "content": "DAO gobernanza architecture is a sistema de enforceable process rules. Proposal creation, voting, y execution must be deterministic, auditable, y resistant a manipulation.\n\nA robusto gobernanza modelo defines:\n1) proposal lifecycle estados y transitions,\n2) voter eligibility y power calculation,\n3) quorum/approval thresholds por action class,\n4) execution preconditions y cancellation paths.\n\nGobernanza failures usually come desde ambiguity: unclear thresholds, inconsistent snapshot timing, o weak transition validation.\n\nEstado transitions should be explicit y testable. Invalid transitions (para example executed -> voting) should fail con deterministic errores.\n\nVoting-power logic must also be transparent. Whether delegation, time-weighting, o quadratic models are usado, outcomes should be reproducible desde public inputs.\n\nDAO herramientas calidad is measured por predictability under pressure. During contentious proposals, deterministic behavior y clear reason codes are que preserve legitimacy.",
            "duration": "45 min"
          },
          "lesson-45-1-2": {
            "title": "Proposal Lifecycle Manager Desafio",
            "content": "Manage DAO proposal estados y transitions.",
            "duration": "45 min"
          },
          "lesson-45-1-3": {
            "title": "Voting Power Calculator Desafio",
            "content": "Calculate voting power con delegation y quadratic options.",
            "duration": "45 min"
          },
          "lesson-45-1-4": {
            "title": "Delegation Manager Desafio",
            "content": "Manage vote delegation between DAO members.",
            "duration": "45 min"
          }
        }
      },
      "mod-45-2": {
        "title": "Treasury y Execution",
        "description": "Engineer treasury y execution herramientas con policy gates, timelock safeguards, y auditable automation outcomes.",
        "lessons": {
          "lesson-45-2-1": {
            "title": "DAO Treasury Management",
            "content": "DAO treasury management is where gobernanza intent becomes real financial action. Treasury herramientas must therefore combine flexibility con strict policy constraints.\n\nCore controls include:\n- spending limits y role-based authority,\n- timelock windows para sensitive actions,\n- multisig/escalation paths,\n- deterministic execution logs.\n\nAutomated execution should never hide policy checks. Every executed action should reference el proposal, required approvals, y control checks passed.\n\nFailure handling is equally important. If execution fails mid-flujo, herramientas should expose exact failure stage y safe retry/rollback guidance.\n\nTreasury sistemas should also produce reconciliation artifacts: proposed vs executed amounts, remaining budget, y exception records.\n\nOperationally mature DAOs treat treasury automation as regulated process infrastructure: explicit controls, reproducible evidence, y clear cuentaability boundaries.",
            "duration": "45 min"
          },
          "lesson-45-2-2": {
            "title": "Treasury Spending Limit Desafio",
            "content": "Implement spending limits y budget tracking para DAO treasury.",
            "duration": "45 min"
          },
          "lesson-45-2-3": {
            "title": "Timelock Executor Desafio",
            "content": "Implement timelock para delayed proposal execution.",
            "duration": "45 min"
          },
          "lesson-45-2-4": {
            "title": "Automated Action Trigger Desafio",
            "content": "Implement automated triggers para DAO actions based en conditions.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-gaming": {
    "title": "Gaming y Game Estado Management",
    "description": "Construir produccion-ready en-chain game sistemas en Solana: efficient estado models, turn integrity, fairness controls, y scalable player progression economics.",
    "duration": "5 weeks",
    "tags": [
      "gaming",
      "game-state",
      "randomness",
      "turn-based",
      "progression"
    ],
    "modules": {
      "mod-46-1": {
        "title": "Game Estado Architecture",
        "description": "Diseno game estado y turn logic con deterministic transitions, storage efficiency, y anti-cheat validation boundaries.",
        "lessons": {
          "lesson-46-1-1": {
            "title": "En-Chain Game Diseno",
            "content": "En-chain game diseno en Solana is a sistemas-engineering tradeoff between fairness, responsiveness, y cost. El best disenos keep critical rules verifiable while minimizing expensive estado writes.\n\nCore architecture decisions:\n1) que estado must be en-chain para trust,\n2) que can remain off-chain para speed,\n3) como turn validity is enforced deterministically.\n\nTurn-based mechanics should usa explicit estado transitions y guard checks (current actor, phase, cooldown, resource limits). If transitions are ambiguous, replay y dispute resolution become difficult.\n\nEstado compression y compact encoding matter because game loops can generate many updates. Efficient schemas reduce rent y compute pressure while preserving auditability.\n\nA produccion game modelo should also define anti-cheat boundaries. Even con deterministic logic, you need clear validation para illegal actions, stale turns, y duplicate submissions.\n\nConfiable game infrastructure is measured por predictable outcomes under stress: same input actions, same resulting estado, clear reject reasons para invalid actions.",
            "duration": "45 min"
          },
          "lesson-46-1-2": {
            "title": "Turn Manager Desafio",
            "content": "Implement turn-based game mechanics con action validation.",
            "duration": "45 min"
          },
          "lesson-46-1-3": {
            "title": "Game Estado Compressor Desafio",
            "content": "Compress game estado para efficient en-chain storage.",
            "duration": "45 min"
          },
          "lesson-46-1-4": {
            "title": "Player Progression Tracker Desafio",
            "content": "Track player experience, levels, y achievements.",
            "duration": "45 min"
          }
        }
      },
      "mod-46-2": {
        "title": "Randomness y Fairness",
        "description": "Implement fairness-oriented randomness y integrity controls ese keep gameplay auditable y dispute-resistant.",
        "lessons": {
          "lesson-46-2-1": {
            "title": "En-Chain Randomness",
            "content": "Randomness is one de el hardest fairness problems en blockchain games because execution is deterministic. Robusto disenos avoid naive pseudo-randomness tied directly a manipulable context.\n\nPractico fairness patrones include commit-reveal, VRF-backed randomness, y delayed-seed schemes. Each has latencia/trust tradeoffs:\n- commit-reveal: simple y transparent, but requires multi-step interaction,\n- VRF: stronger unpredictability, but introduces oracle/dependency considerations,\n- delayed-seed methods: lower overhead but weaker guarantees under adversarial pressure.\n\nFairness engineering should specify:\n1) who can influence randomness inputs,\n2) when values become immutable,\n3) como unresolved rounds are handled en timeout.\n\nProduccion sistemas should emit deterministic round evidence (commit hash, reveal value, validation result) so disputes can be resolved quickly.\n\nGame fairness is credible when randomness mechanisms are explicit, verifiable, y resilient a timing manipulation.",
            "duration": "45 min"
          },
          "lesson-46-2-2": {
            "title": "Commit-Reveal Desafio",
            "content": "Implement commit-reveal scheme para fair randomness.",
            "duration": "45 min"
          },
          "lesson-46-2-3": {
            "title": "Dice Roller Desafio",
            "content": "Implement verifiable dice rolling con randomness.",
            "duration": "45 min"
          },
          "lesson-46-2-4": {
            "title": "Loot Table Desafio",
            "content": "Implement weighted loot tables para game rewards.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-permanent-storage": {
    "title": "Permanent Storage y Arweave",
    "description": "Integrate permanent decentralized storage con Solana usando Arweave-style workflows: content addressing, manifest integrity, y verifiable long-term datos access.",
    "duration": "4 weeks",
    "tags": [
      "storage",
      "arweave",
      "permanent",
      "bundling",
      "manifest"
    ],
    "modules": {
      "mod-47-1": {
        "title": "Arweave Fundamentals",
        "description": "Understand permanent-storage architecture y construir deterministic linking between Solana estado y external immutable content.",
        "lessons": {
          "lesson-47-1-1": {
            "title": "Permanent Storage Architecture",
            "content": "Permanent storage integration is a datos durability contract. En Solana, storing full content en-chain is often impractical, so sistemas rely en immutable external storage references anchored por en-chain metadata.\n\nA robusto architecture defines:\n1) canonical content identifiers,\n2) integrity verification method,\n3) fallback retrieval behavior,\n4) lifecycle policy para metadata updates.\n\nContent-addressed diseno is critical. If identifiers are not tied a content hash semantics, integrity guarantees weaken y replayed/wrong assets can be served.\n\nStorage integration should also separate control-plane y datos-plane concerns: en-chain records govern ownership/version pointers, while external storage handles large payload persistence.\n\nProduccion confiabilidad requires deterministic verification reportes (ID format validity, expected hash match, availability checks). Este makes failures diagnosable y prevents silent corruption.\n\nPermanent storage sistemas succeed when users can independently verify ese referenced content matches que gobernanza o protocol estado claims.",
            "duration": "45 min"
          },
          "lesson-47-1-2": {
            "title": "Transaccion ID Validador Desafio",
            "content": "Validate Arweave transaccion IDs y URLs.",
            "duration": "45 min"
          },
          "lesson-47-1-3": {
            "title": "Storage Cost Estimator Desafio",
            "content": "Estimate Arweave storage costs based en datos size.",
            "duration": "45 min"
          },
          "lesson-47-1-4": {
            "title": "Bundle Optimizer Desafio",
            "content": "Optimize datos bundling para efficient Arweave uploads.",
            "duration": "45 min"
          }
        }
      },
      "mod-47-2": {
        "title": "Manifests y Verification",
        "description": "Work con manifests, verification pipelines, y cost/rendimiento controls para confiable long-lived datos serving.",
        "lessons": {
          "lesson-47-2-1": {
            "title": "Arweave Manifests",
            "content": "Manifests turn many stored assets into one navigable root, but they introduce their own integrity responsibilities. A manifest is only trustworthy if path mapping y referenced content IDs are validated consistently.\n\nKey safeguards:\n- deterministic path normalization,\n- duplicate/ambiguous key rejection,\n- strict transaccion-ID validation,\n- recursive integrity checks para referenced content.\n\nManifest herramientas should produce auditable outputs: resolved entries count, missing references, y hash verification status por path.\n\nDesde an operational standpoint, cost optimizacion should not compromise integrity. Bundling strategies, compression, y metadata minimization are useful only if verification remains straightforward y deterministic.\n\nWell-run permanent-storage pipelines treat manifests as governed artifacts con versioned schema expectations y repeatable validation en CI.",
            "duration": "45 min"
          },
          "lesson-47-2-2": {
            "title": "Manifest Construirer Desafio",
            "content": "Construir y parse Arweave manifests.",
            "duration": "45 min"
          },
          "lesson-47-2-3": {
            "title": "Datos Verifier Desafio",
            "content": "Verify datos integrity y availability en Arweave.",
            "duration": "45 min"
          },
          "lesson-47-2-4": {
            "title": "Storage Indexer Desafio",
            "content": "Index y query stored datos por tags.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-staking-economics": {
    "title": "Staking y Validador Economics",
    "description": "Understand Solana staking y validador economics para real-world decision-making: delegation strategy, reward dynamics, commission effects, y operational sustainability.",
    "duration": "5 weeks",
    "tags": [
      "staking",
      "validator",
      "delegation",
      "rewards",
      "economics"
    ],
    "modules": {
      "mod-48-1": {
        "title": "Staking Fundamentals",
        "description": "Aprende native staking mechanics con deterministic reward modeling, validador selection criteria, y delegation riesgo framing.",
        "lessons": {
          "lesson-48-1-1": {
            "title": "Solana Staking Architecture",
            "content": "Solana staking economics is an incentives sistema connecting delegators, validadors, y network seguridad. Good delegation decisions require more than chasing headline APY.\n\nDelegators should evaluate:\n1) validador rendimiento consistency,\n2) commission policy y changes over time,\n3) uptime y vote behavior,\n4) concentration riesgo across el validador set.\n\nReward modeling should be deterministic y transparent. Calculations must scomo gross rewards, commission effects, y net delegator outcome under explicit assumptions.\n\nDiversification matters. Concentrating stake purely en top performers can increase ecosystem centralization riesgo even if short-term yield appears higher.\n\nProduccion staking herramientas should expose scenario analysis (commission changes, rendimiento drops, epoch variance) so delegators can make resilient choices rather than reactive moves.\n\nStaking calidad is measured por sustainable net returns plus contribution a healthy validador distribution.",
            "duration": "45 min"
          },
          "lesson-48-1-2": {
            "title": "Staking Rewards Calculator Desafio",
            "content": "Calculate staking rewards con commission y inflation.",
            "duration": "45 min"
          },
          "lesson-48-1-3": {
            "title": "Validador Selector Desafio",
            "content": "Select validadors based en rendimiento y commission.",
            "duration": "45 min"
          },
          "lesson-48-1-4": {
            "title": "Stake Rebalancing Desafio",
            "content": "Optimize stake distribution across validadors.",
            "duration": "45 min"
          }
        }
      },
      "mod-48-2": {
        "title": "Validador Operations",
        "description": "Analyze validador-side economics, operational cost pressure, y incentive alignment para long-term network health.",
        "lessons": {
          "lesson-48-2-1": {
            "title": "Validador Economics",
            "content": "Validador economics balances revenue opportunities against operational costs y confiabilidad obligations. Sustainable validadors optimize para long-term trust, not short-term extraction.\n\nRevenue sources include inflation rewards y comision-related earnings; cost structure includes hardware, networking, maintenance, y operational staffing.\n\nKey operational metrics para validador viability:\n- effective uptime y vote success,\n- commission competitiveness,\n- stake retention trend,\n- incident frequency y recovery calidad.\n\nCommission strategy should be explicit y predictable. Sudden commission spikes can damage delegator trust y long-term stake stability.\n\nEconomic analysis should include downside modeling: reduced stake, higher incident costs, o prolonged rendimiento degradation.\n\nHealthy validador economics supports network resilience. Herramientas should help operators y delegators reason about sustainability, not just peak-period earnings.",
            "duration": "45 min"
          },
          "lesson-48-2-2": {
            "title": "Validador Profit Calculator Desafio",
            "content": "Calculate validador profitability.",
            "duration": "45 min"
          },
          "lesson-48-2-3": {
            "title": "Epoch Schedule Calculator Desafio",
            "content": "Calculate epoch timing y reward distribution schedules.",
            "duration": "45 min"
          },
          "lesson-48-2-4": {
            "title": "Stake Cuenta Manager Desafio",
            "content": "Manage stake cuenta lifecycle including activation y deactivation.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-account-abstraction": {
    "title": "Cuenta Abstraction y Smart Carteras",
    "description": "Implement smart-cartera/cuenta-abstraction patrones en Solana con programmable authorization, recovery controls, y policy-driven transaccion validation.",
    "duration": "6 weeks",
    "tags": [
      "account-abstraction",
      "smart-wallet",
      "multisig",
      "recovery",
      "session-keys"
    ],
    "modules": {
      "mod-49-1": {
        "title": "Smart Cartera Fundamentals",
        "description": "Construir smart-cartera fundamentals including multisig y social-recovery disenos con clear trust y failure boundaries.",
        "lessons": {
          "lesson-49-1-1": {
            "title": "Cuenta Abstraction en Solana",
            "content": "Cuenta abstraction en Solana shifts control desde a single key a programmable policy. Smart carteras can enforce richer authorization logic, but policy complexity must be managed carefully.\n\nA robusto smart-cartera diseno defines:\n1) authority modelo (owners/guardians/delegates),\n2) policy scope (que can be approved automatically vs manually),\n3) recovery path (como access is restored safely).\n\nMultisig y social recovery are powerful, but both need deterministic estado transitions y explicit quorum rules. Ambiguous transitions create lockout o unauthorized-access riesgo.\n\nSmart-cartera sistemas should emit structured authorization evidence para each action: which policy matched, which signers approved, y which constraints passed.\n\nProduccion confiabilidad depends en clear emergency controls: pause paths, guardian rotation, y recovery cooldowns.\n\nCuenta abstraction is successful when flexibility increases seguridad y usability together, not when policy logic becomes opaque.",
            "duration": "45 min"
          },
          "lesson-49-1-2": {
            "title": "Multi-Signature Cartera Desafio",
            "content": "Implement M-de-N multi-signature cartera.",
            "duration": "45 min"
          },
          "lesson-49-1-3": {
            "title": "Social Recovery Desafio",
            "content": "Implement social recovery con guardians.",
            "duration": "45 min"
          },
          "lesson-49-1-4": {
            "title": "Session Key Manager Desafio",
            "content": "Manage temporary session keys con limited permissions.",
            "duration": "45 min"
          }
        }
      },
      "mod-49-2": {
        "title": "Programmable Validation",
        "description": "Implement programmable validation policies (limits, allowlists, time/riesgo rules) con deterministic enforcement y auditability.",
        "lessons": {
          "lesson-49-2-1": {
            "title": "Custom Validation Rules",
            "content": "Programmable validation is where smart carteras deliver real value, but it is also where subtle policy bugs appear.\n\nTypical controls include spending limits, destination allowlists, time windows, y riesgo-score gates. Estos controls should be deterministic y composable, con explicit precedence rules.\n\nDiseno principles:\n- fail closed en ambiguous policy matches,\n- keep policy evaluation order stable,\n- attach machine-readable reason codes a approve/reject outcomes.\n\nValidation sistemas should also support policy explainability. Users y auditors need a understand por que a transaccion was blocked o approved.\n\nPara produccion despliegues, policy changes should be versioned y test-fixtured. A new rule must be validated against prior known-good scenarios a avoid accidental lockouts o bypasses.\n\nProgrammable carteras are strongest when validation logic is transparent, testable, y operationally reversible.",
            "duration": "45 min"
          },
          "lesson-49-2-2": {
            "title": "Spending Limit Enforcer Desafio",
            "content": "Enforce daily y per-transaccion spending limits.",
            "duration": "45 min"
          },
          "lesson-49-2-3": {
            "title": "Whitelist Enforcer Desafio",
            "content": "Enforce destination whitelists para transaccions.",
            "duration": "45 min"
          },
          "lesson-49-2-4": {
            "title": "Time Lock Enforcer Desafio",
            "content": "Enforce time-based restrictions en transaccions.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-pda-mastery": {
    "title": "Direccion Derivada de Programa Mastery",
    "description": "Master avanzado PDA engineering en Solana: seed schema diseno, bump handling discipline, y seguro cross-program PDA usage at produccion scale.",
    "duration": "6 weeks",
    "tags": [
      "pda",
      "program-derived-address",
      "seeds",
      "bump",
      "deterministic"
    ],
    "modules": {
      "mod-50-1": {
        "title": "PDA Fundamentals",
        "description": "Construir strong PDA foundations con deterministic derivation, canonical seed composition, y collision-resistant namespace strategy.",
        "lessons": {
          "lesson-50-1-1": {
            "title": "Direcciones Derivadas de Programa",
            "content": "Direcciones Derivadas de Programa (PDAs) are deterministic authority y estado anchors en Solana. Their power comes desde predictable derivation; their riesgo comes desde inconsistent seed discipline.\n\nA strong PDA diseno standard defines:\n1) canonical seed order,\n2) explicit namespace/domain tags,\n3) bump handling rules,\n4) versioning strategy para future evolution.\n\nSeed ambiguity is a common source de bugs. If different handlers derive el same concept con different seed ordering, identity checks become inconsistent y seguridad assumptions break.\n\nPDA validation should always re-derive expected addresses en el trusted side y compare exact keys before mutating estado.\n\nProduccion teams should document seed recipes as API contracts. Changing recipes sin migration planning can orphan estado y break clients.\n\nPDA mastery is mostly discipline: deterministic derivation everywhere, no implicit conventions, no trust en client-provided derivation claims.",
            "duration": "45 min"
          },
          "lesson-50-1-2": {
            "title": "PDA Generator Desafio",
            "content": "Implement PDA generation con seed validation.",
            "duration": "45 min"
          },
          "lesson-50-1-3": {
            "title": "Seed Composer Desafio",
            "content": "Compose complex seed patrones para different usa cases.",
            "duration": "45 min"
          },
          "lesson-50-1-4": {
            "title": "Bump Manager Desafio",
            "content": "Manage bump seeds para PDA verification y signing.",
            "duration": "45 min"
          }
        }
      },
      "mod-50-2": {
        "title": "Avanzado PDA Patrones",
        "description": "Implement avanzado PDA patrones (nested/counter/estadoful) while preserving seguridad invariants y migration seguridad.",
        "lessons": {
          "lesson-50-2-1": {
            "title": "PDA Diseno Patrones",
            "content": "Avanzado PDA patrones solve real scaling y composability needs but increase diseno complexity.\n\nNested PDAs, counter-based PDAs, y multi-tenant PDA namespaces each require explicit invariants around uniqueness, lifecycle, y authority boundaries.\n\nKey safeguards:\n- monotonic counters con replay protection,\n- collision checks en shared namespaces,\n- explicit ownership checks en all derived-estado paths,\n- deterministic migration paths when schema/seed versions evolve.\n\nCross-program PDA interactions must minimize signer scope. invoke_signed should only grant exactly que downstream steps require.\n\nOperationally, avanzado PDA sistemas need deterministic audit artifacts: derivation inputs, expected outputs, y validation results por instruccion path.\n\nComplex PDA architecture is safe when derivation logic remains simple a reason about y impossible a interpret ambiguously.",
            "duration": "45 min"
          },
          "lesson-50-2-2": {
            "title": "Nested PDA Generator Desafio",
            "content": "Generate PDAs derived desde other PDA addresses.",
            "duration": "45 min"
          },
          "lesson-50-2-3": {
            "title": "Counter PDA Generator Desafio",
            "content": "Generate unique PDAs usando incrementing counters.",
            "duration": "45 min"
          },
          "lesson-50-2-4": {
            "title": "PDA Collision Detector Desafio",
            "content": "Detect y prevent PDA seed collisions.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-economics": {
    "title": "Solana Economics y Token Flujos",
    "description": "Analyze Solana economic dynamics en produccion context: inflation/comision-burn interplay, staking flujos, supply movement, y protocol sustainability tradeoffs.",
    "duration": "5 weeks",
    "tags": [
      "economics",
      "inflation",
      "fees",
      "rent",
      "token-flows",
      "sustainability"
    ],
    "modules": {
      "mod-51-1": {
        "title": "Solana Economic Modelo",
        "description": "Understand Solana macro token economics (inflation, burn, rewards, comisiones) con deterministic scenario modeling.",
        "lessons": {
          "lesson-51-1-1": {
            "title": "Solana Token Economics",
            "content": "Solana economics is el interaction de issuance, burn, staking rewards, y usage demand. Sustainable protocol decisions require comprension estos flujos as a sistema, not isolated metrics.\n\nCore mechanisms include:\n1) inflation schedule y long-term emission behavior,\n2) comision burn y validador reward pathways,\n3) staking participation effects en circulating supply.\n\nEconomic analysis should be scenario-driven. Single-point estimates hide riesgo. Teams should modelo calm/high-usage/low-usage regimes y compare supply-pressure outcomes.\n\nDeterministic calculators are useful para gobernanza y product planning because they make assumptions explicit: epoch cadence, comision volume, staking ratio, y unlock schedules.\n\nHealthy economic reasoning links network-level flujos a protocol-level choices (treasury policy, incentive emissions, comision strategy).\n\nEconomic calidad improves when teams publish assumption-driven reportes instead de headline narratives.",
            "duration": "45 min"
          },
          "lesson-51-1-2": {
            "title": "Inflation Calculator Desafio",
            "content": "Calculate inflation rate y staking rewards over time.",
            "duration": "45 min"
          },
          "lesson-51-1-3": {
            "title": "Comision Burn Calculator Desafio",
            "content": "Calculate comision burns y their deflationary impact.",
            "duration": "45 min"
          },
          "lesson-51-1-4": {
            "title": "Rent Economics Calculator Desafio",
            "content": "Calculate rent costs y exemption thresholds.",
            "duration": "45 min"
          }
        }
      },
      "mod-51-2": {
        "title": "Token Flujo Analysis",
        "description": "Modelo token flujo dynamics y sustainability signals usando supply categories, unlock events, y behavior-driven liquidez effects.",
        "lessons": {
          "lesson-51-2-1": {
            "title": "Token Flujo Dynamics",
            "content": "Token flujo analysis turns abstract economics into operational insight. El key is a track where tokens are (staked, circulating, locked, treasury, pending unlock) y como they move over time.\n\nUseful flujo metrics include:\n- net circulating change,\n- staking inflow/outflow trend,\n- unlock cliff concentration,\n- treasury spend velocity.\n\nUnlock events should be modeled para market-impact riesgo. Large clustered unlocks can create short-term supply shock even when long-term tokenomics is sound.\n\nFlujo herramientas should support deterministic category cuentaing y conservation checks (total categorized supply consistency).\n\nPara gobernanza, flujo analysis is most valuable when tied a policy actions: adjust emissions, change vesting cadence, alter incentive programs.\n\nSustainable token sistemas are not static disenos; they are continuously monitored flujo sistemas con explicit policy comisiondback loops.",
            "duration": "45 min"
          },
          "lesson-51-2-2": {
            "title": "Supply Flujo Tracker Desafio",
            "content": "Track token supply categories y flujos.",
            "duration": "45 min"
          },
          "lesson-51-2-3": {
            "title": "Vesting Schedule Impact Desafio",
            "content": "Calculate token unlock impact en supply.",
            "duration": "45 min"
          },
          "lesson-51-2-4": {
            "title": "Protocol Sustainability Score Desafio",
            "content": "Calculate sustainability metrics para tokenomics.",
            "duration": "45 min"
          }
        }
      }
    }
  }
};

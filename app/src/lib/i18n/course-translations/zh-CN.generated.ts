import type { CourseTranslationMap } from "./types";

export const zhCnGeneratedCourseTranslations: CourseTranslationMap = {
  "solana-fundamentals": {
    "title": "Solana Fundamentals",
    "description": "Production-grade introduction 用于 beginners who want clear Solana 思维模型s, stronger 交易 debugging skills, 和 deterministic 钱包-manager workflows.",
    "duration": "8 hours",
    "tags": [
      "solana",
      "fundamentals",
      "accounts",
      "transactions",
      "pdas",
      "spl-token"
    ],
    "modules": {
      "module-getting-started": {
        "title": "快速入门",
        "description": "Core execution model, 账户 semantics, 和 交易 construction patterns you need before writing programs or complex clients.",
        "lessons": {
          "solana-mental-model": {
            "title": "Solana 思维模型",
            "content": "# Solana 思维模型\n\nSolana development gets much easier once you stop thinking in terms of \"contracts that own state\" 和 start thinking in terms of \"programs that operate on 账户.\" On Solana, the durable state of your app does not live inside executable code. It lives in 账户, 和 every 指令 explicitly says which 账户 it wants to read or write. Programs are stateless logic: they validate inputs, apply rules, 和 update 账户 data when authorized.\n\nA 交易 is a signed message containing one or more ordered 指令. Each 指令 names a target program, the 账户 it needs, 和 serialized data. The runtime processes those 指令 in order, 和 the 交易 is atomic: either all 指令 succeed, or none are committed. This matters 用于 correctness. If your second 指令 depends on the first 指令's output, 交易 atomicity guarantees you never end up in a half-applied state.\n\n用于 execution validity, several fields matter together: a fee payer, a recent blockhash, 指令 payloads, 和 all required signatures. The fee payer funds 交易 fees. The recent blockhash gives the message a freshness window, preventing replay of old signed messages. Required signatures prove authorization from signers declared by 指令 账户 metadata. Missing or invalid signatures cause rejection before 指令 logic runs.\n\nSolana's parallelism comes from 账户 access metadata. Because each 指令 lists read 和 write 账户 up front, the runtime can schedule non-conflicting 交易 simultaneously. If two 交易 only read the same 账户, they can run in parallel. If they both write the same 账户, one must wait. This read/write locking model is a core reason Solana can scale while preserving deterministic outcomes.\n\nWhen reading chain state, you'll also see commitment levels: processed, confirmed, 和 finalized. Conceptually, processed means observed quickly, confirmed means voted by the cluster, 和 finalized means rooted deeply enough that rollback risk is minimal. Treat commitment as a consistency/latency trade-off knob, not a fixed-time guarantee.\n",
            "duration": "35 min"
          },
          "accounts-model-deep-dive": {
            "title": "账户 model 深入解析",
            "content": "# 账户 model 深入解析\n\nEvery on-chain object on Solana is an 账户 使用 a standard envelope. You can reason about any 账户 using a small set of fields: address, lamports, owner, executable flag, 和 data bytes length/content. Address (a public key) identifies the 账户. Lamports represent native SOL balance in the smallest unit (1 SOL = 1,000,000,000 lamports). Owner is the program allowed to modify 账户 data 和 debit lamports according to runtime rules. Executable indicates whether the 账户 stores runnable program code. Data length tells you how many bytes are allocated 用于 state.\n\nSystem 钱包 账户 are usually owned by the System Program 和 often have `dataLen = 0`. Program 账户 are executable 和 typically owned by loader/runtime programs, not by your application directly. Token balances do not live directly in 钱包 账户. SPL tokens use dedicated token 账户, each tied to a specific mint 和 owner, because token state has its own program-defined layout 和 rules.\n\nRent-exemption is the 实战 baseline 用于 persistent storage. The more bytes an 账户 allocates, the higher the minimum lamports needed to keep it alive without rent collection risk. Even if you never inspect binary data manually, 账户 size still affects user costs 和 protocol economics. Good schema 设计 means allocating only what you need 和 planning upgrades carefully.\n\nOwner semantics are 安全-critical. If an 账户 claims to be token state but is not owned by the token program, your app should reject it. If an 账户 is executable, treat it as code, not mutable application data. If you understand owner + executable + data length, you can classify most 账户 types quickly 和 avoid many integration mistakes.\n\nThe fastest way to build confidence is to inspect concrete 账户 examples 和 explain what each field implies operationally.\n",
            "duration": "40 min"
          },
          "transactions-and-instructions": {
            "title": "交易 & 指令",
            "content": "# 交易 & 指令\n\nAn 指令 is the smallest executable unit on Solana: `programId + account metas + opaque data bytes`. A 交易 wraps one or more 指令 plus signatures 和 message metadata. This 设计 gives you composability 和 atomicity in one envelope.\n\nThink of 指令 账户 as an explicit dependency graph. Each 账户 meta marks whether the 账户 is writable 和 whether a signature is required. During 交易 execution, the runtime uses those flags 用于 access checks 和 lock scheduling. If your 指令 tries to mutate an 账户 not marked writable, it fails. If a required signer did not sign, it fails before your program logic runs.\n\nThe 交易 message also carries fee payer 和 recent blockhash. Fee payer is straightforward: who funds execution. Recent blockhash is subtler: it anchors freshness. Signed messages are replay-resistant because old blockhashes expire. This is why 交易 builders usually fetch a fresh blockhash close to send time.\n\n指令 ordering is deterministic 和 significant. If 指令 B depends on 账户 changes from 指令 A, place A first. If any 指令 fails, the whole 交易 is rolled back. You should 设计 multi-step flows 使用 this all-or-nothing behavior in mind.\n\n用于 CLI workflow, a healthy baseline is: inspect config, target the right cluster, verify active 钱包, 和 check balance before sending anything. That sequence reduces avoidable errors 和 improves team reproducibility. In local scripts, log your derived addresses 和 交易 summaries so teammates can reason about intent 和 outcomes.\n\nYou do not need RPC calls to understand this model, but you do need rigor in message construction: explicit 账户, explicit ordering, explicit signatures, 和 explicit freshness.\n\n## Why this matters in real apps\n\nWhen production incidents happen, teams usually debug 交易 construction first: wrong signer, wrong writable flag, stale blockhash, or wrong 指令 ordering. Engineers who model 交易 as explicit data structures can diagnose these failures quickly. Engineers who treat 交易 like opaque 钱包 blobs usually need trial-和-error.\n\n## What you should be able to do after this 课时\n\n- Explain the difference between 指令-level validation 和 交易-level validation.\n- Predict when two 交易 can execute in parallel 和 when they will conflict.\n- Build a deterministic pre-send checklist 用于 local scripts 和 frontend clients.\n",
            "duration": "35 min"
          },
          "build-sol-transfer-transaction": {
            "title": "Build a SOL transfer 交易",
            "content": "# Build a SOL transfer 交易\n\nImplement a deterministic `buildTransferTx(params)` helper in the project file:\n\n- `src/lib/courses/solana-fundamentals/project/walletManager.ts`\n- Use `@solana/web3.js`\n- Return a 交易 使用 exactly one `SystemProgram.transfer` 指令\n- Set `feePayer` 和 `recentBlockhash` from params\n- No network calls\n\nThis in-page challenge validates your object-shape reasoning. The authoritative checks 用于 课时 4 run in repository unit tests, so keep your project implementation aligned 使用 those tests.\n",
            "duration": "35 min"
          }
        }
      },
      "module-programs-and-pdas": {
        "title": "Programs & PDAs",
        "description": "Program behavior, deterministic PDA 设计, 和 SPL token 思维模型s 使用 实战 safety checks.",
        "lessons": {
          "programs-what-they-are": {
            "title": "Programs: what they are (和 aren’t)",
            "content": "# Programs: what they are (和 aren’t)\n\nA Solana program is executable 账户 code, not an object that secretly owns mutable storage. Your program receives 账户 from the 交易, verifies constraints, 和 writes only to 账户 it is authorized to modify. This explicitness is a feature: it keeps data dependencies visible 和 helps the runtime parallelize safely.\n\nProgram 账户 are marked executable 和 deployed through loader programs. Upgrades are governed by upgrade authority (when configured), which is why production 治理 around authority custody matters. If your protocol says it is immutable, users should be able to verify upgrade authority was revoked.\n\nWhat programs are not: they are not ambient state scanners. A program cannot discover arbitrary chain data by itself at runtime. If an 账户 is required, it must be passed in the 指令 账户 list. This is a foundational 安全 和 性能 constraint. It prevents hidden state dependencies 和 makes execution deterministic from the message alone.\n\n跨程序调用 (CPI) is how one program composes 使用 another. During CPI, your program calls into another program, passing 账户 metas 和 指令 data. This enables rich composition: token transfers from your app logic, metadata updates, or protocol-to-protocol operations. But CPI also increases failure surface. You must validate assumptions before 和 after CPI, 和 you must track which signer 和 writable privileges are being forwarded.\n\nAt a high level, a robust Solana program follows a pattern: validate signer/owner/seed constraints, deserialize 账户 data, enforce business invariants, perform state transitions, 和 optionally perform CPI calls. Keeping this pipeline explicit makes audits easier 和 upgrades safer.\n\nThe 实战 takeaway: programs are deterministic policy engines over 账户. If you keep 账户 boundaries clear, many 安全 和 correctness questions become mechanical rather than mystical.\n",
            "duration": "35 min"
          },
          "program-derived-addresses-pdas": {
            "title": "程序派生地址 (PDAs)",
            "content": "# 程序派生地址 (PDAs)\n\nA 程序派生地址 (PDA) is a deterministic 账户 address derived from seeds plus a program ID, 使用 one key property: it is intentionally off-curve, so no private key exists 用于 it. This lets your program control addresses deterministically without requiring a human-held signer.\n\nDerivation starts 使用 seed bytes. Seeds can encode user IDs, mint addresses, version tags, 和 other namespace components. The runtime appends a bump seed when needed 和 searches 用于 an off-curve output. The bump is an integer that makes derivation succeed while preserving deterministic reproducibility.\n\nWhy PDAs matter: they make address calculation stable across clients 和 on-chain logic. If both sides derive the same PDA from the same seed recipe, they can verify identity without extra lookup tables. This powers patterns like per-user state 账户, escrow vaults, 和 protocol configuration 账户.\n\nVerification is straightforward 和 critical. Off-chain clients derive PDA 和 include it in 指令. On-chain programs derive the expected PDA again 和 compare against the supplied 账户 key. If mismatch, reject. This closes an entire class of 账户-substitution attacks.\n\nWho signs 用于 a PDA? Not a 钱包. The program can authorize as PDA during CPI by using invoke_signed 使用 the exact seed set 和 bump. Conceptually, runtime verifies the derivation proof 和 grants signer semantics to that PDA 用于 the invoked 指令.\n\nChanging any seed value changes the derived PDA. This is both feature 和 footgun: excellent 用于 namespacing, dangerous if you accidentally alter seed encoding rules between versions. Keep seed schemas explicit, versioned, 和 documented.\n\nIn short: PDAs are deterministic, non-keypair addresses that let programs model authority 和 state structure cleanly.\n",
            "duration": "40 min"
          },
          "spl-token-basics": {
            "title": "SPL Tokens 基础",
            "content": "# SPL Tokens 基础\n\nSPL Token is Solana’s standard token program family 用于 fungible assets. A token mint 账户 defines token-level configuration: decimals, total supply accounting, 和 authorities such as mint authority or freeze authority. A mint does not store each user’s balance directly. Balances live in token 账户.\n\nAssociated Token 账户 (ATAs) are the default token-账户 convention: one canonical token 账户 per (owner, mint) pair. This convention simplifies UX 和 interoperability because 钱包 和 protocols can derive the expected 账户 location without extra indexing.\n\nA common 初级 mistake is treating 钱包 addresses as token balance containers. Native SOL lives on system 账户, but SPL token balances live on token 账户 owned by the token program. That means transfers move balances between token 账户, not directly from 钱包 pubkey to 钱包 pubkey.\n\nAuthority 设计 matters. Mint authority controls token issuance. Freeze authority can halt movement in specific designs. Removing or 治理-wrapping authorities is a major trust signal in production deployments. If authority policies are unclear, integration risk rises quickly.\n\nThe token model also supports extension pathways. Token-2022 introduces optional features such as transfer fees 和 additional metadata/behavior controls. You do not need Token-2022 to understand fundamentals, but you should know it exists so you can avoid assuming every token mint behaves exactly like legacy SPL Token defaults.\n\nOperationally, safe token logic means: verify mint, verify owner program, verify ATA derivation where expected, 和 reason about authorities before trusting balances or transfer permissions.\n\nOnce you internalize mint vs token-账户 separation 和 authority boundaries, most SPL token flows become predictable 和 debuggable.\n",
            "duration": "40 min"
          },
          "wallet-manager-cli-sim": {
            "title": "钱包 Manager CLI-sim",
            "content": "# 钱包 Manager CLI-sim\n\nImplement a deterministic CLI parser + command executor in:\n\n- `src/lib/courses/solana-fundamentals/project/walletManager.ts`\n\nRequired behavior:\n\n- `address` prints the active pubkey\n- `build-transfer --to <PUBKEY> --sol <AMOUNT> --blockhash <BH>` prints stable JSON:\n  `{ from, to, lamports, feePayer, recentBlockhash, instructionProgramId }`\n\nNo network calls. Keep key order stable in output JSON. Repository tests validate this 课时's deterministic behavior.\n",
            "duration": "35 min"
          }
        }
      }
    }
  },
  "anchor-development": {
    "title": "Anchor Development",
    "description": "Project-journey 课程 用于 developers moving from 基础 to real Anchor engineering: deterministic 账户模型ing, 指令 builders, 测试 discipline, 和 reliable client UX.",
    "duration": "10 hours",
    "tags": [
      "anchor",
      "solana",
      "pda",
      "accounts",
      "testing",
      "counter"
    ],
    "modules": {
      "anchor-v2-module-basics": {
        "title": "Anchor 基础",
        "description": "Anchor architecture, 账户 constraints, 和 PDA foundations 使用 explicit ownership of 安全-critical decisions.",
        "lessons": {
          "anchor-mental-model": {
            "title": "Anchor 思维模型",
            "content": "# Anchor 思维模型\n\nAnchor is best understood as a contract between three layers that must agree on shape: your Rust handlers, generated interface metadata (IDL), 和 client-side 指令 builders. In raw Solana programs you manually decode bytes, manually validate 账户, 和 manually return compact error numbers. Anchor keeps the same runtime model but moves repetitive work into declarations. You still define 安全-critical behavior, yet you do it through explicit 账户 structs, constraints, 和 typed 指令 arguments.\n\nThe `#[program]` 模块 is where 指令 handlers live. Each function gets a typed `Context<T>` plus explicit arguments. The corresponding `#[derive(Accounts)]` struct tells Anchor exactly what 账户 must be provided 和 what checks happen before handler logic executes. This includes signer requirements, mutability, PDA seed verification, ownership checks, 和 relational checks like `has_one`. If validation fails, the 交易 aborts before touching your business logic.\n\nIDL is the bridge that makes the developer experience consistent across Rust 和 TypeScript. It describes 指令 names, args, 账户, events, 和 custom errors. Clients can generate typed methods from that shape, reducing drift between frontend code 和 on-chain interfaces. When teams ship fast, drift is a common failure mode: wrong 账户 ordering, stale discriminators, or stale arg names. IDL-driven clients make those mistakes less likely.\n\nProvider 和 钱包 concepts complete the flow. The provider wraps an RPC connection plus signer abstraction 和 commitment preferences. It does not replace 钱包 安全, but it centralizes 交易 send/confirm behavior 和 test setup. In practice, production reliability comes from understanding this boundary: Anchor helps 使用 ergonomics 和 consistency, but you still own protocol invariants, 账户 设计, 和 threat modeling.\n\n用于 this 课程, treat Anchor as a typed 指令 framework on top of Solana’s explicit 账户 runtime. That framing lets you reason clearly about what is generated, what remains your responsibility, 和 how to test deterministic pieces without needing devnet in CI.\n\n## What Anchor gives you vs what it does not\n\nAnchor gives you: typed 账户 contexts, standardized serialization, structured errors, 和 IDL-driven client ergonomics. Anchor does not give you: automatic business safety, correct authority 设计, or threat modeling. Those are still protocol engineering decisions.\n\n## By the end of this 课时\n\n- You can explain the Rust handler -> IDL -> client flow without hand-waving.\n- You can identify which checks belong in 账户 constraints versus handler logic.\n- You can debug IDL drift issues (wrong 账户 order, stale args, stale client bindings).\n",
            "duration": "40 min"
          },
          "anchor-accounts-constraints-and-safety": {
            "title": "账户, constraints, 和 safety",
            "content": "# 账户, constraints, 和 safety\n\nMost serious Solana vulnerabilities come from 账户 validation mistakes, not from arithmetic. Anchor’s constraint system exists to turn those checks into declarative, auditable rules. You declare intent in the 账户 context, 和 the framework enforces it before 指令 logic runs. This means your handlers can focus on state transitions while constraints guard the perimeter.\n\nStart 使用 core markers: `Signer<'info>` proves signature authority, 和 `#[account(mut)]` declares state can change. Forgetting `mut` produces runtime failures because Solana locks 账户 writability up front. This is not cosmetic metadata; it is part of execution scheduling 和 safety. Then ownership checks ensure an 账户 belongs to the expected program. If a malicious user passes an 账户 that has the same bytes but wrong owner, strong ownership constraints stop 账户 substitution attacks.\n\nPDA constraints 使用 `seeds` 和 `bump` verify deterministic 账户 identity. Instead of trusting a user-provided address, you define the derivation recipe 和 compare runtime inputs against it. This pattern prevents attackers from redirecting logic to arbitrary writable 账户. `has_one` links 账户 relationships, such as enforcing `counter.authority == authority.key()`. That relation check is simple but high leverage: it prevents privileged actions from being executed by unrelated signers.\n\nAnchor also supports custom `constraint = ...` expressions 用于 protocol invariants, like minimum collateral or authority domain rules. Use these sparingly but deliberately: put invariant checks near 账户 definitions when they are structural, 和 keep business flow checks in handlers when they depend on 指令 arguments or prior state.\n\nA 实战 review checklist: verify every mutable 账户 has an explicit reason to be mutable; verify every signer is necessary; verify every PDA seed recipe is stable 和 versioned; verify ownership checks are present where parsing assumes specific layout; verify relational constraints (`has_one`) 用于 privileged paths. 安全 here is explicitness. Constraints do not remove responsibility, but they make responsibility visible 和 testable.\n",
            "duration": "45 min"
          },
          "anchor-pdas-in-practice": {
            "title": "PDAs in Anchor",
            "content": "# PDAs in Anchor\n\n程序派生地址 are the backbone of predictable 账户 topology in Anchor applications. A PDA is derived from seed bytes plus program ID 和 intentionally lives off the ed25519 curve, so no private key exists 用于 it. This lets your program control authority 用于 deterministic addresses through `invoke_signed` semantics while keeping user keypairs out of the trust path.\n\nIn Anchor, PDA derivation logic appears in 账户 constraints. Typical patterns look like `seeds = [b\"counter\", authority.key().as_ref()], bump`. This expresses three things at once: namespace (`counter`), ownership relation (authority), 和 uniqueness under your program ID. The `bump` value is the extra byte required to land off-curve. You can compute it on demand 使用 Anchor, or store it in 账户 state 用于 future CPI convenience.\n\nShould you store bump or always re-derive? Re-deriving keeps state smaller 和 avoids stale bump fields if derivation recipes ever evolve. Storing bump can simplify downstream 指令 construction 和 reduce repeated derivation cost. In practice, many production programs store bump when they expect frequent PDA signing calls 和 keep the seed recipe immutable. Whichever path you choose, document it 和 test it.\n\nSeed schema discipline matters. If you silently change seed ordering, text encoding, or domain tags, you derive different addresses 和 break 账户 continuity. Teams usually treat seeds as protocol versioned API: include explicit namespace tags, stable byte encoding rules, 和 migration plans when evolution is unavoidable.\n\n用于 this project journey, we will derive a counter PDA from authority + static domain seed 和 use that address in both init 和 increment 指令 builders. The goal is to make 账户 identity deterministic, inspectable, 和 testable without network dependencies. You can then layer real 交易 sending later, confident that 账户 和 data layouts are already correct.\n",
            "duration": "40 min"
          },
          "anchor-counter-init-deterministic": {
            "title": "Initialize Counter PDA (deterministic)",
            "content": "# Initialize Counter PDA (deterministic)\n\nImplement deterministic helper functions 用于 a Counter project:\n\n- `deriveCounterPda(programId, authorityPubkey)`\n- `buildInitCounterIx(params)`\n\nThis 课时 validates client-side reasoning without RPC calls. Your output must include stable PDA + bump shape, key signer/writable metadata, 和 deterministic init 指令 bytes.\n\nNotes:\n- Keep 账户 key ordering stable.\n- Use the fixed init discriminator bytes from the 课时 hints.\n- Return deterministic JSON in `run(input)` so tests can compare exact output.\n",
            "duration": "35 min"
          }
        }
      },
      "anchor-v2-module-pdas-accounts-testing": {
        "title": "PDAs, 账户, 和 测试",
        "description": "Deterministic 指令 builders, stable state emulation, 和 测试 strategy that separates pure logic from network integration.",
        "lessons": {
          "anchor-increment-builder-and-emulator": {
            "title": "Increment 指令 builder + state layout",
            "content": "# Increment 指令 builder + state layout\n\nImplement deterministic increment behavior in pure TypeScript:\n\n- Build a reusable state representation 用于 counter data.\n- Implement `applyIncrement` as a pure transition function.\n- Enforce explicit overflow behavior (`Counter overflow` error).\n\nThis challenge focuses on deterministic correctness of state transitions, not network execution.\n",
            "duration": "35 min"
          },
          "anchor-testing-without-flakiness": {
            "title": "测试 strategy without flakiness",
            "content": "# 测试 strategy without flakiness\n\nA reliable Solana curriculum should teach deterministic engineering first, then optional network integration. Flaky tests are usually caused by external dependencies: RPC latency, faucet limits, cluster state drift, blockhash expiry, 和 钱包 setup mismatch. These are real operational concerns, but they should not block learning core protocol logic.\n\n用于 Anchor projects, split 测试 into layers. Unit tests validate data layout, discriminator bytes, PDA derivation, 账户 key ordering, 和 指令 payload encoding. These tests are fast 和 deterministic. They can run in CI without 验证者 or internet. If they fail, the error usually points to a real bug in serialization or 账户 metadata.\n\nIntegration tests add runtime behavior: 交易 simulation, 账户 creation, CPI paths, 和 event assertions. These are valuable but more fragile. Keep them focused 和 avoid making every PR depend on remote cluster health. Use local 验证者 or controlled environment when possible, 和 treat external devnet tests as optional confidence checks rather than gatekeeping checks.\n\nWhen writing deterministic tests, prefer explicit expected values 和 fixed key ordering. 用于 example, assert exact JSON output 使用 stable key order 用于 summaries, assert exact byte arrays 用于 指令 discriminators, 和 assert exact signer/writable flags in 账户 metas. These checks catch regressions that broad snapshot tests can miss.\n\nAlso test failure paths intentionally: overflow behavior, invalid pubkeys, wrong argument shapes, 和 stale 账户 discriminators. Production incidents often happen on edge paths that had no tests.\n\nA 实战 rule: unit tests should prove your client 和 serialization logic are correct independent of chain conditions. Integration tests should prove network workflows behave when environment is healthy. Keeping this boundary clear gives you both speed 和 confidence.\n",
            "duration": "35 min"
          },
          "anchor-client-composition-and-ux": {
            "title": "Client composition & UX",
            "content": "# Client composition & UX\n\nOnce 指令 layouts 和 PDA logic are deterministic, client integration becomes a composition exercise: 钱包 adapter 用于 signing, provider/connection 用于 transport, 交易 builder 用于 指令 packing, 和 UI state 用于 pending/success/error handling. Anchor helps by keeping 账户 schemas 和 指令 names aligned via IDL, but robust UX still depends on clear boundaries.\n\nA typical flow is: derive addresses, build 指令, create 交易, set fee payer 和 recent blockhash, request 钱包 signature, send raw 交易, then confirm 使用 chosen commitment. Each stage can fail 用于 different reasons. If your UI collapses all failures into one generic message, users cannot recover 和 developers cannot debug quickly.\n\nSimulation failures usually mean 账户 metadata mismatch, invalid 指令 data, missing signer, wrong owner program, or constraint violations. Signature errors indicate 钱包/user path issues. Blockhash errors are freshness issues. Insufficient funds often involve fee payer SOL balance, not just business 账户 balances. Mapping these classes to actionable errors improves trust 和 reduces support load.\n\nFee payer deserves explicit UX. The user may authorize a 交易 but still fail because payer lacks lamports 用于 fees or rent. Surfacing fee payer 和 estimated cost before signing avoids confusion. 用于 multi-party flows, make fee policy explicit.\n\n用于 this 课程 project, we keep deterministic logic in pure helpers 和 treat network send/confirm as optional outer layer. That architecture gives you stable local tests while still enabling production integration later. If a network call fails, you can quickly isolate whether the bug is in deterministic 指令 construction or in runtime environment state.\n\nIn short: robust Anchor UX is not one API call. It is a staged pipeline 使用 clear error taxonomy, explicit payer semantics, 和 deterministic inner logic that can be tested without chain access.\n",
            "duration": "40 min"
          },
          "anchor-counter-project-checkpoint": {
            "title": "Counter project checkpoint",
            "content": "# Counter project checkpoint\n\nCompose the full deterministic flow:\n\n1. Derive counter PDA from authority + program ID.\n2. Build init 指令 metadata.\n3. Build increment 指令 metadata.\n4. Emulate state transitions: `init -> increment -> increment`.\n5. Return stable JSON summary in exact key order:\n\n`{ authority, pda, initIxProgramId, initKeys, incrementKeys, finalCount }`\n\nNo network calls. All checks are strict string matches.\n",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-frontend": {
    "title": "Solana Frontend Development",
    "description": "Project-journey 课程 用于 frontend engineers who want production-ready Solana dashboards: deterministic reducers, replayable event pipelines, 和 trustworthy 交易 UX.",
    "duration": "10 hours",
    "tags": [
      "frontend",
      "dashboard",
      "state-model",
      "event-replay",
      "determinism"
    ],
    "modules": {
      "frontend-v2-module-fundamentals": {
        "title": "Frontend Fundamentals 用于 Solana",
        "description": "Model 钱包/账户 state correctly, 设计 交易 lifecycle UX, 和 enforce deterministic correctness rules 用于 replayable debugging.",
        "lessons": {
          "frontend-v2-wallet-state-accounts-model": {
            "title": "钱包 state + 账户 思维模型 用于 UI devs",
            "content": "# 钱包 state + 账户 思维模型 用于 UI devs\n\nMost Solana frontend bugs are not visual bugs. They are model bugs. A dashboard can look polished while silently computing balances from the wrong 账户 class, mixing lamports 使用 token units, or treating temporary pending state as confirmed truth. The first production-grade skill is to build a strict 思维模型 和 enforce it in code. 钱包 address, system 账户 balance, token 账户 balance, 和 position value are related but not interchangeable.\n\nA connected 钱包 gives your app identity 和 signature capability. It does not directly provide full portfolio state. Native SOL lives on the 钱包 system 账户 in lamports, while SPL balances live in token 账户, often associated token 账户 (ATAs). If your state shape does not represent this distinction explicitly, downstream logic becomes fragile. 用于 example, transfer previews might show a 钱包 address as a token destination, but execution requires token 账户 addresses. Good frontends represent these as separate types 和 derive display labels from those types.\n\nPrecision is equally important. Lamports 和 token amounts should remain integer strings in your model layer. UI formatting can convert those values 用于 display, but business logic should avoid float math to prevent drift 和 non-deterministic tests. This 课程 uses deterministic fixtures 和 fixed-scale arithmetic because reproducibility is essential 用于 debugging. If one engineer sees \\\"5.000001\\\" 和 another sees \\\"5.000000\\\" 用于 the same payload, your incident response becomes noise.\n\nState ownership is another common failure point. Portfolio views often merge data from event streams, cached fetches, 和 optimistic 交易 journals. Without clear precedence rules, you can double-count transfers or overwrite fresh data 使用 stale cache entries. A robust model treats each input as an event 和 computes derived state through deterministic reducers. That approach gives you replayability: when a bug appears, you can replay the exact event sequence 和 inspect every transition.\n\nA production dashboard also needs explicit error classes 用于 parsing 和 modeling. Invalid mint metadata, malformed amount strings, or missing ATA links should produce typed failures, not silent fallback behavior. Silent fallbacks feel user-friendly in the short term, but they hide corruption that later appears as impossible balances or broken transfers.\n\nFinally, 钱包 state should include confidence metadata. Is this balance from confirmed events? From optimistic local prediction? From replay snapshot N? Confidence-aware UX prevents overclaiming 和 helps users understand why values may shift.\n\n## 实战 思维模型 map\n\nKeep four layers explicit:\n1. Identity layer (钱包, signer, session metadata).\n2. State layer (system 账户, token 账户, mint metadata).\n3. Event layer (journal entries, corrections, dedupe keys, confidence).\n4. View layer (formatted balances, sorted rows, UX status labels).\n\nWhen these layers blur together, bugs look random. When they stay separate, you can isolate failures quickly.\n\n## Pitfall: treating 钱包 pubkey as the universal balance location\n\n钱包 pubkey identifies a user, but SPL balances live in token 账户. If you collapse the two, transfer builders, explorers, 和 reconciliation logic diverge.\n\n## Production Checklist\n\n1. Keep lamports 和 token amounts as integer strings in core model.\n2. Represent 钱包 address, ATA address, 和 mint address as separate fields.\n3. Derive UI values from deterministic reducers, not ad-hoc component state.\n4. Attach confidence metadata to displayed balances.\n5. Emit typed parser/model errors instead of silent defaults.\n",
            "duration": "45 min"
          },
          "frontend-v2-transaction-lifecycle-ui": {
            "title": "交易 lifecycle 用于 UI: pending/confirmed/finalized, optimistic UI",
            "content": "# 交易 lifecycle 用于 UI: pending/confirmed/finalized, optimistic UI\n\nFrontend 交易 UX is a state machine problem. Users press one button, but your app traverses multiple phases: intent creation, 交易 construction, signature request, submission, 和 confirmation at one or more commitment levels. If these phases are collapsed into one boolean \\\"loading\\\" flag, you lose correctness 和 your recovery paths become guesswork.\n\nThe lifecycle starts 使用 deterministic planning. Before any 钱包 popup, construct a serializable 交易 intent: 账户, amounts, expected side effects, 和 idempotency key. This intent should be inspectable 和 testable without network access. In production, this split pays off because many failures happen before send: invalid 账户 metas, stale assumptions about ATAs, wrong decimals, or malformed 指令 payloads. A deterministic planner catches these early 和 produces actionable errors.\n\nAfter signing, submission moves the 交易 into a pending state. Pending means the network may or may not accept execution. Your UI can use optimistic overlays, but optimistic updates should be scoped 和 reversible. 用于 example, show \\\"pending transfer\\\" in activity feed immediately, but avoid mutating durable balance totals until at least confirmed commitment. If you mutate balances too early, user trust drops when signature rejection or simulation failure occurs.\n\nCommitment levels should be modeled explicitly. \\\"processed\\\" provides quick feedback, \\\"confirmed\\\" provides stronger confidence, 和 \\\"finalized\\\" is strongest. You do not need to promise exact timing. You do need to communicate confidence boundaries clearly. A common production bug is labeling processed as final 和 then rendering inconsistent data during cluster stress.\n\nOptimistic rollback is often neglected. Every optimistic action needs a rollback rule keyed by idempotency token. If confirmation fails, rollback should remove optimistic journal entries 和 restore derived state by replaying deterministic events. This is why event-driven state models are 实战 用于 frontend apps: they make rollback a replay operation instead of imperative patchwork.\n\nTelemetry should also be phase-specific. Log whether failures happen in build, sign, send, or confirm. Group by 钱包 type, program ID, 和 error class. This lets teams distinguish infrastructure incidents from modeling bugs.\n\n## Pitfall: over-writing confirmed state 使用 stale optimistic assumptions\n\nOptimistic state should be additive 和 reversible. If optimistic patches directly replace canonical state, delayed confirmations or failures create confusing balance jumps.\n\n## Production Checklist\n\n1. Model 交易 lifecycle as explicit states, not one loading flag.\n2. Keep deterministic planner output separate from 钱包/RPC adapter layer.\n3. Track optimistic entries 使用 idempotency keys 和 rollback rules.\n4. Label commitment confidence in UI copy.\n5. Emit phase-specific telemetry 用于 build/sign/send/confirm.\n",
            "duration": "45 min"
          },
          "frontend-v2-data-correctness-idempotency": {
            "title": "Data correctness: dedupe, ordering, idempotency, correction events",
            "content": "# Data correctness: dedupe, ordering, idempotency, correction events\n\nFrontend teams frequently assume event streams are perfectly ordered 和 unique. Production systems rarely behave that way. You can receive duplicate events, out-of-order events, delayed price updates, 和 correction signals that invalidate earlier records. If your reducer assumes ideal sequencing, dashboard totals drift 和 support incidents become hard to reproduce.\n\nDeterministic ordering is the first control. In this 课程 we replay events by (ts, id). Timestamp alone is insufficient because equal timestamps are common in batched systems. A deterministic tie-breaker gives every engineer 和 CI runner the same final state.\n\nIdempotency is the second control. Each event id should be applied at most once. If the same id appears twice, state must not change after the first apply. This rule protects against retries, websocket reconnect bursts, 和 duplicate queue deliveries.\n\nCorrection handling is the third control. A correction event references an earlier event id 和 signals that its effect should be removed. You can implement this by replaying from journal 使用 corrected ids excluded, or by inverse operations when your model supports exact inverses. Replay is slower but simpler 和 safer 用于 educational deterministic engines.\n\nHistory modeling deserves attention too. Users need recent activity, but history should not become an unstructured debug dump. Each history row should include event id, timestamp, type, 和 concise summary. If corrected events remain visible, label them explicitly so users 和 support staff understand why balances changed.\n\nAnother correctness risk is cross-domain ordering. Token events 和 price events may arrive at different rates. Value calculations should depend on the latest known price per mint 和 should never use transient float conversions. Fixed-scale integer math avoids rounding divergence across environments.\n\nWhen reducers are deterministic 和 replayable, regression 测试 improves dramatically. You can compare snapshots after every N events, compute checksums, 和 verify that refactors preserve behavior. This style catches subtle bugs earlier than end-to-end tests.\n\nFinally, correctness is not only code. It is product communication. If corrections can alter history, UI should surface that possibility in copy 和 state labels. Hiding it creates the appearance of randomness.\n\n## Pitfall: applying out-of-order events directly to live state without replay\n\nApplying arrivals as-is can produce transiently wrong balances 和 non-reproducible bugs. Deterministic replay gives consistent outcomes 和 auditable transitions.\n\n## Production Checklist\n\n1. Sort replay by deterministic keys (ts, id).\n2. Deduplicate by event id before applying transitions.\n3. Support correction events that remove prior effects.\n4. Keep history rows explicit 和 correction-aware.\n5. Use fixed-scale arithmetic 用于 value calculations.\n",
            "duration": "45 min"
          },
          "frontend-v2-core-reducer": {
            "title": "Build core state model + reducer from events",
            "content": "# Build core state model + reducer from events\n\nImplement a deterministic reducer 用于 dashboard state:\n- apply event stream transitions 用于 balances 和 mint metadata\n- enforce idempotency by event id\n- support correction markers 用于 replaced events\n- emit stable history summaries\n",
            "duration": "35 min"
          }
        }
      },
      "frontend-v2-module-token-dashboard": {
        "title": "Token Dashboard Project",
        "description": "Build reducer, replay snapshots, query metrics, 和 deterministic dashboard outputs that remain stable under partial or delayed data.",
        "lessons": {
          "frontend-v2-stream-replay-snapshots": {
            "title": "Implement event stream simulator + replay timeline + snapshots",
            "content": "# Implement event stream simulator + replay timeline + snapshots\n\nBuild deterministic replay tooling:\n- replay sorted events by (ts, id)\n- snapshot every N applied events\n- compute stable checksum 用于 replay output\n- return { finalState, snapshots, checksum }\n",
            "duration": "35 min"
          },
          "frontend-v2-query-layer-metrics": {
            "title": "Implement query layer + computed metrics",
            "content": "# Implement query layer + computed metrics\n\nImplement dashboard query/view logic:\n- search/filter/sort rows deterministically\n- compute total 和 row valueUsd 使用 fixed-scale integer math\n- expose stable view model 用于 UI rendering\n",
            "duration": "35 min"
          },
          "frontend-v2-production-ux-hardening": {
            "title": "Production UX: caching, pagination, error banners, skeletons, rate limits",
            "content": "# Production UX: caching, pagination, error banners, skeletons, rate limits\n\nAfter model correctness, frontend quality is mostly about user trust under imperfect conditions. Users do not evaluate your dashboard by clean demo paths. They evaluate it when data is delayed, partial, duplicated, or rejected. Production UX hardening means making those states understandable 和 recoverable.\n\nCaching strategy should be explicit. Event snapshots, derived views, 和 summary cards should have clear freshness rules. A stale-but-marked cache is often better than blank loading screens, but stale data must never masquerade as confirmed current data. Include freshness timestamps 和, when possible, source confidence labels (cached, replayed, confirmed).\n\nPagination 和 virtualized lists need deterministic sorting to avoid row jumps between pages. If sort keys are unstable, users see items move unexpectedly as new events arrive. Use primary 和 secondary stable keys, 和 preserve cursor semantics during live updates.\n\nError banners should be scoped by subsystem. Parser errors are not network errors. Replay checksum mismatches are not 钱包 signature errors. Distinct error classes reduce panic 和 help users choose next actions. A generic red toast that says \\\"something went wrong\\\" is operationally expensive.\n\nSkeleton states must communicate structure rather than fake certainty. Show placeholder rows 和 chart bounds, but avoid hardcoding values that look real. If users screen-record issues, misleading skeletons complicate incident investigation.\n\nRate limits are common in real dashboards, even 使用 private APIs. Your UI should surface backoff state 和 avoid firehose re-requests from multiple components. Centralize data fetching 和 de-duplicate in-flight requests by key. This prevents self-inflicted throttling.\n\nLive mode 和 replay mode should share the same reducer 和 query pipeline. Live mode streams events progressively; replay mode applies fixture timelines deterministically. If these modes use different code paths, bugs hide in mode-specific branches 和 become hard to reproduce.\n\nA 实战 approach is to store event journal 和 snapshots, then render all UI from derived selectors. This architecture supports recoverability: you can reset to snapshot N, replay events, 和 inspect differences. It also supports support tooling: attach snapshot checksum 和 model version to error reports.\n\n### Devnet Bonus (optional)\n\nYou can add an RPC adapter behind a feature flag 和 map live 账户 updates into the same event format. Keep this optional 和 never required 用于 core correctness.\n\n## Pitfall: shipping polished visuals 使用 unscoped failure states\n\nIf users cannot tell whether an issue is stale cache, parse failure, or upstream throttle, confidence erodes even when core model logic is correct.\n\n## Production Checklist\n\n1. Expose freshness metadata 用于 cached 和 live data.\n2. Keep list sorting deterministic across pagination.\n3. Classify errors by subsystem 使用 actionable copy.\n4. De-duplicate in-flight fetches 和 respect rate limits.\n5. Render live 和 replay modes through shared reducer/selectors.\n",
            "duration": "45 min"
          },
          "frontend-v2-dashboard-summary-checkpoint": {
            "title": "Emit stable DashboardSummary from fixtures",
            "content": "# Emit stable DashboardSummary from fixtures\n\nCompose deterministic checkpoint output:\n- owner, token count, totalValueUsd\n- top tokens sorted deterministically\n- recent activity rows\n- invariants 和 determinism metadata (fixture hash + model version)\n",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "defi-solana": {
    "title": "DeFi on Solana",
    "description": "高级 project-journey 课程 用于 engineers building swap systems: deterministic offline Jupiter-style planning, route ranking, minOut safety, 和 reproducible diagnostics.",
    "duration": "12 hours",
    "tags": [
      "defi",
      "swap",
      "routing",
      "jupiter",
      "offline",
      "deterministic"
    ],
    "modules": {
      "defi-v2-module-swap-fundamentals": {
        "title": "Swap Fundamentals",
        "description": "Understand CPMM math, quote anatomy, 和 deterministic routing tradeoffs 使用 safety-first user protections.",
        "lessons": {
          "defi-v2-amm-basics-fees-slippage-impact": {
            "title": "AMM 基础 on Solana: pools, fees, slippage, 和 价格影响",
            "content": "# AMM 基础 on Solana: pools, fees, slippage, 和 价格影响\n\nWhen users click “Swap,” they usually assume there is one objective truth: the current price. In practice, frontend swap systems compute an estimate from pool reserves 和 route assumptions. The estimate can be excellent, but it is still a model. DeFi UI quality depends on how honestly 和 consistently that model is represented.\n\nIn a constant-product AMM, each pool maintains an invariant close to x * y = k. A swap changes reserves asymmetrically, 和 the output amount is non-linear relative to input size. Small trades can track spot estimates closely, while larger trades move further along the curve 和 experience more impact. That non-linearity is why frontend code must never compare routes using only “price per token” labels. You need route-aware output calculations at the target trade size.\n\nOn Solana, swaps also occur across varied pool designs 和 fee tiers. Some pools are deep 和 low fee; others are shallow but still attractive 用于 small size due to path composition. Fee bps are often compared in isolation, but total execution quality comes from three interacting pieces: fee deduction, reserve depth, 和 route hop count. A route 使用 slightly higher fee can still produce higher net output if reserves are materially deeper.\n\nSlippage 和 价格影响 are often conflated in UI copy, but they answer different questions. 价格影响 asks: what movement does this trade itself induce against current reserves? Slippage tolerance asks: what worst-case output should still be accepted at execution time? One is descriptive of current route mechanics, the other is a user safety bound. Production interfaces should surface both values clearly 和 compute minOut deterministically from outAmount 和 slippage bps.\n\nDeterministic arithmetic matters as much as financial logic. If planners use floating-point shortcuts, two environments can produce subtly different minOut values 和 route ranking. Those tiny differences create major operational pain in tests, incident response, 和 support reproductions. Integer arithmetic over u64-style amount strings should remain the primary model path; formatting 用于 users should happen only at presentation boundaries.\n\nEven in an offline educational planner, safety invariants belong at the core. Outputs must never exceed reserveOut. Reserves must remain non-negative after virtual simulation. Missing pools should fail fast 使用 typed errors, not fallback behavior. These checks mirror production expectations 和 train the same engineering discipline needed 用于 real integrations.\n\nA robust frontend 思维模型 is therefore: token universe + pool universe + deterministic quote math + route ranking policy + user safety constraints. If any layer is implicit, the system will still run, but behavior under volatility becomes hard to explain. If all layers are explicit 和 typed, the same planner can power UI previews, tests, 和 diagnostics 使用 minimal drift.\n\n## Quick numeric intuition\n\nIf two routes have spot prices that look similar, a larger input can still produce materially different output because you travel further on each curve. That is why route comparison must happen at the exact user amount, not a tiny reference trade.\n\n## What you should internalize from this 课时\n\n- Execution quality is output-at-size, not headline spot price.\n- Slippage tolerance is a user protection bound, not a market forecast.\n- Deterministic integer math is a product feature, not only a technical preference.\n\n### Pitfalls\n\n1. Comparing routes by headline “price” instead of exact outAmount at the user’s size.\n2. Treating slippage tolerance as if it were the same metric as 价格影响.\n3. Using floating point in route ranking or minOut logic.\n\n### Production Checklist\n\n1. Keep amount math in integer-safe paths.\n2. Surface outAmount, fee impact, 和 minOut separately.\n3. Enforce invariant checks 用于 each hop simulation.\n4. Keep route ranking deterministic 使用 explicit tie-breakers.\n5. Log enough context to reproduce route decisions.\n",
            "duration": "50 min"
          },
          "defi-v2-quote-anatomy-and-risk": {
            "title": "Quote anatomy: in/out, fees, minOut, 和 worst-case execution",
            "content": "# Quote anatomy: in/out, fees, minOut, 和 worst-case execution\n\nA production quote is not one number. It is a structured object that must tell users what they send, what they likely receive, how much they pay in fees, 和 what minimum output protection applies. When frontend systems treat quote payloads as loose JSON blobs, users lose trust quickly because route changes 和 execution deviations look arbitrary.\n\nThe first mandatory fields are inAmount 和 outAmount in raw integer units. Without raw values, deterministic checks become fragile. UI formatting should be derived from token decimals, but core state should retain raw strings 用于 exact comparisons 和 invariant logic. If an app compares rounded display numbers, route ties can break unpredictably.\n\nSecond, quote systems should expose fee breakdown per hop. Aggregate fee bps is useful, but it hides which pools drive cost. 用于 route explainability 和 debugging, users 和 engineers need pool-level fee contributions. This is particularly important 用于 two-hop routes where one leg may be cheap 和 the other expensive.\n\nThird, minOut must be explicit, reproducible, 和 tied to user-configured slippage bps. The computation is deterministic: floor(outAmount * (10000 - slippageBps) / 10000). Showing this value is not optional 用于 serious UX. It is the user’s principal safety guard against stale quotes 和 rapid market movement between quote 和 submission.\n\nFourth, quote freshness 和 worst-case framing should be visible. Even in offline training systems, the planner should model the idea that the route is valid 用于 a moment, not forever. In production, stale quote handling 和 forced re-quote boundaries prevent accidental execution 使用 outdated assumptions.\n\nA useful engineering pattern is to model quote objects as immutable snapshots. Each snapshot includes selected route, per-hop details, total fees, impact estimate, 和 minOut. If selection changes, produce a new snapshot instead of mutating fields in place. This gives deterministic audit trails 和 cleaner state transitions.\n\n用于 this 课程, 课时 logic remains offline 和 deterministic, but the same 设计 prepares teams 用于 real Jupiter integrations later. By the time network adapters are introduced, your model 和 tests already guarantee stable route math 和 explainability.\n\nQuote anatomy also influences support burden. When a user asks why they received less than expected, the answer is much faster if the system preserves route path, slippage setting, 和 minOut from the exact planning state. Without that, teams rely on post-hoc guesses.\n\n### Pitfalls\n\n1. Displaying outAmount without minOut 和 route-level fees.\n2. Mutating selected quote objects in place instead of creating snapshots.\n3. Computing fee percentages from rounded UI values instead of raw amounts.\n\n### Production Checklist\n\n1. Keep quote payloads immutable 和 versioned.\n2. Store per-hop fee contributions 和 total fee amount.\n3. Compute 和 show minOut from explicit slippage bps.\n4. Preserve raw amounts 和 decimals separately.\n5. Expose route freshness metadata in UI state.\n",
            "duration": "50 min"
          },
          "defi-v2-routing-fragmentation-two-hop": {
            "title": "Routing: why two-hop can beat one-hop",
            "content": "# Routing: why two-hop can beat one-hop\n\nUsers often assume direct pair routes are always best because they are simpler. In fragmented liquidity systems, that assumption fails frequently. A direct SOL -> JUP pool might have shallow depth, while SOL -> USDC 和 USDC -> JUP pools together can produce better net output despite two fees 和 two curve traversals. A production router should evaluate both one-hop 和 two-hop candidates 和 rank them deterministically.\n\nThe engineering challenge is not just finding paths. It is comparing paths under consistent assumptions. Every candidate should be quoted 使用 the same input amount, same deterministic arithmetic, 和 same fee/impact accounting. If one path uses rounded display math while another uses raw amounts, route ranking loses meaning.\n\nTwo-hop routing also requires stable tie-break policies. Suppose two candidates produce equal outAmount at integer precision. One has one hop; the other has two hops. A deterministic system should prefer fewer hops. If hop count also ties, lexicographic route ID ordering can resolve final rank. The exact policy can vary, but it must be explicit 和 stable.\n\nLiquidity fragmentation introduces another subtle point: 中级 mint risk. A two-hop path through a highly liquid stable pair can be excellent, but if the second pool is thin, the route can still degrade at larger sizes. This is why route scoring should be quote-size aware 和 not reused blindly across different trade amounts.\n\n用于 offline 课程 logic, we model pools as a static universe 和 simulate reserves virtually per quote path. Even this simplified model teaches key production habits: avoid mutating source fixtures, isolate simulation state per candidate, 和 validate safety constraints at each hop.\n\nRouting quality is also a UX problem. If a selected route changes due to input edits or quote refresh, users should see why: outAmount delta, fee change, 和 path change. Silent route switching feels suspicious even when mathematically correct.\n\nIn larger systems, routers may consider split routes, gas/compute constraints, or venue reliability. This 课程 intentionally limits scope to one-hop 和 two-hop deterministic candidates so core reasoning remains clear 和 testable.\n\nFrom an implementation perspective, route objects should be treated as typed artifacts 使用 stable IDs 和 explicit hop metadata. That discipline reduces accidental coupling between UI state 和 planner internals. When engineers can serialize a route candidate, replay it 使用 the same input, 和 get the same result, incident response becomes straightforward.\n\n### Pitfalls\n\n1. Assuming direct pairs always outperform multi-hop routes.\n2. Reusing quotes computed 用于 one trade size at another size.\n3. Non-deterministic tie-breaking that causes route flicker.\n\n### Production Checklist\n\n1. Enumerate one-hop 和 two-hop routes systematically.\n2. Quote every candidate 使用 the same deterministic math path.\n3. Keep tie-break policy explicit 和 stable.\n4. Simulate virtual reserves without mutating source fixtures.\n5. Surface route-change reasons in UI.\n",
            "duration": "50 min"
          }
        }
      },
      "defi-v2-module-offline-jupiter-planner": {
        "title": "Jupiter-Style Swap Planner Project (Offline)",
        "description": "Build deterministic quoting, route selection, 和 minOut safety checks, then package stable checkpoint artifacts 用于 reproducible reviews.",
        "lessons": {
          "defi-v2-quote-cpmm": {
            "title": "Implement token/pool model + constant-product quote calc",
            "content": "# Implement token/pool model + constant-product quote calc\n\nImplement deterministic CPMM quoting:\n- out = (reserveOut * inAfterFee) / (reserveIn + inAfterFee)\n- fee = floor(inAmount * feeBps / 10000)\n- impactBps from spot vs effective execution price\n- return outAmount, feeAmount, inAfterFee, impactBps\n",
            "duration": "35 min"
          },
          "defi-v2-router-best": {
            "title": "Implement route enumeration 和 best-route selection",
            "content": "# Implement route enumeration 和 best-route selection\n\nImplement deterministic route planner:\n- enumerate one-hop 和 two-hop candidates\n- quote each candidate at exact input size\n- select best route using stable tie-breakers\n",
            "duration": "35 min"
          },
          "defi-v2-safety-minout": {
            "title": "Implement slippage/minOut, fee breakdown, 和 safety invariants",
            "content": "# Implement slippage/minOut, fee breakdown, 和 safety invariants\n\nImplement deterministic safety layer:\n- apply slippage to compute minOut\n- simulate route 使用 virtual reserve updates\n- return structured errors 用于 invalid pools/routes\n- enforce non-negative reserve 和 bounded output invariants\n",
            "duration": "35 min"
          },
          "defi-v2-production-swap-ux": {
            "title": "Production swap UX: stale quotes, protection, 和 simulation",
            "content": "# Production swap UX: stale quotes, protection, 和 simulation\n\nA deterministic route engine is necessary but not sufficient 用于 production. Users experience DeFi through timing, messaging, 和 safety affordances. A mathematically correct planner can still feel broken if stale quote handling, retry behavior, 和 error communication are weak.\n\nStale quotes are the most common operational issue. In volatile markets, quote quality decays quickly. Interfaces should track quote age 和 invalidate plans beyond a strict threshold. When invalidation happens, route 和 minOut should be recomputed before submit. Reusing stale plans to “speed up” UX usually creates worse outcomes 和 support burden.\n\nUser protection should be layered. Slippage bounds protect against adverse movement, but they do not protect against malformed route payloads or mismatched 账户 assumptions. Safety validation should run before any 钱包 prompt 和 should return explicit, typed errors. “Something went wrong” is not enough in swap flows.\n\nSimulation messaging matters as much as simulation itself. If route simulation fails pre-send, users need actionable context: which hop failed, whether pool liquidity was insufficient, whether the route is missing required pools, 和 whether re-quoting could help. Generic error banners create user churn.\n\nRetry logic must be bounded 和 stateful. Blind retries 使用 unchanged input are often just repeated failures. Good UX distinguishes retryable states (temporary network issue) from deterministic planner errors (invalid route topology). 用于 deterministic planner errors, force state change before retry.\n\nAnother production concern is observability. Record route ID, inAmount, outAmount, minOut, fee totals, 和 invariant results 用于 each attempt. These logs make incident triage 和 postmortems dramatically faster. Without structured traces, teams often blame “market conditions” 用于 planner bugs.\n\nPagination 和 list updates also affect trust. Swap history UIs should preserve deterministic ordering 和 avoid jitter when data refreshes. If past swaps reorder unpredictably, users perceive reliability issues even when 交易 are correct.\n\nOptional live integrations should be feature-flagged 和 isolated. The offline deterministic engine should remain the source of truth, while live adapters map external responses into the same internal types. That boundary keeps tests stable 和 protects core behavior from third-party schema changes.\n\nFinally, production swap UX should make deterministic planner outcomes explainable to non-expert users. If a route is rejected, the interface should provide a concrete reason 和 a clear next action such as reducing size or selecting a different output token. Clear messaging converts system correctness into user trust.\n\n### Pitfalls\n\n1. Allowing stale quotes to remain actionable without forced re-quote.\n2. Retrying deterministic planner errors without changing route or inputs.\n3. Hiding failure reason details behind generic notifications.\n\n### Production Checklist\n\n1. Track quote freshness 和 invalidate aggressively.\n2. Enforce pre-submit invariant validation.\n3. Separate retryable network failures from deterministic planner failures.\n4. Log route 和 safety metadata 用于 every attempt.\n5. Keep offline engine as canonical model 用于 optional live adapters.\n",
            "duration": "50 min"
          },
          "defi-v2-checkpoint": {
            "title": "Produce stable SwapPlan + SwapSummary checkpoint",
            "content": "# Produce stable SwapPlan + SwapSummary checkpoint\n\nCompose deterministic checkpoint artifacts:\n- build swap plan from selected route quote\n- include fixtureHash 和 modelVersion\n- emit stable summary 使用 path, minOut, fee totals, impact, 和 invariants\n",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-security": {
    "title": "Solana 安全 & Auditing",
    "description": "Production-grade deterministic vuln lab 用于 Solana auditors who need repeatable exploit evidence, precise remediation guidance, 和 high-signal audit artifacts.",
    "duration": "10 hours",
    "tags": [
      "security",
      "audit",
      "vuln-lab",
      "solana"
    ],
    "modules": {
      "security-v2-threat-model-and-method": {
        "title": "Threat Model & Audit Method",
        "description": "账户-centric threat modeling, deterministic exploit reproduction, 和 evidence discipline 用于 credible audit findings.",
        "lessons": {
          "security-v2-threat-model": {
            "title": "Solana threat model 用于 auditors: 账户, owners, signers, writable, PDAs",
            "content": "# Solana threat model 用于 auditors: 账户, owners, signers, writable, PDAs\n\n安全 work on Solana starts 使用 one non-negotiable fact: 指令 callers choose the 账户 list. Programs do not receive trusted implicit context. They receive exactly the 账户 metas 和 指令 data encoded in a 交易 message. This 设计 is powerful 用于 composability 和 性能, but it means almost every critical exploit is an 账户 validation exploit in disguise. If you internalize this early, your audits become more mechanical 和 less guess-based.\n\nA good 思维模型 is to treat each 指令 as a contract boundary 使用 five mandatory validations: identity, authority, ownership, mutability, 和 derivation. Identity asks whether the supplied 账户 is the 账户 the 指令 expects. Authority asks whether the actor that is allowed to mutate state actually signed. Ownership asks whether 账户 data should be interpreted under the current program or a different one. Mutability asks whether writable access is both requested 和 justified. Derivation asks whether PDA paths are deterministic 和 verified against canonical seeds plus bump. Missing any of those layers creates openings that attackers repeatedly use.\n\nSigner checks are not optional on privileged paths. If the 指令 changes authority, moves funds, or updates risk parameters, the authority 账户 must be a signer 和 must be the expected authority from state. One common bug is checking only that “some signer exists.” That is still broken. Audits should explicitly map each privileged transition to a concrete signer relationship 和 verify that relation is enforced before state mutation.\n\nOwner checks are equally critical. Programs often parse 账户 bytes into local structs. Without owner checks, an attacker can pass arbitrary bytes that deserialize into a shape that looks valid but is controlled by another program or by no program assumptions at all. This is 账户 substitution. It is the root cause of many catastrophic incidents 和 should be surfaced early in review notes.\n\nPDA checks are where many teams lose determinism. Seed recipes need to be explicit, stable, 和 versioned. If the runtime accepts user-provided bump values without recomputation, or if seed ordering differs between handlers, spoofed addresses can pass inconsistent checks. Auditors should insist on exact re-derivation 和 equality checks in all sensitive paths.\n\nWritable flags matter 用于 two reasons: correctness 和 attack surface. Over-broad writable sets increase risk by allowing unnecessary state transitions in CPI-heavy flows. Under-declared mutability causes runtime failure, which is safer but still a reliability bug.\n\nFinally, threat modeling should include arithmetic constraints. Even if auth is correct, unchecked u64 math can corrupt balances through underflow or overflow 和 invalidate all higher-level assumptions.\n\n## Auditor workflow per 指令\n\n用于 each handler, run the same sequence: identify privileged outcome, list required 账户, verify signer/owner/PDA relationships, verify writable scope, then test malformed 账户 lists. Repeating this fixed loop prevents “I think it looks safe” audits.\n\n## What you should be able to do after this 课时\n\n- Turn a vague concern into a concrete validation checklist.\n- Explain why 账户 substitution 和 PDA spoofing recur in Solana incidents.\n- Build deterministic negative-path scenarios before writing remediation notes.\n\n## Checklist\n- Map each 指令 to a clear privilege model.\n- Verify authority 账户 is required signer 用于 privileged actions.\n- Verify authority key equality against stored state authority.\n- Verify every parsed 账户 has explicit owner validation.\n- Verify each PDA is re-derived from canonical seeds 和 bump.\n- Verify writable 账户 are minimal 和 justified.\n- Verify arithmetic uses checked operations 用于 u64 transitions.\n- Verify negative-path tests exist 用于 unauthorized 和 malformed 账户.\n\n## Red flags\n- Privileged state updates without signer checks.\n- Parsing unchecked 账户 data from unknown owners.\n- PDA acceptance based on partial seed checks.\n- Handlers that trust client-provided bump blindly.\n- Arithmetic updates using plain + 和 - on balances.\n\n## How to verify (simulator)\n- Run vulnerable mode on signer-missing scenario 和 inspect trace.\n- Re-run fixed mode 和 confirm ERR_NOT_SIGNER.\n- Execute owner-missing scenario 和 compare vulnerable vs fixed outcomes.\n- Execute pda-spoof scenario 和 confirm fixed mode emits ERR_BAD_PDA.\n- Compare trace hashes to verify deterministic event ordering.\n",
            "duration": "55 min"
          },
          "security-v2-evidence-chain": {
            "title": "Evidence chain: reproduce, trace, impact, fix, verify",
            "content": "# Evidence chain: reproduce, trace, impact, fix, verify\n\nStrong 安全 reports are built on evidence chains, not opinions. In the Solana context, that means moving from a claim such as “missing signer check exists” to a deterministic chain: reproduce exploit conditions, capture a stable execution trace, quantify impact, apply a patch, 和 verify that the same steps now fail 使用 expected error codes while invariants hold. This chain is what turns audit work into an engineering artifact.\n\nReproduction should be deterministic 和 minimal. Every scenario should declare initial 账户, authority/signer flags, vault ownership assumptions, 和 指令 inputs. If reproductions depend on external RPC timing or changing liquidity conditions, confidence drops 和 triage slows down. In this 课程 lab, scenarios are fixture-driven 和 offline so every replay produces the same state transitions.\n\nTrace capture is the core of audit evidence. Instead of recording only final balances, log each relevant event in stable order: InstructionStart, AccountRead, CheckPassed/CheckFailed, BalanceChange, InstructionEnd. These events let reviewers verify exactly which assumptions passed 和 where validation was skipped. They also help map exploitability to code-level checks. 用于 example, if signer checks are absent in vulnerable mode, the trace should explicitly show that signer validation was skipped or never evaluated.\n\nImpact analysis should be quantitative. 用于 signer 和 owner bugs, compute drained lamports or unauthorized state changes. 用于 PDA bugs, show mismatch between expected derived address 和 accepted address. 用于 arithmetic bugs, show underflow or overflow conditions 和 resulting corruption. Impact details inform severity 和 prioritization.\n\nPatch validation should not just say “fixed.” It should prove exploit steps now fail 用于 the right reason. If signer exploit now fails, error code should be ERR_NOT_SIGNER. If PDA spoof now fails, error code should be ERR_BAD_PDA. This specificity catches regressions where one bug is accidentally masked by unrelated behavior.\n\nVerification closes the chain 使用 invariant checks. Examples: vault balance remains a valid u64 string, authority remains unchanged, 和 no unauthorized lamport delta occurs in fixed mode. These invariants convert patch confidence into measurable guarantees.\n\nWhen teams do this consistently, reports become executable documentation. New engineers can replay scenarios 和 understand why controls exist. Incident response becomes faster because prior failure signatures 和 remediation patterns are already captured.\n\n## Checklist\n- Define each scenario 使用 explicit initial state 和 指令 inputs.\n- Capture deterministic, ordered trace events 用于 each run.\n- Hash traces 使用 canonical JSON 用于 reproducibility.\n- Quantify impact using before/after deltas.\n- Map each finding to explicit evidence references.\n- Re-run identical scenarios in fixed mode.\n- Verify fixed-mode failures use expected error codes.\n- Record post-fix invariant results 使用 stable IDs.\n\n## Red flags\n- Reports 使用 no reproduction steps.\n- Non-deterministic traces that change between runs.\n- Impact described qualitatively without deltas.\n- Patch claims without fixed-mode replay evidence.\n- Invariant lists omitted from verification section.\n\n## How to verify (simulator)\n- Run signer-missing in vulnerable mode, save trace hash.\n- Run same scenario in fixed mode, confirm ERR_NOT_SIGNER.\n- Run owner-missing 和 confirm ERR_BAD_OWNER in fixed mode.\n- Run pda-spoof 和 compare expected/accepted PDA fields.\n- Generate audit report JSON 和 markdown summary from checkpoint builder.\n",
            "duration": "55 min"
          },
          "security-v2-bug-classes": {
            "title": "Common Solana bug classes 和 mitigations",
            "content": "# Common Solana bug classes 和 mitigations\n\nAuditors on Solana repeatedly encounter the same core bug families. The implementation details differ across protocols, but exploit mechanics are surprisingly consistent: identity confusion, authority confusion, derivation drift, arithmetic corruption, 和 unsafe cross-program assumptions. A robust review process categorizes findings by class, applies known verification patterns, 和 tests negative paths intentionally.\n\n**Missing signer checks** are high-severity because they directly break authorization. The fix is conceptually simple: require signer 和 key relation. Yet teams miss it when refactoring 账户 structs or switching between typed 和 unchecked 账户 wrappers. Auditors should scan all state-mutating handlers 和 ask: who can call this 和 what proves authorization?\n\n**Missing owner checks** create 账户 substitution risk. Programs may deserialize 账户 bytes 和 trust semantic fields without proving the 账户 is owned by the expected program. In mixed CPI systems, this is especially dangerous because 账户 shapes can look valid while semantics differ. Mitigation is explicit owner validation before parsing 和 strict 账户 type usage.\n\n**PDA seed/bump mismatch** appears when seed ordering, domain tags, or bump handling drifts between 指令. One handler derives [\"vault\", authority], another derives [authority, \"vault\"], a third trusts client-provided bump. Attackers search those inconsistencies to route privileged logic through spoofed addresses. Mitigation is canonical seed schema, exact re-derivation on every sensitive path, 和 tests that intentionally pass malformed PDA candidates.\n\n**CPI authority confusion** happens when one program delegates authority assumptions to another without strict scope. If signer seeds or delegated permissions are broader than intended, downstream calls can perform unintended state transitions. Mitigation includes explicit CPI allowlists, minimal writable/signer metas, 和 scope-limited delegated authorities.\n\n**Integer overflow/underflow** remains a 实战 class in accounting-heavy systems. Rust release mode behavior makes unchecked arithmetic unacceptable 用于 balances 和 fee logic. Mitigation is checked operations, u128 intermediates 用于 multiply/divide paths, 和 boundary-focused tests.\n\nMitigation quality depends on verification quality. Unit tests should include adversarial 账户 substitutions, malformed seeds, missing signers, 和 boundary arithmetic. If tests only cover happy paths, high-severity bugs will survive code review.\n\nThe audit deliverable should translate classes into implementation guidance. Engineers need clear, actionable remediations 和 concrete reproduction conditions, not generic warnings. The best reports include checklists that can be wired into CI 和 release gates.\n\n## Checklist\n- Enumerate all privileged 指令 和 expected signers.\n- Verify owner checks before parsing external 账户 layouts.\n- Pin 和 document PDA seed schemas 和 bump usage.\n- Validate CPI target program IDs against allowlist.\n- Minimize writable 和 signer 账户 metas in CPI.\n- Enforce checked math 用于 all u64 state transitions.\n- Add negative tests 用于 each bug class.\n- Require deterministic traces 用于 安全-critical tests.\n\n## Red flags\n- Any privileged mutation path without explicit signer requirement.\n- Any unchecked 账户 deserialization path.\n- Any 指令 that accepts bump without re-derivation.\n- Any CPI call to dynamic or user-selected program ID.\n- Any unchecked arithmetic on balances or supply values.\n\n## How to verify (simulator)\n- Use 课时 4 scenario to confirm unauthorized withdraw in vulnerable mode.\n- Use 课时 5 scenario to confirm spoofed PDA acceptance in vulnerable mode.\n- Use 课时 6 patch suite to verify fixed-mode errors by code.\n- Run checkpoint report 和 ensure all scenarios are marked reproduced.\n- Inspect invariant result array 用于 all fixed-mode scenarios.\n",
            "duration": "55 min"
          }
        }
      },
      "security-v2-vuln-lab": {
        "title": "Vuln Lab Project Journey",
        "description": "Exploit, patch, verify, 和 produce audit-ready artifacts 使用 deterministic traces 和 invariant-backed conclusions.",
        "lessons": {
          "security-v2-exploit-signer-owner": {
            "title": "Break it: exploit missing signer + owner checks",
            "content": "# Break it: exploit missing signer + owner checks\n\nImplement a deterministic exploit-proof formatter 用于 signer/owner vulnerabilities.\n\nExpected output fields:\n- scenario\n- before/after vault balance\n- before/after recipient lamports\n- trace hash\n- explanation 使用 drained lamports\n\nUse canonical key ordering so tests can assert exact JSON output.",
            "duration": "40 min"
          },
          "security-v2-exploit-pda-spoof": {
            "title": "Break it: exploit PDA spoof mismatch",
            "content": "# Break it: exploit PDA spoof mismatch\n\nImplement a deterministic PDA spoof proof output.\n\nYou must show:\n- expected PDA\n- accepted PDA\n- mismatch boolean\n- trace hash\n\nThis 课时 validates evidence generation 用于 derivation mismatches.",
            "duration": "40 min"
          },
          "security-v2-patch-validate": {
            "title": "Fix it: validations + invariant suite",
            "content": "# Fix it: validations + invariant suite\n\nImplement patch validation output that confirms:\n- signer check\n- owner check\n- PDA check\n- safe u64 arithmetic\n- exploit blocked state 使用 error code\n\nKeep output deterministic 用于 exact assertion.",
            "duration": "45 min"
          },
          "security-v2-writing-reports": {
            "title": "Writing audit reports: severity, likelihood, blast radius, remediation",
            "content": "# Writing audit reports: severity, likelihood, blast radius, remediation\n\nA strong audit report is an engineering document, not a narrative essay. It should allow a reader to answer four questions quickly: what failed, how exploitable it is, how much damage it can cause, 和 what exact change prevents recurrence. 安全 writing quality directly affects fix quality because implementation teams ship what they can interpret.\n\nSeverity should be tied to impact 和 exploit preconditions. A missing signer check in a withdraw path is typically critical if it allows unauthorized asset movement 使用 low prerequisites. A PDA mismatch may be high or medium depending on reachable code paths 和 available controls. Severity labels without rationale are not useful. Include explicit exploit path assumptions 和 whether attacker capital or privileged positioning is required.\n\nLikelihood should capture 实战 exploitability, not theoretical possibility. 用于 example, if a bug requires impossible 账户 states under current architecture, likelihood may be low even if impact is high. Conversely, if a bug is reachable by submitting a standard 指令 使用 crafted 账户 metas, likelihood is high. Be specific.\n\nBlast radius should describe what can be drained or corrupted: one vault, one market, protocol-wide state, or 治理 authority. This framing helps teams stage incident response 和 patch rollout.\n\nRecommendations must be precise 和 testable. “Add better validation” is too vague. “Require authority signer, verify authority key matches vault state, verify vault owner equals program id, 和 verify PDA from [\"vault\", authority] + bump” is actionable. Include expected error codes so QA can validate behavior reliably.\n\nEvidence references are also important. Each finding should point to deterministic traces, scenario IDs, 和 checkpoint artifacts so another engineer can replay without interpretation gaps.\n\nFinally, include verification results. A patch is not complete until exploit scenarios fail deterministically 和 invariants hold. Reports that end before verification force downstream teams to rediscover completion criteria.\n\nReport structure should also prioritize scanability. Teams reviewing multiple findings under incident pressure need consistent field ordering 和 concise language that maps directly to engineering actions. If one finding uses narrative prose while another uses structured reproduction steps, remediation speed drops because readers spend time normalizing format instead of executing fixes.\n\nA reliable pattern is one finding per vulnerability class 使用 explicit evidence references grouped by scenario ID. That allows QA, auditors, 和 protocol engineers to coordinate on the same deterministic artifacts. The same approach also improves long-term maintenance: when code changes, teams can rerun scenario IDs 和 compare trace hashes to detect regressions in report assumptions.\n\n## Checklist\n- State explicit vulnerability class 和 affected 指令 path.\n- Include reproducible scenario ID 和 deterministic trace hash.\n- Quantify impact 使用 concrete state/balance deltas.\n- Assign severity 使用 rationale tied to exploit preconditions.\n- Assign likelihood based on realistic attacker capabilities.\n- Describe blast radius at 账户/protocol boundary.\n- Provide exact remediation steps 和 expected error codes.\n- Include verification outcomes 和 invariant results.\n\n## Red flags\n- Severity labels without impact rationale.\n- Recommendations without concrete validation rules.\n- No reproduction steps or trace references.\n- No fixed-mode verification evidence.\n- No distinction between impact 和 likelihood.\n\n## How to verify (simulator)\n- Generate report JSON from checkpoint builder.\n- Confirm findings include evidenceRefs 用于 each scenario.\n- Confirm remediation includes patch IDs.\n- Confirm verification results mark each scenario as blocked in fixed mode.\n- Generate markdown summary 和 compare to report content ordering.\n",
            "duration": "55 min"
          },
          "security-v2-audit-report-checkpoint": {
            "title": "Checkpoint: deterministic AuditReport JSON + markdown",
            "content": "# Checkpoint: deterministic AuditReport JSON + markdown\n\nCreate the final deterministic checkpoint payload:\n- 课程 + version\n- scenario IDs\n- finding count\n\nThis checkpoint mirrors the final 课程 artifact produced by the simulator report builder.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "token-engineering": {
    "title": "Token Engineering on Solana",
    "description": "Project-journey 课程 用于 teams launching real Solana tokens: deterministic Token-2022 planning, authority 设计, supply simulation, 和 operational launch discipline.",
    "duration": "10 hours",
    "tags": [
      "tokens",
      "token-2022",
      "launch",
      "authorities",
      "simulation"
    ],
    "modules": {
      "token-v2-module-fundamentals": {
        "title": "Token Fundamentals -> Token-2022",
        "description": "Understand token primitives, mint policy anatomy, 和 Token-2022 extension controls 使用 explicit 治理 和 threat-model framing.",
        "lessons": {
          "token-v2-spl-vs-token-2022": {
            "title": "SPL tokens vs Token-2022: what extensions change",
            "content": "# SPL tokens vs Token-2022: what extensions change\n\nToken engineering starts 使用 a clean boundary between base token semantics 和 configurable policy. Legacy SPL Token gives you a stable fungible primitive: mint metadata, token 账户, mint authority, freeze authority, 和 transfer/mint/burn 指令. Token-2022 preserves that core interface but adds extension slots that let teams activate richer behavior without rewriting token logic from scratch. That compatibility is useful, but it also creates a new class of 治理 和 safety decisions that frontend 和 protocol engineers need to model explicitly.\n\nThe key 思维模型: Token-2022 is not a separate economic system; it is an extended 账户 layout 和 指令 surface. Extensions are opt-in, 和 each extension adds bytes, authorities, 和 state transitions that must be considered during mint initialization 和 lifecycle management. If you treat extensions as cosmetic add-ons, you can ship a token that is technically valid but operationally fragile.\n\n用于 production teams, the first decision is policy minimalism. Every enabled extension increases complexity in 钱包, indexers, 和 downstream integrations. Transfer fees may fit treasury goals but can break assumptions in partner protocols. Default 账户 state can enforce safety posture but may confuse users if 账户 thaw flow is unclear. Permanent delegate can simplify managed flows but dramatically expands power if authority boundaries are weak. The right approach is to map each extension to a concrete requirement 和 document the explicit threat model it introduces.\n\nToken-2022 also changes launch sequencing. You must pre-size mint 账户 用于 chosen extensions, initialize extension data in deterministic order, 和 verify authority alignment before live distribution. This is where deterministic offline planning is valuable: you can generate a launch pack, inspect 指令-like payloads, 和 validate invariants before touching network systems. That practice catches configuration drift early 和 gives reviewers a reproducible artifact.\n\nFinally, extension-aware 设计 is an integration problem, not just a contract problem. Product 和 support teams need clear messaging 用于 fee behavior, frozen 账户 states, 和 delegated capabilities. If users cannot predict token behavior from 钱包 prompts 和 docs, operational risk rises even when code is formally correct.\n\n## Decision framework 用于 extension selection\n\n用于 each extension, force three answers before enabling it:\n1. What concrete product requirement does this solve now?\n2. Which authority can abuse this if compromised?\n3. How will users 和 integrators observe this behavior in UX 和 docs?\n\nIf any answer is vague, extension scope is probably too broad.\n\n## Pitfall: Extension creep without threat modeling\n\nAdding multiple extensions \"用于 flexibility\" often creates overlapping authority powers 和 unpredictable UX. Enable only the extensions your product can govern, monitor, 和 explain end-to-end.\n\n## Sanity Checklist\n\n1. Define one explicit business reason per extension.\n2. Document extension authorities 和 revocation strategy.\n3. Verify partner compatibility assumptions before launch.\n4. Produce deterministic initialization artifacts 用于 review.\n",
            "duration": "45 min"
          },
          "token-v2-mint-anatomy": {
            "title": "Mint anatomy: authorities, decimals, supply, freeze, mint",
            "content": "# Mint anatomy: authorities, decimals, supply, freeze, mint\n\nA production token launch succeeds or fails on parameter discipline. The mint 账户 is a compact policy object: it defines decimal precision, minting authority, optional freeze authority, 和 extension configuration. Token 账户 then represent balances 用于 owners, usually through ATAs. If these pieces are configured inconsistently, downstream systems see contradictory behavior 和 user trust erodes quickly.\n\nDecimals are one of the most underestimated parameters. They influence UI formatting, fee interpretation, 和 business logic in integrations. While high precision can feel \"future-proof,\" excessive decimals often create rounding edge cases in analytics 和 partner systems. Constraining decimals to a documented operational range 和 validating it at config time is a 实战 defensive rule.\n\nAuthority layout should be explicit 和 minimal. Mint authority controls supply growth. Freeze authority controls 账户-level transfer ability. Update authority (用于 metadata-linked policy) can affect user-facing trust 和 protocol assumptions. Teams often reuse one operational key 用于 convenience, then struggle to separate powers later. A better pattern is to predefine authority roles 和 revocation milestones as part of launch 治理.\n\nSupply planning should distinguish issuance from distribution. Initial supply tells you what is minted; recipient allocations tell you what is distributed at launch. Those values should be validated 使用 exact integer math, not float formatting. Invariant checks such as `recipientsTotal <= initialSupply` are simple but prevent serious release mistakes.\n\nToken-2022 extensions deepen this anatomy. Transfer fee config introduces fee basis points 和 caps; default 账户 state changes 账户 activation posture; permanent delegate creates a privileged transfer actor. Each extension implies additional authority 和 monitoring requirements. Your launch plan must encode these requirements as explicit steps 和 include human-readable labels so reviewers can confirm intent.\n\nFinally, deterministic address derivation in 课程 tooling is a useful engineering discipline. Even when pseudo-addresses are used 用于 offline planning, stable derivation functions improve reproducibility 和 reduce reviewer ambiguity. The same mindset carries to real deployments where deterministic 账户 derivation is foundational.\n\nStrong teams also pair mint-anatomy reviews 使用 explicit incident playbooks: what to do if an authority key is lost, rotated, or compromised, 和 how to communicate those events to integrators without causing panic.\n\n## Pitfall: One-key authority convenience\n\nUsing a single key 用于 minting, freezing, 和 metadata updates simplifies setup but concentrates risk. Authority compromise then becomes a full-token compromise rather than a contained incident.\n\n## Sanity Checklist\n\n1. Validate decimals 和 supply fields before plan generation.\n2. Record mint/freeze/update authority roles 和 custody model.\n3. Confirm recipient allocation totals 使用 integer math.\n4. Review extension authorities independently from mint authority.\n",
            "duration": "45 min"
          },
          "token-v2-extension-safety-pitfalls": {
            "title": "Extension safety pitfalls: fee configs, delegate abuse, default 账户 state",
            "content": "# Extension safety pitfalls: fee configs, delegate abuse, default 账户 state\n\nToken-2022 extensions let teams express policy in a standard token framework, but policy power is exactly where operational failures happen. 安全 issues in token launches are rarely exotic cryptography failures. They are usually configuration mistakes: fee caps set too high, delegates granted too broadly, or frozen default states introduced without recovery controls. Production engineering must treat extension configuration as safety-critical logic.\n\nTransfer fee configuration is a good example. A basis-point fee looks simple, yet behavior depends on cap interaction 和 token decimals. If maxFee is undersized, large transfers saturate quickly 和 effective fee curve becomes nonlinear. If maxFee is oversized, treasury extraction can exceed expected user tolerance. Deterministic simulations across example transfer sizes are essential before launch, 和 those simulations should be reviewed by both protocol 和 product teams.\n\nPermanent delegate is another high-risk feature. It can enable managed flows, but it also creates a privileged actor that may transfer tokens without normal owner signatures depending on policy scope. If delegate authority is not governed by clear controls 和 revocation procedures, compromise risk rises sharply. In many incidents, teams enabled delegate-like authority 用于 convenience, then discovered too late that 治理 和 monitoring were insufficient.\n\nDefault 账户 state introduces user-experience 和 compliance implications. A frozen default state can enforce controlled activation, but it also creates onboarding failure if thaw paths are unclear or unavailable in partner 钱包. Teams should verify thaw strategy, authority custody, 和 fallback procedures before enabling frozen defaults in production.\n\nThe safest engineering workflow is deterministic 和 reviewable: validate config, normalize extension fields, generate initialization plan labels, simulate transfer outcomes, 和 produce invariant lists. That sequence creates a shared artifact 用于 engineering, 安全, legal, 和 support stakeholders. When questions arise, teams can inspect exact intended policy rather than infer from fragmented scripts.\n\nFinally, treat extension combinations as compounded risk. Each extension may be individually reasonable, yet combined authority interactions can create hidden escalation paths. Cross-extension threat modeling is therefore mandatory 用于 serious launches.\n\n## Pitfall: Fee 和 delegate settings shipped without scenario simulation\n\nTeams often validate only \"happy path\" transfer examples. Without boundary simulations 和 authority abuse scenarios, dangerous configurations can pass review 和 surface only after users are affected.\n\n## Sanity Checklist\n\n1. Simulate fee behavior at low/medium/high transfer sizes.\n2. Document delegate authority scope 和 emergency revocation path.\n3. Verify frozen default 账户 have explicit thaw operations.\n4. Review combined extension authority interactions 用于 escalation risk.\n",
            "duration": "45 min"
          },
          "token-v2-validate-config-derive": {
            "title": "Validate token config + derive deterministic addresses offline",
            "content": "# Validate token config + derive deterministic addresses offline\n\nImplement strict config validation 和 deterministic pseudo-derivation:\n- validate decimals, u64 strings, recipient totals, extension fields\n- derive stable pseudo mint 和 ATA addresses from hash seeds\n- return normalized validated config + derivations\n",
            "duration": "35 min"
          }
        }
      },
      "token-v2-module-launch-pack": {
        "title": "Token Launch Pack Project",
        "description": "Build deterministic validation, planning, 和 simulation workflows that produce reviewable launch artifacts 和 clear go/no-go criteria.",
        "lessons": {
          "token-v2-build-init-plan": {
            "title": "Build Token-2022 initialization 指令 plan",
            "content": "# Build Token-2022 initialization 指令 plan\n\nCreate a deterministic offline initialization plan:\n- create mint 账户 step\n- init mint step 使用 decimals\n- append selected extension steps in stable order\n- base64 encode step payloads 使用 explicit encoding version\n",
            "duration": "35 min"
          },
          "token-v2-simulate-fees-supply": {
            "title": "Build mint-to + transfer-fee math + simulation",
            "content": "# Build mint-to + transfer-fee math + simulation\n\nImplement pure simulation 用于 transfer fees 和 launch distribution:\n- fee = min(maxFee, floor(amount * feeBps / 10000))\n- aggregate distribution totals deterministically\n- ensure no negative supply 和 no oversubscription\n",
            "duration": "35 min"
          },
          "token-v2-launch-checklist": {
            "title": "Launch checklist: params, upgrade/authority strategy, airdrop/测试 plan",
            "content": "# Launch checklist: params, upgrade/authority strategy, airdrop/测试 plan\n\nA successful token launch is an operations exercise as much as a programming task. By the time users see your token in 钱包, dozens of choices have already constrained safety, 治理, 和 UX. Production token engineering therefore needs a launch checklist that turns abstract 设计 intent into verifiable execution steps.\n\nStart 使用 parameter closure. Name, symbol, decimals, initial supply, authority addresses, extension configuration, 和 recipient allocations must be finalized 和 reviewed as one immutable package before execution. Many launch incidents come from late parameter changes made in disconnected scripts. Deterministic launch pack generation prevents this by forcing a single source of truth.\n\nAuthority strategy is the second pillar. Decide which authorities remain active after launch, which are revoked, 和 which move to multisig custody. A common best practice is staged authority reduction: keep temporary controls through rollout validation, then revoke or transfer to 治理 once monitoring baselines are stable. This must be planned explicitly, not improvised during launch day.\n\n测试 strategy should include deterministic offline tests 和 limited online rehearsal. Offline checks validate config schemas, 指令 payload encoding, fee simulations, 和 supply invariants. Optional devnet rehearsal validates operational playbooks (funding, sequence execution, monitoring) but should not be your only validation layer. If offline checks fail, devnet success is not meaningful.\n\nAirdrop 和 distribution planning should include recipient reconciliation 和 rollback strategy. Teams often focus on minting 和 forget operational constraints around recipient list correctness, timing windows, 和 support escalation paths. Deterministic distribution plans 使用 stable labels make reconciliation simpler 和 reduce accidental double execution.\n\nMonitoring 和 communication are equally important. Define launch metrics in advance: minted supply observed, distribution completion count, fee behavior sanity, 和 extension-specific health checks. Publish user-facing notices 用于 any non-obvious behavior such as transfer fees or frozen default 账户 state. Clear communication lowers support load 和 improves trust.\n\nFinally, write down hard stop conditions. If invariants fail, if authority keys mismatch, or if distribution deltas diverge from expected totals, launch should pause immediately. Engineering discipline means refusing to proceed when safety checks are red.\n\n## Pitfall: Treating launch as a one-shot script run\n\nWithout an explicit checklist 和 rollback criteria, teams can execute technically valid 指令 that violate business or 治理 intent. Successful launches are controlled workflows, not single commands.\n\n## Sanity Checklist\n\n1. Freeze a canonical config payload before execution.\n2. Approve authority lifecycle 和 revocation milestones.\n3. Run deterministic offline simulation 和 invariant checks.\n4. Reconcile recipient totals 和 distribution labels.\n5. Define go/no-go criteria 和 escalation owners.\n",
            "duration": "45 min"
          },
          "token-v2-launch-pack-checkpoint": {
            "title": "Emit stable LaunchPackSummary",
            "content": "# Emit stable LaunchPackSummary\n\nCompose full project output as stable JSON:\n- normalized authorities 和 extensions\n- supply totals 和 optional fee model examples\n- deterministic plan metadata 和 invariants\n- fixtures hash + encoding version metadata\n",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-mobile": {
    "title": "Solana Mobile Development",
    "description": "Build production-ready mobile Solana dApps 使用 MWA, robust 钱包 session architecture, explicit signing UX, 和 disciplined distribution operations.",
    "duration": "6 hours",
    "tags": [
      "mobile",
      "saga",
      "dapp-store",
      "react-native"
    ],
    "modules": {
      "module-mobile-wallet-adapter": {
        "title": "Mobile 钱包 Adapter",
        "description": "Core MWA protocol, session lifecycle control, 和 resilient 钱包 handoff patterns 用于 production mobile apps.",
        "lessons": {
          "mobile-wallet-overview": {
            "title": "Mobile 钱包 概览",
            "content": "# Mobile 钱包 概览\n\nSolana Mobile development is built around the Solana Mobile Stack (SMS), a set of standards 和 tooling designed 用于 secure, high-quality crypto-native mobile experiences. SMS is more than a hardware initiative; it defines interoperable 钱包 communications, trusted execution patterns, 和 distribution infrastructure tailored to Web3 apps.\n\nA core piece is the Mobile 钱包 Adapter (MWA) protocol. Instead of embedding private keys in dApps, MWA connects mobile dApps to external 钱包 apps 用于 authorization, signing, 和 交易 submission. This separation mirrors browser 钱包 安全 on desktop 和 prevents dApps from directly handling secret keys.\n\nSaga introduced the first flagship reference device 用于 Solana Mobile concepts, including Seed Vault-oriented workflows. Even when users are not on Saga, SMS standards remain useful because protocol-level interoperability is the target: any 钱包 implementing MWA can serve compatible apps.\n\nThe Solana dApp Store is another foundational element. It provides a distribution channel 用于 crypto applications 使用 policy considerations better aligned to on-chain apps than traditional app stores. Teams can ship 钱包-native functionality, tokenized features, 和 on-chain social mechanics without the same constraints often imposed by conventional app marketplaces.\n\nKey architectural principles 用于 mobile Solana apps:\n\n- Keep signing in 钱包 context, not app context.\n- Treat session authorization as revocable 和 short-lived.\n- Build graceful fallback if 钱包 app is missing.\n- Optimize 用于 intermittent connectivity 和 mobile latency.\n\nTypical mobile flow:\n\n1. dApp requests authorization via MWA.\n2. 钱包 prompts user to approve 账户 access.\n3. dApp builds 交易 和 requests signatures.\n4. 钱包 returns signed payload or submits 交易.\n5. dApp observes confirmation 和 updates UI.\n\nMobile UX needs explicit state transitions (authorizing, awaiting 钱包, signing, submitted, confirmed). Ambiguity causes user drop-off quickly on small screens.\n\n用于 Solana teams, mobile is not a “mini web app”; it requires deliberate protocol 和 UX 设计 choices. SMS 和 MWA provide a secure baseline so developers can ship on-chain experiences 使用 production-grade signing 和 session models on handheld devices.\n\n## 实战 architecture split\n\nTreat your mobile stack as three independent systems:\n1. UI app state 和 navigation.\n2. 钱包 transport/session state (MWA lifecycle).\n3. On-chain 交易 intent 和 confirmation state.\n\nIf you couple these layers tightly, 钱包 switch interruptions 和 app backgrounding can corrupt flow state. If they stay separated, recovery is predictable.\n\n## What users should feel\n\nGood mobile crypto UX is not \"fewer steps at all costs.\" It is clear intent, explicit signing context, 和 safe recoverability when app switching or network instability happens.\n",
            "duration": "35 min"
          },
          "mwa-integration": {
            "title": "MWA Integration",
            "content": "# MWA Integration\n\nIntegrating Mobile 钱包 Adapter typically starts 使用 `@solana-mobile/mobile-wallet-adapter` APIs 和 an interaction pattern built around `transact()`. Within a 交易 session, the app can authorize, request capabilities, sign 交易, 和 handle 钱包 responses in a structured way.\n\nA simplified integration flow:\n\n```typescript\nimport { transact } from '@solana-mobile/mobile-wallet-adapter-protocol-web3js';\n\nawait transact(async (wallet) => {\n  const auth = await wallet.authorize({\n    cluster: 'devnet',\n    identity: { name: 'JazzCode Mobile', uri: 'https://jazzcode.vercel.app' },\n  });\n\n  const account = auth.accounts[0];\n  // build tx, request signing/submission\n});\n```\n\nAuthorization yields one or more 账户 plus auth tokens 用于 session continuation. Persist these tokens carefully 和 invalidate them on sign-out. Do not assume tokens remain valid forever; 钱包 apps can revoke sessions.\n\n用于 signing, you can request:\n\n- `signTransactions` (sign only)\n- `signAndSendTransactions` (钱包 signs 和 submits)\n- `signMessages` (SIWS-like auth flows)\n\nDeep links are used under the hood to switch between your dApp 和 钱包. That means state restoration matters: your app should survive process backgrounding 和 resume pending operation state on return.\n\n实战 engineering tips:\n\n- Implement idempotent 交易 request handling.\n- Show a visible “Waiting 用于 钱包 approval” state.\n- Handle user cancellation explicitly, not as generic failure.\n- Retry network submission separately from signing when possible.\n\n安全 considerations:\n\n- Bind sessions to app identity metadata.\n- Use short-lived backend nonces 用于 message-sign auth.\n- Never log full signed payloads 使用 sensitive context.\n\nMWA is effectively your mobile signing transport layer. If its state machine is robust, your app feels professional 和 trustworthy. If state handling is weak, users experience “stuck” flows 和 may distrust the dApp even if on-chain logic is correct.",
            "duration": "35 min"
          },
          "mobile-transaction": {
            "title": "Build a Mobile 交易 Function",
            "content": "# Build a Mobile 交易 Function\n\nImplement a helper that formats a deterministic MWA 交易 request summary string.\n\nExpected output format:\n\n`<cluster>|<payer>|<instructionCount>`\n\nUse this exact order 和 delimiter.",
            "duration": "50 min"
          }
        }
      },
      "module-dapp-store-and-distribution": {
        "title": "dApp Store & Distribution",
        "description": "Publishing, operational readiness, 和 trust-centered mobile UX practices 用于 Solana app distribution.",
        "lessons": {
          "dapp-store-submission": {
            "title": "dApp Store Submission",
            "content": "# dApp Store Submission\n\nPublishing to the Solana dApp Store requires more than packaging binaries. Teams should treat submission as a product, compliance, 和 安全 review process. A strong submission demonstrates safe 钱包 interactions, clear user communication, 和 operational readiness.\n\nSubmission readiness checklist:\n\n- Stable release builds 用于 target Android devices.\n- Clear app identity 和 support channels.\n- 钱包 interaction flows tested 用于 cancellation 和 failure recovery.\n- Privacy policy 和 terms aligned to on-chain behaviors.\n- Transparent handling of tokenized features 和 in-app value flows.\n\nOne distinguishing concept in Solana mobile distribution is token-aware product 设计. Apps may use NFT-gated access, on-chain subscriptions, or tokenized entitlements. These flows must be understandable to users 和 not hide financial consequences. Review teams typically evaluate whether permissions 和 钱包 prompts are proportional to app behavior.\n\nNFT-based licensing models can be implemented by checking ownership of specific collection assets at runtime. If licensing depends on assets, build robust indexing 和 refresh behavior so users are not locked out due to temporary RPC/indexer mismatch.\n\nOperational 最佳实践 用于 review success:\n\n- Provide reproducible test 账户 和 walkthroughs.\n- Include a “safe mode” or demo path if 钱包 connection fails.\n- Avoid unexplained signature prompts.\n- Log non-sensitive diagnostics 用于 support.\n\nPost-submission lifecycle matters too. Plan how you will handle urgent fixes, 钱包 SDK updates, 和 chain-level incidents. Mobile releases can take time to propagate, so feature flags 和 backend kill-switches 用于 risky pathways are valuable.\n\nDistribution strategy should also include analytics around onboarding funnels, 钱包 connect success rates, 和 交易 completion rates. These metrics identify mobile-specific friction that desktop-oriented teams often miss.\n\nA successful dApp Store submission reflects secure protocol integration 和 mature product operations. If your 钱包 interactions are explicit, fail-safe, 和 user-centered, your app is far more likely to pass review 和 retain users in production.",
            "duration": "35 min"
          },
          "mobile-best-practices": {
            "title": "Mobile 最佳实践",
            "content": "# Mobile 最佳实践\n\nMobile crypto UX requires balancing speed, safety, 和 trust. Users make high-stakes decisions on small screens, often on unstable networks. Solana mobile apps should therefore optimize 用于 explicitness 和 recoverability, not just visual polish.\n\n**Biometric gating** is useful 用于 sensitive local actions (revealing seed-dependent views, exporting 账户 data, approving high-risk actions), but 钱包-level signing decisions should remain in 钱包 app context. Avoid building fake in-app “confirm” screens that look like signing prompts.\n\n**Session keys 和 scoped auth** improve UX by reducing repetitive approvals. However, scope must be tightly constrained (allowed methods, time window, limits). Session credentials should be revocable 和 auditable.\n\n**Offline 和 poor-network behavior** must be handled intentionally:\n\n- Queue non-critical reads.\n- Retry idempotent submissions 使用 backoff.\n- Distinguish “signed but not submitted” from “submitted but unconfirmed.”\n\n**Push notifications** are valuable 用于 交易 outcomes, liquidation alerts, 和 治理 events. Notifications should include enough context 用于 user safety but never leak sensitive data.\n\nUX patterns that consistently improve conversion:\n\n- Show 交易 simulation summaries before 钱包 handoff.\n- Display clear statuses: building, awaiting signature, submitted, confirmed.\n- Provide explorer links 和 retry actions.\n- Use plain-language error messages 使用 suggested fixes.\n\n安全 hygiene:\n\n- Pin trusted RPC endpoints or use reputable providers 使用 fallback.\n- Validate 账户 ownership 和 expected program IDs on all client-side decoded data.\n- Protect analytics pipelines from sensitive payload leakage.\n\nAccessibility 和 internationalization matter 用于 global adoption. Ensure touch targets, contrast, 和 localization of risk messages are adequate. 用于 crypto workflows, misunderstanding can cause irreversible loss.\n\nFinally, measure reality: connect success rate, signature approval rate, drop-off after 钱包 switch, 和 average time-to-confirmation. Mobile teams that instrument these metrics can iteratively remove friction 和 increase trust.\n\nGreat Solana mobile apps feel predictable under stress. If users always understand what they are signing, what state they are in, 和 how to recover, your product is operating at production quality.",
            "duration": "35 min"
          }
        }
      }
    }
  },
  "solana-testing": {
    "title": "测试 Solana Programs",
    "description": "Build robust Solana 测试 systems across local, simulated, 和 network environments 使用 explicit 安全 invariants 和 release-quality confidence gates.",
    "duration": "6 hours",
    "tags": [
      "testing",
      "bankrun",
      "anchor",
      "devnet"
    ],
    "modules": {
      "module-testing-foundations": {
        "title": "测试 Foundations",
        "description": "Core test strategy across unit/integration layers 使用 deterministic workflows 和 adversarial case coverage.",
        "lessons": {
          "testing-approaches": {
            "title": "测试 Approaches",
            "content": "# 测试 Approaches\n\n测试 Solana programs requires multiple layers because failures can occur in logic, 账户 validation, 交易 composition, or network behavior. A production 测试 strategy usually combines unit tests, integration tests, 和 end-to-end validation across local 验证者 和 devnet.\n\n**Unit tests** validate isolated business logic 使用 minimal runtime overhead. In Rust, pure helper functions (math, state transitions, invariant checks) should be unit-tested aggressively because they are easy to execute 和 fast in CI.\n\n**Integration tests** execute against realistic program invocation paths. 用于 Anchor projects, this often means `anchor test` 使用 local 验证者 setup, 账户 initialization flows, 和 指令-level assertions. Integration tests should cover both positive 和 adversarial inputs, including invalid 账户, unauthorized signers, 和 boundary values.\n\n**End-to-end tests** include frontend/client composition plus 钱包 和 RPC interactions. They catch issues that lower layers miss: incorrect 账户 ordering, wrong PDA derivations in client code, 和 serialization mismatches.\n\nCommon tools:\n\n- `solana-program-test` 用于 Rust-side 测试 使用 in-process banks simulation.\n- `solana-bankrun` 用于 deterministic TypeScript integration 测试.\n- Anchor TypeScript client 用于 指令 building 和 assertions.\n- Playwright/Cypress 用于 app-level 交易 flow tests.\n\nTest coverage priorities:\n\n1. Authorization 和 signer checks.\n2. 账户 ownership 和 PDA seed constraints.\n3. Arithmetic boundaries 和 fee logic.\n4. CPI behavior 和 failure rollback.\n5. Upgrade compatibility 和 migration paths.\n\nA frequent anti-pattern is only 测试 happy paths 使用 one 钱包 和 static inputs. This misses most exploit classes. Robust suites include malicious 账户 substitution, stale or duplicated 账户, 和 partial failure simulation.\n\nIn CI, separate fast deterministic suites from slower network-dependent suites. Run deterministic tests on every push, 和 run heavier devnet suites on merge or release.\n\nEffective Solana 测试 is about confidence under adversarial conditions, not just green checkmarks. If your tests model attacker behavior 和 账户-level edge cases, you will prevent the majority of production incidents before 部署.\n\n## 实战 suite 设计 rule\n\nMap every critical 指令 to at least one test in each layer:\n- unit test 用于 pure invariant/math logic\n- integration test 用于 账户 validation 和 state transitions\n- environment test 用于 钱包/RPC orchestration\n\nIf one layer is missing, incidents usually appear in that blind spot first.",
            "duration": "35 min"
          },
          "bankrun-testing": {
            "title": "Bankrun 测试",
            "content": "# Bankrun 测试\n\nSolana Bankrun provides deterministic, high-speed test execution 用于 Solana programs from TypeScript environments. It emulates a local bank-like runtime where 交易 can be processed predictably, 账户 can be inspected directly, 和 temporal state can be manipulated 用于 测试 scenarios like vesting unlocks or oracle staleness.\n\nCompared 使用 relying on external devnet, Bankrun gives repeatability. This is crucial 用于 CI pipelines where flaky network behavior can mask regressions.\n\nTypical Bankrun workflow:\n\n1. Start test context 使用 target program loaded.\n2. Create keypairs 和 funded test 账户.\n3. Build 和 process 交易 via BanksClient-like API.\n4. Assert post-交易 账户 state.\n5. Advance slots/time 用于 time-dependent logic tests.\n\nConceptual setup:\n\n```typescript\n// pseudocode\nconst context = await startBankrun({ programs: [...] });\nconst client = context.banksClient;\n\n// process tx and inspect accounts deterministically\n```\n\nWhy Bankrun is powerful:\n\n- Fast iteration 用于 protocol teams.\n- Deterministic block/slot control.\n- Rich 账户 inspection without explorer dependency.\n- Easy simulation of multi-step protocol flows.\n\nHigh-value Bankrun test scenarios:\n\n- Liquidation eligibility after oracle/time movement.\n- Vesting 和 cliff unlock schedule transitions.\n- Fee accumulator updates across many operations.\n- CPI behavior 使用 mocked downstream 账户 states.\n\nCommon mistakes:\n\n- Asserting only 交易 success without state validation.\n- Ignoring rent 和 账户 lamport changes.\n- Not 测试 replay/idempotency behaviors.\n\nUse helper factories 用于 test 账户 和 PDA derivations so tests remain concise. Keep 交易 builders in reusable utilities to avoid drift between test 和 production clients.\n\nBankrun is not a replacement 用于 all environments, but it is one of the best layers 用于 deterministic integration confidence on Solana. Teams that invest in comprehensive Bankrun suites tend to catch state-machine bugs significantly earlier than teams relying only on devnet smoke tests.",
            "duration": "35 min"
          },
          "write-bankrun-test": {
            "title": "Write a Counter Program Bankrun Test",
            "content": "# Write a Counter Program Bankrun Test\n\nImplement a helper that returns the expected counter value after a sequence of increment operations. This mirrors a deterministic assertion you would use in a Bankrun test.\n\nReturn the final numeric value as a string.",
            "duration": "50 min"
          }
        }
      },
      "module-advanced-testing": {
        "title": "高级 测试",
        "description": "Fuzzing, devnet validation, 和 CI/CD release controls 用于 safer protocol changes.",
        "lessons": {
          "fuzzing-trident": {
            "title": "Fuzzing 使用 Trident",
            "content": "# Fuzzing 使用 Trident\n\nFuzzing explores large input spaces automatically to find bugs that handcrafted tests miss. 用于 Solana 和 Anchor programs, Trident-style fuzzing workflows generate randomized 指令 sequences 和 parameter values, then check invariants such as “total supply never decreases incorrectly” or “vault liabilities never exceed assets.”\n\nUnlike unit tests that validate expected examples, fuzzing asks: what if inputs are weird, extreme, or adversarial in combinations we did not think about?\n\nCore fuzzing components:\n\n- **Generators** 用于 指令 inputs 和 账户 states.\n- **Harness** that executes generated 交易.\n- **Invariants** that must always hold.\n- **Shrinking** to minimize failing inputs 用于 debugging.\n\nUseful invariants in DeFi protocols:\n\n- Conservation of value across transfers 和 burns.\n- Non-negative balances 和 debt states.\n- Authority invariants (only valid signer modifies privileged state).\n- Price 和 collateral constraints under liquidation logic.\n\nFuzzing strategy tips:\n\n- Start 使用 a small 指令 set 和 one invariant.\n- Add stateful multi-step scenarios (deposit->borrow->repay->withdraw).\n- Include random 账户 ordering 和 malicious 账户 substitution cases.\n- Track coverage to avoid blind spots.\n\nCoverage analysis matters: if fuzzing never reaches critical branches (error paths, CPI failure handlers, liquidation branches), it gives false confidence. Integrate branch coverage tools where possible.\n\nTrident 和 similar fuzzers are especially good at discovering arithmetic edge cases, stale state assumptions, 和 unexpected state transitions from unusual call sequences.\n\nCI integration approach:\n\n- Run short fuzz campaigns on every PR.\n- Run longer campaigns nightly.\n- Persist failing seeds as regression tests.\n\nFuzzing should complement, not replace, deterministic tests. Deterministic suites provide explicit behavior guarantees; fuzzing provides adversarial exploration at scale.\n\n用于 serious Solana protocols handling user funds, fuzzing is no longer optional. It is one of the highest-leverage investments 用于 preventing unknown-unknown bugs before mainnet exposure.",
            "duration": "35 min"
          },
          "devnet-testing": {
            "title": "Devnet 测试",
            "content": "# Devnet 测试\n\nDevnet 测试 bridges the gap between deterministic local tests 和 real-world network conditions. While local 验证者 和 Bankrun are ideal 用于 speed 和 reproducibility, devnet reveals behavior under real RPC latency, block production timing, fee markets, 和 账户 history constraints.\n\nA robust devnet test strategy includes:\n\n- Automated program 部署 to a dedicated devnet keypair.\n- Deterministic fixture creation (airdrop, mint setup, PDAs).\n- Smoke tests 用于 critical 指令 paths.\n- Monitoring of 交易 confirmation 和 log outputs.\n\nImportant devnet caveats:\n\n- State is shared 和 can be noisy.\n- Airdrop limits can throttle tests.\n- RPC providers may differ in reliability 和 rate limits.\n\nTo reduce flakiness:\n\n- Use dedicated namespaces/seeds per CI run.\n- Add retries 用于 transient network failures.\n- Bound test runtime 和 fail 使用 actionable logs.\n\nProgram upgrade 测试 is particularly important on devnet. Validate that new binaries preserve 账户 compatibility 和 migrations execute as expected. Incompatible changes can brick existing 账户 if not tested.\n\nChecklist 用于 release-candidate validation:\n\n1. Deploy upgraded program binary.\n2. Run migration 指令.\n3. Execute backward-compatibility read paths.\n4. Execute all critical write 指令.\n5. Verify event/log schema expected by indexers.\n\n用于 financial protocols, include oracle integration tests 和 liquidation path checks against live-like feeds if possible.\n\nDevnet should not be your only quality gate, but it is the best pre-mainnet signal 用于 environment-related issues. Teams that ship without meaningful devnet validation often discover RPC edge cases 和 timing bugs in production.\n\nTreat devnet as a staging environment 使用 disciplined test orchestration, clear observability, 和 explicit rollback plans.",
            "duration": "35 min"
          },
          "ci-cd-pipeline": {
            "title": "CI/CD Pipeline 用于 Solana",
            "content": "# CI/CD Pipeline 用于 Solana\n\nA mature Solana CI/CD pipeline enforces quality gates across code, tests, 安全 checks, 和 部署 workflows. 用于 program teams, CI is not just linting Rust 和 TypeScript; it is about protecting on-chain invariants before irreversible releases.\n\nRecommended pipeline stages:\n\n1. **Static checks**: formatting, lint, type checks.\n2. **Unit/integration tests**: deterministic local execution.\n3. **安全 checks**: dependency scan, optional static analyzers.\n4. **Build artifacts**: reproducible program binaries.\n5. **Staging deploy**: optional devnet 部署 和 smoke tests.\n6. **Manual approval** 用于 production deploy.\n\nGitHub Actions is a common choice. A typical workflow matrix runs Rust 和 Node tasks in parallel to reduce cycle time. Cache Cargo 和 pnpm dependencies aggressively.\n\nExample conceptual workflow snippets:\n\n```yaml\n- run: cargo test --workspace\n- run: pnpm lint && pnpm typecheck && pnpm test\n- run: anchor build\n- run: anchor test --skip-local-validator\n```\n\n用于 deployments:\n\n- Store deploy keypairs in secure secrets management.\n- Restrict deploy jobs to protected branches/tags.\n- Emit program IDs 和 交易 signatures as artifacts.\n\nProgram verification is critical. Where possible, verify deployed binary matches source-controlled build output. This strengthens trust 和 simplifies audits.\n\nOperational safety practices:\n\n- Use feature flags 用于 high-risk logic activation.\n- Keep rollback strategy documented.\n- Monitor post-deploy metrics (error rates, failed tx ratio, latency).\n\nInclude regression tests 用于 previously discovered bugs. Every production incident should produce a permanent automated test.\n\nA strong CI/CD pipeline is an engineering control, not a convenience. It reduces release risk, accelerates safe iteration, 和 provides confidence that code changes preserve 安全 和 protocol correctness under production conditions.",
            "duration": "35 min"
          }
        }
      }
    }
  },
  "solana-indexing": {
    "title": "Solana Indexing & Analytics",
    "description": "Build a production-grade Solana event indexer 使用 deterministic decoding, resilient ingestion contracts, checkpoint recovery, 和 analytics outputs teams can trust.",
    "duration": "10 hours",
    "tags": [
      "indexing",
      "analytics",
      "events",
      "tokens",
      "solana"
    ],
    "modules": {
      "indexing-v2-foundations": {
        "title": "Indexing Foundations",
        "description": "Events model, token decoding, 和 交易 parsing fundamentals 使用 schema discipline 和 deterministic normalization.",
        "lessons": {
          "indexing-v2-events-model": {
            "title": "Events model: 交易, logs, 和 program 指令",
            "content": "# Events model: 交易, logs, 和 program 指令\n\nIndexing Solana starts 使用 understanding where data lives 和 how to extract structured events from raw chain data. Unlike EVM chains where events are explicit log topics, Solana encodes program state changes in 账户 updates 和 program logs. Your indexer must parse these sources 和 transform them into a queryable event stream.\n\nA 交易 on Solana contains one or more 指令. Each 指令 targets a program, includes 账户 metas, 和 carries opaque 指令 data. When executed, programs emit log entries via solana_program::msg or similar macros. These logs, combined 使用 pre/post 账户 states, form the raw material 用于 event indexing.\n\nThe indexer pipeline typically follows: fetch → parse → normalize → store. Fetch retrieves 交易 metadata via RPC or geyser plugins. Parse extracts program logs 和 账户 diffs. Normalize converts raw data into domain-specific events 使用 stable schemas. Store persists events 使用 appropriate indexing 用于 queries.\n\nKey concepts 用于 normalization: 指令 program IDs identify which decoder to apply, 账户 ownership determines data layout, 和 log prefixes often indicate event types (e.g., \"Transfer\", \"Mint\", \"Burn\"). Your indexer must handle multiple program versions gracefully, maintaining backward compatibility as 指令 layouts evolve.\n\nIdempotency is critical. Block reorganizations are rare on Solana but possible during forks. Your indexing pipeline should handle replayed 交易 without duplicating events. This typically means using 交易 signatures as unique keys 和 implementing upsert semantics in the storage layer.\n\n## Operator 思维模型\n\nTreat your indexer as a data product 使用 explicit contracts:\n1. ingest contract (what raw inputs are accepted),\n2. normalization contract (stable event schema),\n3. serving contract (what query consumers can rely on).\n\nWhen these contracts are versioned 和 documented, protocol upgrades become manageable instead of breaking downstream analytics unexpectedly.\n\n## Checklist\n- Understand 交易 → 指令 → logs hierarchy\n- Identify program IDs 和 账户 ownership 用于 data layout selection\n- Normalize raw logs into domain events 使用 stable schemas\n- Implement idempotent ingestion using 交易 signatures\n- Plan 用于 program version evolution in decoders\n\n## Red flags\n- Parsing logs without validating program IDs\n- Assuming fixed 账户 ordering across program versions\n- Missing idempotency leading to duplicate events\n- Storing raw data without normalized event extraction\n",
            "duration": "45 min"
          },
          "indexing-v2-token-decoding": {
            "title": "Token 账户 decoding 和 SPL layout",
            "content": "# Token 账户 decoding 和 SPL layout\n\nSPL Token 账户 follow a standardized binary layout that indexers must parse to track balances 和 mint operations. Understanding this layout enables you to extract meaningful data from raw 账户 bytes without relying on external APIs.\n\nA token 账户 contains: mint address (32 bytes), owner address (32 bytes), amount (8 bytes u64), delegate (32 bytes, optional), state (1 byte), is_native (1 byte + 8 bytes if native), delegated_amount (8 bytes), 和 close_authority (36 bytes optional). The total size is typically 165 bytes 用于 standard 账户.\n\n账户 discriminators help identify 账户 types. SPL Token 账户 are owned by the Token Program (TokenkegQfeZyiNwAJbNbGKPFXCWuBvf9Ss623VQ5DA) or Token-2022. Your indexer should verify ownership before attempting to parse, as malicious 账户 could mimic data layouts.\n\nDecoding involves: read the 32-byte mint, verify it matches expected token, read the 64-bit amount (little-endian), convert to decimal representation using mint decimals, 和 track owner 用于 balance aggregation. Always handle malformed data gracefully - truncated 账户 or unexpected sizes should not crash the indexer.\n\n用于 balance diffs, compare pre-交易 和 post-交易 states. A transfer emits no explicit event but changes two 账户 amounts. Your indexer must detect these changes by comparing states before 和 after 指令 execution.\n\n## Checklist\n- Verify token program ownership before parsing\n- Decode mint, owner, 和 amount fields correctly\n- Handle little-endian u64 conversion properly\n- Support both Token 和 Token-2022 programs\n- Implement graceful handling 用于 malformed 账户\n\n## Red flags\n- Parsing without ownership verification\n- Ignoring mint decimals in amount conversion\n- Assuming fixed 账户 sizes without bounds checking\n- Missing balance diff detection 用于 transfers\n",
            "duration": "50 min"
          },
          "indexing-v2-decode-token-account": {
            "title": "Challenge: Decode token 账户 + diff token balances",
            "content": "# Challenge: Decode token 账户 + diff token balances\n\nImplement deterministic token 账户 decoding 和 balance diffing:\n\n- Parse a 165-byte SPL Token 账户 layout\n- Extract mint, owner, 和 amount fields\n- Compute balance differences between pre/post states\n- Return normalized event objects 使用 stable ordering\n\nYour solution will be validated against multiple test cases 使用 various token 账户 states.",
            "duration": "45 min"
          },
          "indexing-v2-transaction-meta": {
            "title": "交易 meta parsing: logs, errors, 和 inner 指令",
            "content": "# 交易 meta parsing: logs, errors, 和 inner 指令\n\n交易 metadata provides the context needed to index complex operations. Understanding how to parse logs, handle errors, 和 traverse inner 指令 enables comprehensive event extraction.\n\nProgram logs follow a hierarchical structure. The outermost logs show 指令 execution order, while inner logs reveal CPI calls. Each log line typically includes a prefix indicating severity or type: \"Program\", \"Invoke\", \"Success\", \"Fail\", or custom program messages. Your parser should handle nested invocation levels correctly.\n\nError handling distinguishes between 交易-level failures 和 指令-level failures. A 交易 may succeed overall while individual 指令 fail (和 are rolled back). Conversely, a single failing 指令 can cause the entire 交易 to fail. Indexers should record these distinctions 用于 accurate analytics.\n\nInner 指令 reveal the complete execution trace. When a program makes CPI calls, these appear as inner 指令 in 交易 metadata. Indexers must traverse both top-level 和 inner 指令 to capture all state changes. This is especially important 用于 protocols like Jupiter that route through multiple DEXs.\n\nLog filtering improves efficiency. Rather than parsing all logs, indexers can filter by program ID prefixes or known event signatures. However, be cautious - aggressive filtering might miss important events during protocol upgrades or edge cases.\n\n## Checklist\n- Parse program logs 使用 proper nesting level tracking\n- Distinguish 交易-level from 指令-level errors\n- Traverse inner 指令 用于 complete CPI traces\n- Implement efficient log filtering by program ID\n- Handle both success 和 failure scenarios\n\n## Red flags\n- Ignoring inner 指令 和 missing CPI events\n- Treating all log failures as 交易 failures\n- Parsing without log level/depth context\n- Missing error context in indexed events\n",
            "duration": "50 min"
          }
        }
      },
      "indexing-v2-pipeline": {
        "title": "Indexing Pipeline & Analytics",
        "description": "Build end-to-end indexer pipeline behavior: idempotent ingestion, checkpoint recovery, 和 analytics aggregation at production scale.",
        "lessons": {
          "indexing-v2-index-transactions": {
            "title": "Challenge: Index 交易 to normalized events",
            "content": "# Challenge: Index 交易 to normalized events\n\nImplement a 交易 indexer that produces normalized Event objects:\n\n- Parse 指令 logs 和 identify event types\n- Extract transfer events 使用 from/to/amount/mint\n- Handle multiple events per 交易\n- Return stable, canonical JSON 使用 sorted keys\n- Support idempotency via 交易 signature deduplication",
            "duration": "50 min"
          },
          "indexing-v2-pagination-caching": {
            "title": "Pagination, checkpointing, 和 caching semantics",
            "content": "# Pagination, checkpointing, 和 caching semantics\n\nProduction indexers must handle large datasets efficiently while maintaining consistency. Pagination, checkpointing, 和 caching form the backbone of scalable indexing infrastructure.\n\nPagination strategies depend on query patterns. Cursor-based pagination using 交易 signatures provides stable ordering even during concurrent writes. Offset-based pagination can miss or duplicate entries during high-write periods. 用于 time-series data, consider partitioning by slot or block time.\n\nCheckpointing enables recovery from failures. Indexers should periodically save their processing position (last processed slot/signature) to durable storage. On restart, resume from the checkpoint rather than re-indexing from genesis. This pattern is essential 用于 long-running indexers handling months of chain history.\n\nCaching reduces redundant RPC calls. 账户 metadata, program IDs, 和 decoded 指令 layouts can be cached 使用 appropriate TTLs. However, cache invalidation is critical - stale cache entries can lead to incorrect decoding or missed events. Consider using slot-based versioning 用于 cache entries.\n\nIdempotent writes prevent data corruption. Even 使用 checkpointing, duplicate processing can occur during retries. Use 交易 signatures as unique identifiers 和 implement upsert semantics. Database constraints or unique indexes should enforce this at the storage layer.\n\n## Checklist\n- Implement cursor-based pagination 用于 stable ordering\n- Save periodic checkpoints 用于 failure recovery\n- Cache 账户 metadata 使用 slot-based invalidation\n- Enforce idempotent writes via unique constraints\n- Handle backfills without duplicating events\n\n## Red flags\n- Using offset pagination 用于 high-write datasets\n- Missing checkpointing requiring full re-index on restart\n- Caching without proper invalidation strategies\n- Allowing duplicate events from retry logic\n",
            "duration": "45 min"
          },
          "indexing-v2-analytics": {
            "title": "Analytics aggregation: per 钱包, per token metrics",
            "content": "# Analytics aggregation: per 钱包, per token metrics\n\nRaw event data becomes valuable through aggregation. Building analytics pipelines enables insights into user behavior, token flows, 和 protocol usage patterns.\n\nPer-钱包 analytics track individual user activity. Key metrics include: 交易 count, unique tokens held, total volume transferred, first/last activity timestamps, 和 interaction patterns 使用 specific programs. These metrics power user segmentation 和 engagement analysis.\n\nPer-token analytics track asset-level metrics. Important aggregations include: total transfer volume, unique holders, holder distribution (whales vs retail), velocity (average time between transfers), 和 cross-program usage. These inform tokenomics analysis 和 market research.\n\nTime-windowed aggregations support trend analysis. Daily, weekly, 和 monthly rollups enable comparison across time periods. Consider using tumbling windows 用于 fixed periods or sliding windows 用于 moving averages. Materialized views can pre-compute common aggregations 用于 query 性能.\n\nNormalization ensures consistent comparisons. Convert all amounts to human-readable decimals, normalize timestamps to UTC, 和 use consistent address formatting (base58). Deduplicate events from failed 交易 that may still appear in logs.\n\n## Checklist\n- Aggregate per-钱包 metrics (volume, token count, activity)\n- Aggregate per-token metrics (holders, velocity, distribution)\n- Implement time-windowed rollups 用于 trend analysis\n- Normalize amounts, timestamps, 和 addresses\n- Exclude failed 交易 from aggregates\n\n## Red flags\n- Mixing raw 和 decimal-adjusted amounts\n- Including failed 交易 in volume metrics\n- Missing time normalization across timezones\n- Storing unbounded raw data without aggregation\n",
            "duration": "45 min"
          },
          "indexing-v2-analytics-checkpoint": {
            "title": "Checkpoint: Produce stable JSON analytics summary",
            "content": "# Checkpoint: Produce stable JSON analytics summary\n\nImplement the final analytics checkpoint that produces a deterministic summary:\n\n- Aggregate events into per-钱包 和 per-token metrics\n- Generate sorted, stable JSON output\n- Include timestamp 和 summary statistics\n- Handle edge cases (empty datasets, single events)\n\nThis checkpoint validates your complete indexing pipeline from raw data to analytics.",
            "duration": "50 min"
          }
        }
      }
    }
  },
  "solana-payments": {
    "title": "Solana Payments & Checkout Flows",
    "description": "Build production-grade Solana payment flows 使用 robust validation, replay-safe idempotency, secure webhooks, 和 deterministic receipts 用于 reconciliation.",
    "duration": "10 hours",
    "tags": [
      "payments",
      "checkout",
      "webhooks",
      "transactions",
      "solana"
    ],
    "modules": {
      "payments-v2-foundations": {
        "title": "Payment Foundations",
        "description": "Address validation, idempotency strategy, 和 payment intent 设计 用于 reliable checkout behavior.",
        "lessons": {
          "payments-v2-address-validation": {
            "title": "Address validation 和 memo strategies",
            "content": "# Address validation 和 memo strategies\n\nPayment flows on Solana require robust address validation 和 thoughtful memo strategies. Unlike traditional payment systems 使用 账户 numbers, Solana uses base58-encoded public keys that must be validated before any value transfer.\n\nAddress validation involves three layers: format validation, derivation check, 和 ownership verification. Format validation ensures the string is valid base58 和 decodes to 32 bytes. Derivation check optionally verifies the address is on the Ed25519 curve (用于 钱包 addresses) or off-curve (用于 PDAs). Ownership verification confirms the 账户 exists 和 is owned by the expected program.\n\nMemos attach metadata to payments. The SPL Memo program enables attaching UTF-8 strings to 交易. Common use cases include: order IDs, invoice references, customer identifiers, 和 compliance data. Memos are not encrypted 和 are visible on-chain, so never include sensitive information.\n\nMemo 最佳实践: keep under 256 bytes 用于 efficiency, use structured formats (JSON) 用于 machine parsing, include versioning 用于 future compatibility, 和 hash sensitive identifiers rather than storing them plaintext. Consider using deterministic memo formats that can be regenerated from payment context 用于 idempotency checks.\n\nAddress poisoning is an attack vector where attackers create addresses visually similar to legitimate ones. Countermeasures include: displaying addresses 使用 checksums, using name services (Solana Name Service, Bonfida) where appropriate, 和 implementing confirmation steps 用于 large transfers.\n\n## Merchant-safe memo template\n\nA 实战 memo format is:\n`v1|order:<id>|shop:<merchantId>|nonce:<shortHash>`\n\nThis keeps memos short, parseable, 和 versioned while avoiding direct storage of sensitive user details.\n\n## Checklist\n- Validate base58 encoding 和 32-byte length\n- Distinguish between on-curve 和 off-curve addresses\n- Verify 账户 ownership 用于 program-specific payments\n- Use SPL Memo program 用于 structured metadata\n- Implement address poisoning protections\n\n## Red flags\n- Transferring to unvalidated addresses\n- Storing sensitive data in plaintext memos\n- Skipping ownership checks 用于 token 账户\n- Trusting visually similar addresses without verification\n",
            "duration": "45 min"
          },
          "payments-v2-idempotency": {
            "title": "Idempotency keys 和 replay protection",
            "content": "# Idempotency keys 和 replay protection\n\nPayment systems must handle network failures gracefully. Idempotency ensures that retrying a failed request produces the same outcome as the original, preventing duplicate charges 和 inconsistent state.\n\nIdempotency keys are unique identifiers generated by clients 用于 each payment intent. The server stores processed keys 和 their outcomes, returning cached results 用于 duplicate submissions. Keys should be: globally unique (UUID v4), client-generated, 和 persisted 使用 sufficient TTL to handle extended retry windows.\n\nKey generation strategies include: UUID v4 使用 timestamp prefix, hash of payment parameters (amount, recipient, timestamp), or structured keys combining merchant ID 和 local sequence numbers. The key must be stable across retries but unique across distinct payments.\n\nReplay protection prevents malicious or accidental re-execution. Beyond idempotency, 交易 should include: recent blockhash freshness (prevents old 交易 replay), durable nonce 用于 offline signing scenarios, 和 amount/time bounds where applicable.\n\nError classification affects retry behavior. Network errors warrant retries 使用 exponential backoff. Validation errors (insufficient funds, invalid address) should fail fast without retry. Timeout errors require careful handling - the payment may have succeeded, so query status before retrying.\n\n## Checklist\n- Generate unique idempotency keys 用于 each payment intent\n- Store processed keys 使用 outcomes 用于 deduplication\n- Implement appropriate TTL based on retry windows\n- Use recent blockhash 用于 交易 freshness\n- Classify errors 用于 appropriate retry strategies\n\n## Red flags\n- Allowing duplicate payments from retries\n- Generating idempotency keys server-side only\n- Ignoring timeout ambiguity in status checking\n- Storing keys without expiration\n",
            "duration": "50 min"
          },
          "payments-v2-payment-intent": {
            "title": "Challenge: Create payment intent 使用 validation",
            "content": "# Challenge: Create payment intent 使用 validation\n\nImplement a payment intent creator 使用 full validation:\n\n- Validate recipient address format (base58, 32 bytes)\n- Validate amount (positive, within limits)\n- Generate deterministic idempotency key\n- Return structured payment intent object\n- Handle edge cases (zero amount, invalid address)\n\nYour implementation will be tested against various valid 和 invalid inputs.",
            "duration": "45 min"
          },
          "payments-v2-tx-building": {
            "title": "交易 building 和 key metadata",
            "content": "# 交易 building 和 key metadata\n\nBuilding payment 交易 requires careful attention to 指令 construction, 账户 metadata, 和 program interactions. The goal is creating valid, efficient 交易 that minimize fees while ensuring correctness.\n\n指令 construction follows a pattern: identify the program (System Program 用于 SOL transfers, Token Program 用于 SPL transfers), prepare 账户 metas 使用 correct writable/signer flags, serialize 指令 data according to the program's layout, 和 compute the 交易 message 使用 all required fields.\n\n账户 metadata is critical. 用于 SOL transfers, you need: from (signer + writable), to (writable). 用于 SPL transfers: token 账户 from (signer + writable), token 账户 to (writable), owner (signer), 和 potentially a delegate if using delegated transfer. Missing or incorrect flags cause runtime failures.\n\nFee optimization strategies include: batching multiple payments into one 交易 (up to compute unit limits), using address lookup tables (ALTs) 用于 账户 referenced multiple times, 和 setting appropriate compute unit limits to avoid overpaying 用于 simple operations.\n\n交易 validation before submission: verify all required signatures are present, check recent blockhash is fresh, estimate compute units if possible, 和 validate 指令 data encoding matches the expected layout.\n\n## Checklist\n- Set correct writable/signer flags on all 账户\n- Use appropriate program 用于 transfer type (SOL vs SPL)\n- Validate 指令 data encoding\n- Include recent blockhash 用于 freshness\n- Consider batching 用于 multiple payments\n\n## Red flags\n- Missing signer flags on fee payer\n- Incorrect writable flags on recipient 账户\n- Using wrong program ID 用于 token type\n- Stale blockhash causing 交易 rejection\n",
            "duration": "45 min"
          }
        }
      },
      "payments-v2-implementation": {
        "title": "Implementation & Verification",
        "description": "交易 building, webhook authenticity checks, 和 deterministic receipt generation 使用 clear error-state handling.",
        "lessons": {
          "payments-v2-transfer-tx": {
            "title": "Challenge: Build transfer 交易",
            "content": "# Challenge: Build transfer 交易\n\nImplement a transfer 交易 builder:\n\n- Build SystemProgram.transfer 用于 SOL transfers\n- Build TokenProgram.transfer 用于 SPL transfers\n- Return 指令 bundle 使用 correct key metadata\n- Include fee payer 和 blockhash\n- Support deterministic output 用于 测试",
            "duration": "50 min"
          },
          "payments-v2-webhooks": {
            "title": "Webhook signing 和 verification",
            "content": "# Webhook signing 和 verification\n\nWebhooks enable asynchronous payment notifications. 安全 requires cryptographic signing so recipients can verify webhook authenticity 和 detect tampering.\n\nWebhook signing uses HMAC-SHA256 使用 a shared secret. The sender computes: signature = HMAC-SHA256(secret, payload). The signature is included in a header (e.g., X-Webhook-Signature). Recipients recompute the HMAC 和 compare, using constant-time comparison to prevent timing attacks.\n\nPayload canonicalization ensures consistent signing. JSON objects must be serialized 使用: sorted keys (alphabetical), no extra whitespace, consistent number formatting, 和 UTF-8 encoding. Without canonicalization, {\"a\":1,\"b\":2} 和 {\"b\":2,\"a\":1} produce different signatures.\n\nIdempotency in webhooks prevents duplicate processing. Webhook payloads should include an idempotency key or event ID. Recipients store processed IDs 和 ignore duplicates. This handles retries from the sender 和 network-level duplicates.\n\n安全 最佳实践: rotate secrets periodically, use different secrets per environment (dev/staging/prod), include timestamps 和 reject old webhooks (e.g., >5 minutes), 和 verify IP allowlists where feasible. Never include sensitive data like private keys or full card numbers in webhooks.\n\n## Checklist\n- Sign webhooks 使用 HMAC-SHA256 和 shared secret\n- Canonicalize JSON payloads 使用 sorted keys\n- Include event ID 用于 idempotency\n- Verify signatures 使用 constant-time comparison\n- Implement timestamp validation\n\n## Red flags\n- Unsigned webhooks trusting sender IP alone\n- Non-canonical JSON causing verification failures\n- Missing idempotency handling duplicate events\n- Including secrets or sensitive data in payload\n",
            "duration": "45 min"
          },
          "payments-v2-error-states": {
            "title": "Error state machine 和 receipt format",
            "content": "# Error state machine 和 receipt format\n\nPayment flows require well-defined state machines to handle the complexity of asynchronous confirmations, failures, 和 retries. Clear state transitions 和 receipt formats ensure reliable payment tracking.\n\nPayment states typically include: pending (intent created, not yet submitted), processing (交易 submitted, awaiting confirmation), succeeded (交易 confirmed, payment complete), failed (交易 failed or rejected), 和 cancelled (intent explicitly cancelled before submission). Each state has valid transitions 和 terminal states.\n\nState transition rules: pending can transition to processing, cancelled, or failed; processing can transition to succeeded or failed; succeeded 和 failed are terminal. Invalid transitions (e.g., succeeded → failed) indicate bugs or data corruption.\n\nReceipt format standardization enables interoperability. A payment receipt should include: payment intent ID, 交易 signature (if submitted), amount 和 currency, recipient address, timestamp, status, 和 verification data (e.g., explorer link). Receipts should be JSON 使用 canonical ordering 用于 deterministic hashing.\n\nExplorer links provide transparency. 用于 Solana, construct explorer URLs using: https://explorer.solana.com/tx/{signature}?cluster={network}. Include these in receipts 和 webhook payloads so users can verify 交易 independently.\n\n## Checklist\n- Define clear payment states 和 valid transitions\n- Implement state machine validation\n- Generate standardized receipt JSON\n- Include explorer links 用于 verification\n- Handle all terminal states appropriately\n\n## Red flags\n- Ambiguous states without clear definitions\n- Missing terminal state handling\n- Non-deterministic receipt formats\n- No explorer links 用于 verification\n",
            "duration": "40 min"
          },
          "payments-v2-webhook-receipt": {
            "title": "Challenge: Verify webhook 和 produce receipt",
            "content": "# Challenge: Verify webhook 和 produce receipt\n\nImplement the final payment flow checkpoint:\n\n- Verify signed webhook signature (HMAC-SHA256)\n- Extract payment details from payload\n- Generate standardized receipt JSON\n- Include explorer link 和 verification data\n- Ensure deterministic, sorted output\n\nThis validates the complete payment flow from intent to receipt.",
            "duration": "50 min"
          }
        }
      }
    }
  },
  "solana-nft-compression": {
    "title": "NFTs & Compressed NFTs Fundamentals",
    "description": "Master compressed NFT engineering on Solana: Merkle commitments, proof systems, collection modeling, 和 production 安全 checks.",
    "duration": "12 hours",
    "tags": [
      "nfts",
      "compression",
      "merkle-trees",
      "cnfts",
      "solana"
    ],
    "modules": {
      "cnft-v2-merkle-foundations": {
        "title": "Merkle Foundations",
        "description": "Tree construction, leaf hashing, insertion mechanics, 和 the on-chain/off-chain commitment model behind compressed assets.",
        "lessons": {
          "cnft-v2-merkle-trees": {
            "title": "Merkle trees 用于 state compression",
            "content": "# Merkle trees 用于 state compression\n\nCompressed NFTs (cNFTs) on Solana use Merkle trees to dramatically reduce storage costs. Understanding Merkle trees is essential 用于 working 使用 compressed NFTs 和 building compression-aware applications.\n\nA Merkle tree is a binary hash tree where each leaf node contains a hash of data, 和 each non-leaf node contains the hash of its children. The root hash commits to the entire tree structure 和 all leaf data. This structure enables efficient proofs of inclusion without revealing the entire dataset.\n\nTree construction follows a bottom-up approach: hash each data element to create leaves, pair adjacent leaves 和 hash their concatenation to create parents, 和 repeat until a single root remains. 用于 odd numbers of nodes, the last node is typically promoted to the next level or paired 使用 a zero hash depending on the implementation.\n\nSolana's cNFT implementation uses concurrent Merkle trees 使用 16-bit depth (max 65,536 leaves). The tree state is stored on-chain as a small 账户 containing just the root hash 和 metadata. Actual leaf data (NFT metadata) is stored off-chain, typically via RPC providers or indexers.\n\nKey properties of Merkle trees: any leaf change affects the root, inclusion proofs require only log2(n) hashes, 和 the root serves as a cryptographic commitment to all data. These properties enable state compression while maintaining verifiability.\n\n## 实战 cNFT architecture rule\n\nTreat compressed NFT systems as two synchronized layers:\n1. on-chain commitment layer (tree root + update rules),\n2. off-chain data layer (metadata + indexing + proof serving).\n\nIf either layer is weakly validated, ownership 和 metadata trust can diverge.\n\n## Checklist\n- Understand binary hash tree construction\n- Know how leaf changes propagate to the root\n- Calculate proof size: log2(n) hashes 用于 n leaves\n- Recognize depth limits (16-bit = 65,536 max leaves)\n- Understand on-chain vs off-chain data split\n\n## Red flags\n- Treating Merkle roots as data storage (they're commitments)\n- Ignoring depth limits when planning collections\n- Storing sensitive data assuming it's \"hidden\" in the tree\n- Not validating proofs against the current root\n",
            "duration": "50 min"
          },
          "cnft-v2-leaf-hashing": {
            "title": "Leaf hashing conventions 和 metadata",
            "content": "# Leaf hashing conventions 和 metadata\n\nLeaf hashing determines how NFT metadata is committed to the Merkle tree. Understanding these conventions ensures compatibility 使用 compression standards 和 proper proof generation.\n\nLeaf structure 用于 cNFTs includes: asset ID (derived from tree address 和 leaf index), owner public key, delegate (optional), nonce 用于 uniqueness, 和 metadata hash. The exact encoding follows the Metaplex Bubblegum specification, using deterministic serialization 用于 consistent hashing.\n\nHashing algorithm uses Keccak-256 用于 Ethereum compatibility, 使用 domain separation via prefixed constants. The leaf hash is computed as: hash(prefix || asset_data) where prefix prevents collision 使用 other hash usages. Multiple prefix values exist 用于 different operation types (mint, transfer, burn).\n\nMetadata handling stores the full NFT metadata (name, symbol, uri, creators, royalties) off-chain. Only a hash of the metadata is stored in the leaf. This enables large metadata without on-chain storage costs while maintaining integrity via the hash commitment.\n\nCreator verification uses a separate signing process. Creators sign the asset ID to verify authenticity. These signatures are stored alongside proofs but not in the Merkle tree itself, allowing flexible verification without tree updates.\n\n## Checklist\n- Understand leaf structure components\n- Use correct hashing algorithm (Keccak-256)\n- Include proper domain separation prefixes\n- Store metadata off-chain 使用 hash commitment\n- Handle creator signatures separately from tree\n\n## Red flags\n- Using wrong hashing algorithm\n- Missing domain separation in leaf hashes\n- Storing full metadata on-chain in compressed NFTs\n- Ignoring creator verification requirements\n",
            "duration": "50 min"
          },
          "cnft-v2-merkle-insert": {
            "title": "Challenge: Implement Merkle tree insert + root updates",
            "content": "# Challenge: Implement Merkle tree insert + root updates\n\nBuild a Merkle tree implementation 使用 insertions:\n\n- Insert leaves 和 compute new root\n- Update parent hashes up the tree\n- Handle tree growth 和 depth limits\n- Return deterministic root updates\n\nTest cases will verify correct root evolution after multiple insertions.",
            "duration": "50 min"
          },
          "cnft-v2-proof-generation": {
            "title": "Proof generation 和 path computation",
            "content": "# Proof generation 和 path computation\n\nMerkle proofs enable verification of leaf inclusion without accessing the entire tree. Understanding proof generation is essential 用于 working 使用 compressed NFTs 和 building verification systems.\n\nA Merkle proof consists of: the leaf data (or its hash), a list of sibling hashes (one per level), 和 the leaf index (determining the path). The verifier recomputes the root by hashing the leaf 使用 siblings in the correct order, comparing against the expected root.\n\nProof generation requires traversing from leaf to root. At each level, record the sibling hash (the other child of the parent). The leaf index determines whether the current hash goes left or right in each concatenation. 用于 index i at level n, the position is determined by the nth bit of i.\n\nProof verification recomputes the root: start 使用 the leaf hash, 用于 each sibling in the proof list, concatenate current hash 使用 sibling (order depends on leaf index bit), hash the result, 和 compare final result 使用 expected root. Equality proves inclusion.\n\nProof size efficiency: 用于 a tree 使用 n leaves, proofs contain log2(n) hashes. This is dramatically smaller than the full tree (n hashes), enabling scalable verification. A 65,536 leaf tree requires only 16 hashes per proof.\n\n## Checklist\n- Collect sibling hashes at each tree level\n- Use leaf index bits to determine concatenation order\n- Verify proofs by recomputing root hash\n- Handle edge cases (empty tree, single leaf)\n- Optimize proof size 用于 network transmission\n\n## Red flags\n- Incorrect concatenation order in verification\n- Using wrong sibling hash at any level\n- Not validating proof length matches tree depth\n- Trusting proofs without root comparison\n",
            "duration": "45 min"
          }
        }
      },
      "cnft-v2-proof-system": {
        "title": "Proof System & 安全",
        "description": "Proof generation, verification, collection integrity, 和 安全 hardening against replay 和 metadata spoofing.",
        "lessons": {
          "cnft-v2-proof-verification": {
            "title": "Challenge: Implement proof generation + verifier",
            "content": "# Challenge: Implement proof generation + verifier\n\nBuild a complete proof system:\n\n- Generate proofs from a Merkle tree 和 leaf index\n- Verify proofs against a root hash\n- Handle invalid proofs (wrong siblings, wrong index)\n- Return deterministic boolean results\n\nTests will verify both successful proofs 和 rejection of invalid attempts.",
            "duration": "55 min"
          },
          "cnft-v2-collection-minting": {
            "title": "Collection mints 和 metadata simulation",
            "content": "# Collection mints 和 metadata simulation\n\nCompressed NFT collections use a collection mint as the parent NFT, enabling grouping 和 verification of related assets. Understanding this hierarchy is essential 用于 building collection-aware applications.\n\nCollection structure includes: a standard SPL NFT as the collection mint, cNFTs referencing the collection in their metadata, 和 the Merkle tree containing all cNFT leaves. The collection mint provides on-chain provenance while cNFTs provide scalable asset issuance.\n\nMetadata simulation 用于 测试 allows development without actual on-chain 交易. Simulated metadata includes: name, symbol, uri (typically pointing to off-chain JSON), seller fee basis points (royalties), creators array 使用 shares, 和 collection reference. This data structure matches on-chain format 用于 seamless migration.\n\nRoyalty enforcement through Metaplex's token metadata standard specifies seller fees as basis points (e.g., 500 = 5%). Creators array defines fee distribution 使用 verified flags. cNFTs inherit these settings from their metadata, enforced during transfers via the Bubblegum program.\n\nAttacks on compressed NFTs include: invalid proofs (claiming non-existent NFTs), index manipulation (using wrong leaf index), metadata spoofing (fake off-chain data), 和 collection impersonation (fake collection mints). Mitigations include proof verification, metadata hash validation, 和 collection mint verification.\n\n## Checklist\n- Understand collection mint hierarchy\n- Simulate metadata 用于 测试\n- Implement royalty calculations\n- Verify collection membership\n- Handle metadata hash verification\n\n## Red flags\n- Accepting NFTs without collection verification\n- Ignoring royalty settings in transfers\n- Trusting off-chain metadata without hash validation\n- Not validating proofs 用于 ownership claims\n",
            "duration": "45 min"
          },
          "cnft-v2-attack-surface": {
            "title": "Attack surface: invalid proofs 和 replay",
            "content": "# Attack surface: invalid proofs 和 replay\n\nCompressed NFTs introduce unique 安全 considerations. Understanding attack vectors 和 mitigations is critical 用于 building secure compression-aware applications.\n\nInvalid proof attacks attempt to verify non-existent NFTs. An attacker provides a fabricated leaf hash 和 fake sibling hashes hoping to produce a valid-looking verification. Mitigation: always verify against the current root from a trusted source (RPC, on-chain 账户), 和 validate proof structure (correct depth, valid hash lengths).\n\nIndex manipulation exploits use valid proofs but wrong indices. Since leaf indices determine proof path, changing the index produces a different root computation. Mitigation: bind asset IDs to specific indices 和 validate index-asset correspondence during verification.\n\nReplay attacks re-use old proofs after tree updates. When leaves are added or modified, the root changes 和 old proofs become invalid. However, if an application caches roots, it might accept stale proofs. Mitigation: always use current root, implement proof timestamps where applicable.\n\nMetadata attacks substitute fake off-chain data. Since metadata is stored off-chain 使用 only a hash on-chain, attackers might serve altered metadata files. Mitigation: verify metadata hashes against leaf commitments, use content-addressed storage (IPFS), 和 validate creator signatures.\n\nCollection spoofing creates fake collections mimicking legitimate ones. Attackers mint similar-looking NFTs 使用 fake collection references. Mitigation: verify collection mint addresses against known good lists, check collection verification status, 和 validate authority signatures.\n\n## Checklist\n- Verify proofs against current root\n- Validate leaf index matches asset ID\n- Implement replay protection 用于 proofs\n- Hash-verify off-chain metadata\n- Verify collection mint authenticity\n\n## Red flags\n- Accepting cached/stale roots 用于 verification\n- Ignoring leaf index validation\n- Trusting off-chain metadata without verification\n- Not checking collection verification status\n",
            "duration": "45 min"
          },
          "cnft-v2-compression-checkpoint": {
            "title": "Checkpoint: Simulate mint + verify ownership proof",
            "content": "# Checkpoint: Simulate mint + verify ownership proof\n\nComplete the compression lab checkpoint:\n\n- Simulate minting a cNFT (insert leaf, update root)\n- Generate ownership proof 用于 the minted NFT\n- Verify the proof against current root\n- Output stable audit trace 使用 sorted keys\n- Detect 和 report invalid proof attempts\n\nThis validates your complete Merkle tree implementation 用于 compressed NFTs.",
            "duration": "60 min"
          }
        }
      }
    }
  },
  "solana-governance-multisig": {
    "title": "治理 & Multisig Treasury Ops",
    "description": "Build production-ready DAO 治理 和 multisig treasury systems 使用 deterministic vote accounting, timelock safety, 和 secure execution controls.",
    "duration": "11 hours",
    "tags": [
      "governance",
      "multisig",
      "dao",
      "treasury",
      "solana"
    ],
    "modules": {
      "governance-v2-governance": {
        "title": "DAO 治理",
        "description": "Proposal lifecycle, deterministic voting mechanics, quorum policy, 和 timelock safety 用于 credible DAO 治理.",
        "lessons": {
          "governance-v2-dao-model": {
            "title": "DAO model: proposals, voting, 和 execution",
            "content": "# DAO model: proposals, voting, 和 execution\n\nDecentralized 治理 on Solana follows a proposal-based model where token holders vote on changes 和 the DAO treasury executes approved decisions. Understanding this flow is essential 用于 building 和 participating in on-chain organizations.\n\nThe 治理 lifecycle has four stages: proposal creation (anyone 使用 sufficient stake can propose), voting period (token holders vote 用于/against/abstain), queueing (successful proposals enter a timelock), 和 execution (the proposal's 指令 are executed). Each stage has specific requirements 和 time constraints.\n\nProposal creation requires a minimum token deposit to prevent spam. The proposer submits: title, description link, 和 executable 指令 (typically base64 serialized). Deposits are returned if the proposal passes, forfeited if it fails (depending on DAO configuration).\n\nVoting power is typically determined by token balance at a specific snapshot block. Some DAOs use vote escrow (veToken) models where locking tokens 用于 longer periods multiplies voting power. Quorum requirements ensure sufficient participation - a proposal needs both majority approval 和 minimum participation to pass.\n\nExecution safety involves timelocks between approval 和 execution. This delay (often 1-7 days) allows users to exit if they disagree 使用 the outcome. Emergency powers may exist 用于 critical fixes but should require higher thresholds.\n\n## 治理 reliability rule\n\nA proposal system is only credible if outcomes are reproducible from public inputs. That means deterministic vote math, explicit snapshot rules, clear timelock transitions, 和 auditable execution traces 用于 treasury effects.\n\n## Checklist\n- Understand the four-stage 治理 lifecycle\n- Know proposal deposit 和 spam prevention mechanisms\n- Calculate voting power 和 quorum requirements\n- Implement timelock safety delays\n- Plan 用于 emergency execution paths\n\n## Red flags\n- Allowing proposal creation without deposits\n- Missing quorum requirements 用于 participation\n- Zero timelock on sensitive operations\n- Unclear vote counting methodologies\n",
            "duration": "45 min"
          },
          "governance-v2-quorum-math": {
            "title": "Quorum math 和 vote weight calculation",
            "content": "# Quorum math 和 vote weight calculation\n\nAccurate vote counting is critical 用于 legitimate 治理 outcomes. Understanding quorum requirements, vote weight calculation, 和 edge cases ensures fair decision-making.\n\nQuorum defines minimum participation 用于 a valid vote. Common formulas include: absolute token amount (e.g., 4% of total supply must vote), relative to circulating supply, or dynamic based on recent participation. Quorum prevents small groups from making unilateral decisions.\n\nVote weight calculation considers: token balance at snapshot block, lockup duration multipliers (veToken model), delegation relationships, 和 abstention handling. Abstentions typically count toward quorum but not toward approval ratio.\n\nApproval thresholds vary by proposal type. Simple majority (>50%) is standard 用于 routine matters. Supermajority (e.g., 2/3) may be required 用于 constitutional changes. Some DAOs use quadratic voting to reduce whale influence, though this has sybil resistance challenges.\n\nEdge cases include: ties (usually fail), late vote changes (often blocked after deadline), vote delegation revocation timing, 和 quorum manipulation (e.g., flash loan attacks prevented by snapshot blocks).\n\n## Checklist\n- Define clear quorum formulas 和 minimums\n- Calculate vote weights 使用 snapshot blocks\n- Handle abstentions appropriately\n- Set appropriate approval thresholds by proposal type\n- Protect against manipulation attacks\n\n## Red flags\n- No quorum requirements\n- Vote weight based on current balance (flash loan risk)\n- Unclear tie-breaking rules\n- Changing rules mid-proposal\n",
            "duration": "50 min"
          },
          "governance-v2-timelocks": {
            "title": "Timelock states 和 execution scheduling",
            "content": "# Timelock states 和 execution scheduling\n\nTimelocks provide a critical safety layer between 治理 approval 和 execution. Understanding timelock states 和 transitions ensures reliable proposal execution.\n\nTimelock states include: pending (proposal passed, waiting 用于 delay), ready (delay elapsed, can be executed), executed (指令 processed), cancelled (withdrawn by proposer or guardian), 和 expired (execution window passed). Each state has valid transitions 和 authorized actors.\n\nDelay configuration balances 安全 使用 responsiveness. Too short (hours) allows insufficient reaction time. Too long (weeks) delays urgent fixes. Common ranges are 1-7 days, 使用 shorter delays 用于 routine matters 和 longer 用于 significant changes.\n\nExecution windows prevent indefinite pending states. After the timelock delay, proposals typically have a limited window (e.g., 7-14 days) to be executed. Expired proposals must be re-proposed 和 re-voted.\n\nCancellations add flexibility. Proposers may withdraw proposals before voting ends. Guardians (if configured) may cancel malicious proposals. Cancellation typically returns deposits unless abuse is detected.\n\n## Checklist\n- Define clear timelock state machine\n- Set appropriate delays by proposal type\n- Implement execution window limits\n- Authorize cancellation actors\n- Handle state transitions atomically\n\n## Red flags\n- No execution window limits\n- Missing cancellation mechanisms\n- State transitions without authorization checks\n- Indefinite pending states\n",
            "duration": "45 min"
          },
          "governance-v2-quorum-voting": {
            "title": "Challenge: Implement quorum/voting state machine",
            "content": "# Challenge: Implement quorum/voting state machine\n\nBuild a deterministic voting system:\n\n- Calculate vote weights from token balances\n- Check quorum requirements\n- Determine pass/fail based on thresholds\n- Handle abstentions correctly\n- Return stable state transitions\n\nYour implementation will be tested against various vote distributions.",
            "duration": "50 min"
          }
        }
      },
      "governance-v2-multisig": {
        "title": "Multisig Treasury",
        "description": "Multisig 交易 construction, approval controls, replay defenses, 和 secure treasury execution patterns.",
        "lessons": {
          "governance-v2-multisig": {
            "title": "Multisig 交易 building 和 approvals",
            "content": "# Multisig 交易 building 和 approvals\n\nMultisig 钱包 provide collective control over treasury funds. Understanding multisig construction, approval flows, 和 安全 patterns is essential 用于 treasury operations.\n\nMultisig structure defines: signers (public keys that can approve), threshold (minimum signatures required), 和 指令 (operations to execute). Common configurations include 2-of-3 (two approvals from three signers), 3-of-5, 和 custom arrangements.\n\n交易 lifecycle: propose (one signer creates 交易 使用 指令), approve (other signers review 和 approve), 和 execute (once threshold is met, anyone can execute). Each stage is recorded on-chain 用于 transparency.\n\nApproval tracking maintains state per signer per 交易. Signers can approve, reject, or cancel their approval. Double-signing is prevented by tracking which signers have already approved. Rejections may block 交易 or simply be recorded.\n\n安全 considerations: signer key management (hardware 钱包 recommended), threshold selection (balance 安全 vs availability), timelocks 用于 large transfers, 和 emergency recovery paths. Lost signer keys should not freeze treasury funds permanently.\n\n## Checklist\n- Define signer set 和 threshold\n- Track per-signer approval state\n- Enforce threshold before execution\n- Implement approval/revocation mechanics\n- Plan 用于 lost key scenarios\n\n## Red flags\n- Single signer controlling treasury\n- No approval tracking on-chain\n- Threshold equal to signer count (no redundancy)\n- Missing rejection/cancellation mechanisms\n",
            "duration": "50 min"
          },
          "governance-v2-multisig-builder": {
            "title": "Challenge: Implement multisig tx builder + approval rules",
            "content": "# Challenge: Implement multisig tx builder + approval rules\n\nBuild a multisig 交易 system:\n\n- Create 交易 使用 指令\n- Record signer approvals\n- Enforce threshold requirements\n- Handle approval revocation\n- Generate deterministic 交易 state\n\nTests will verify threshold enforcement 和 approval tracking.",
            "duration": "55 min"
          },
          "governance-v2-safe-defaults": {
            "title": "Safe defaults: owner checks 和 replay guards",
            "content": "# Safe defaults: owner checks 和 replay guards\n\n治理 和 multisig systems require robust 安全 defaults. Understanding common vulnerabilities 和 their mitigations protects treasury funds.\n\nOwner checks validate that 交易 only affect authorized 账户. 用于 treasury operations, verify: the treasury 账户 is owned by the expected program, the signer set matches the multisig configuration, 和 指令 target allowed programs. Missing owner checks enable 账户 substitution attacks.\n\nReplay guards prevent the same approved 交易 from being executed multiple times. Without replay protection, an observer could resubmit an executed 交易 to drain funds. Guards include: unique 交易 nonces, executed flags in 交易 state, 和 signature uniqueness checks.\n\nUpgrade safety considers how 治理 changes affect existing proposals. If the multisig configuration changes, pending proposals should use the old configuration while new proposals use the new one. Atomic configuration changes prevent partial updates.\n\nEmergency stops provide circuit breakers. Guardian roles can pause operations during suspected attacks. Time delays on critical changes allow review periods. These safety valves should be tested regularly.\n\n## Checklist\n- Validate 账户 ownership before operations\n- Implement replay protection (nonces or flags)\n- Handle configuration changes safely\n- Add emergency pause mechanisms\n- Test 安全 controls regularly\n\n## Red flags\n- No owner verification on treasury 账户\n- Missing replay protection\n- Immediate execution of critical changes\n- No emergency stop capability\n",
            "duration": "45 min"
          },
          "governance-v2-treasury-execution": {
            "title": "Challenge: Execute proposal 和 produce treasury diff",
            "content": "# Challenge: Execute proposal 和 produce treasury diff\n\nComplete the 治理 simulator checkpoint:\n\n- Execute approved proposals 使用 timelock validation\n- Apply treasury state changes atomically\n- Generate execution trace 使用 before/after diffs\n- Handle edge cases (expired, cancelled, insufficient funds)\n- Output stable, deterministic audit log\n\nThis validates your complete 治理/multisig implementation.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "solana-performance": {
    "title": "Solana 性能 & Compute Optimization",
    "description": "Master Solana 性能 engineering 使用 measurable optimization workflows: compute budgets, data layouts, encoding efficiency, 和 deterministic cost modeling.",
    "duration": "11 hours",
    "tags": [
      "performance",
      "optimization",
      "compute",
      "serialization",
      "solana"
    ],
    "modules": {
      "performance-v2-foundations": {
        "title": "性能 Foundations",
        "description": "Compute model, 账户/data layout decisions, 和 deterministic cost estimation 用于 交易-level 性能 reasoning.",
        "lessons": {
          "performance-v2-compute-model": {
            "title": "Compute model: budgets, costs, 和 limits",
            "content": "# Compute model: budgets, costs, 和 limits\n\nSolana's compute model enforces deterministic execution limits through compute budgets. Understanding this model is essential 用于 building efficient programs that stay within limits while maximizing utility.\n\nCompute units (CUs) measure execution cost. Every operation consumes CUs: 指令 execution, syscall usage, memory access, 和 logging. The default 交易 limit is 200,000 CUs (1.4 million 使用 prioritization), 和 each 账户 has a 10MB max size limit.\n\nCompute budget 指令 allow 交易 to request specific limits 和 set priority fees. The ComputeBudgetProgram provides: setComputeUnitLimit (override default), setComputeUnitPrice (set priority fee per CU in micro-lamports). Priority fees increase 交易 inclusion probability during congestion.\n\nCost categories include: fixed costs (signature verification, 账户 loading), variable costs (指令 execution, CPI calls), 和 memory costs (账户 data access size). Understanding these categories helps optimize the right areas.\n\nMetering happens during execution. If a 交易 exceeds its compute budget, execution halts 和 the 交易 fails 使用 an error. Failed 交易 still pay fees 用于 consumed CUs, making optimization economically important.\n\n## 实战 optimization loop\n\nUse a repeatable loop:\n1. profile real CU usage,\n2. identify top cost drivers (data layout, CPI count, logging),\n3. optimize one hotspot,\n4. re-measure 和 keep only proven wins.\n\nThis avoids 性能 folklore 和 keeps code quality intact.\n\n## Checklist\n- Understand compute unit consumption categories\n- Use ComputeBudgetProgram 用于 specific limits\n- Set priority fees during congestion\n- Monitor CU usage during development\n- Handle compute limit failures gracefully\n\n## Red flags\n- Ignoring compute limits in program 设计\n- Using default limits unnecessarily high\n- Not 测试 使用 realistic data sizes\n- Missing priority fee strategies 用于 important 交易\n",
            "duration": "45 min"
          },
          "performance-v2-account-layout": {
            "title": "账户 layout 设计 和 serialization cost",
            "content": "# 账户 layout 设计 和 serialization cost\n\n账户 data layout significantly impacts compute costs. Well-designed layouts minimize serialization overhead 和 reduce 账户 access costs.\n\nSerialization formats affect cost. Borsh is the standard 用于 Solana, offering compact binary encoding. Manual serialization can be more efficient 用于 simple structures but increases bug risk. Avoid JSON or other text formats on-chain due to size 和 parsing cost.\n\n账户 size impacts costs linearly. Loading a 10KB 账户 costs more than loading 1KB. Rent exemption requires more lamports 用于 larger 账户. 设计 layouts to minimize size: use fixed-size arrays instead of Vecs where possible, pack booleans into bitflags, 和 use appropriate integer sizes (u8/u16/u32/u64).\n\nData structure alignment affects both size 和 access patterns. Group related fields together 用于 cache efficiency. Place frequently accessed fields at the beginning of the struct. Consider zero-copy deserialization 用于 read-heavy operations.\n\nVersioning enables layout upgrades. Include a version byte at the start of 账户 data. Migration logic can then handle different versions during deserialization. Plan 用于 growth by reserving padding bytes in initial layouts.\n\n## Checklist\n- Use Borsh 用于 standard serialization\n- Minimize 账户 data size\n- Use appropriate integer sizes\n- Plan 用于 versioning 和 upgrades\n- Consider zero-copy 用于 read-heavy paths\n\n## Red flags\n- Using JSON 用于 on-chain data\n- Oversized Vec collections\n- No versioning 用于 upgrade paths\n- Unnecessary large integer types\n",
            "duration": "50 min"
          },
          "performance-v2-cost-model": {
            "title": "Challenge: Implement estimateCost(op) model",
            "content": "# Challenge: Implement estimateCost(op) model\n\nBuild a compute cost estimation system:\n\n- Model costs 用于 different operation types\n- 账户 用于 指令 complexity\n- Include memory access costs\n- Return baseline measurements\n- Handle edge cases (empty operations, large data)\n\nYour estimator will be validated against known operation costs.",
            "duration": "50 min"
          },
          "performance-v2-instruction-data": {
            "title": "指令 data size 和 encoding optimization",
            "content": "# 指令 data size 和 encoding optimization\n\n指令 data size directly impacts 交易 cost 和 throughput. Optimizing encoding reduces fees 和 increases the operations possible within compute limits.\n\nCompact encoding uses minimal bytes to represent data. Use discriminants (u8) to identify 指令 types. Use variable-length encoding (ULEB128) 用于 sizes. Pack multiple boolean flags into a single u8 using bit manipulation. Avoid unnecessary padding.\n\n账户 deduplication reduces 交易 size. If an 账户 appears in multiple 指令, include it once in the 账户 list 和 reference by index. This is especially important 用于 CPI-heavy 交易.\n\nBatching combines multiple operations into one 交易. Instead of N 交易 使用 1 指令 each, use 1 交易 使用 N 指令. Batching amortizes signature verification 和 账户 loading costs across operations.\n\nRight-sizing vectors prevents overallocation. Use Vec::with_capacity when the size is known. Avoid unnecessary clones that increase heap usage. Consider stack-allocated arrays 用于 small, fixed-size data.\n\n## Checklist\n- Use compact discriminants 用于 指令 types\n- Pack boolean flags into bitfields\n- Deduplicate 账户 across 指令\n- Batch operations when possible\n- Right-size collections to avoid waste\n\n## Red flags\n- Using full u32 用于 small discriminants\n- Separate booleans instead of bitflags\n- Duplicate 账户 in 交易 lists\n- Unnecessary data cloning\n",
            "duration": "45 min"
          }
        }
      },
      "performance-v2-optimization": {
        "title": "Optimization & Analysis",
        "description": "Layout optimization, compute budget tuning, 和 before/after 性能 analysis 使用 correctness safeguards.",
        "lessons": {
          "performance-v2-optimized-layout": {
            "title": "Challenge: Implement optimized layout/codec",
            "content": "# Challenge: Implement optimized layout/codec\n\nOptimize an 账户 data layout while preserving semantics:\n\n- Reduce data size through compact encoding\n- Maintain all original functionality\n- Preserve backward compatibility where possible\n- Pass regression tests\n- Measure 和 report size reduction\n\nYour optimized layout should be smaller but functionally equivalent.",
            "duration": "55 min"
          },
          "performance-v2-compute-budget": {
            "title": "Compute budget 指令 基础",
            "content": "# Compute budget 指令 基础\n\nCompute budget 指令 give developers control over resource allocation 和 交易 prioritization. Understanding these tools enables precise optimization.\n\nsetComputeUnitLimit requests a specific CU budget. The default is 200,000, but you can request up to 1,400,000. Requesting more than needed wastes fees since you pay 用于 the limit, not actual usage. Requesting too little causes failures.\n\nsetComputeUnitPrice sets a priority fee in micro-lamports per CU. During congestion, 交易 使用 higher priority fees are more likely to be included. Priority fees are additional to base fees 和 go to 验证者.\n\nRequesting compute units involves tradeoffs. Higher limits enable more complex operations but cost more. Priority fees increase inclusion probability but raise costs. Profile your 交易 to set appropriate limits.\n\nHeap size can also be configured. The default heap is 32KB, extendable to 256KB 使用 compute budget 指令. Large heap enables complex data structures but increases CU consumption.\n\n## Checklist\n- Profile 交易 to determine actual CU usage\n- Set appropriate compute unit limits\n- Use priority fees during congestion\n- Consider heap size 用于 data-heavy operations\n- Monitor cost vs inclusion probability tradeoffs\n\n## Red flags\n- Always using maximum compute unit limits\n- Not setting priority fees during congestion\n- Ignoring heap size constraints\n- Not profiling before optimization\n",
            "duration": "40 min"
          },
          "performance-v2-micro-optimizations": {
            "title": "Micro-optimizations 和 tradeoffs",
            "content": "# Micro-optimizations 和 tradeoffs\n\n性能 optimization involves balancing competing concerns. Understanding tradeoffs helps make informed decisions about when 和 what to optimize.\n\nReadability vs 性能 is a constant tension. Highly optimized code often sacrifices clarity. Optimize hot paths (frequently executed code) aggressively. Keep cold paths (rarely executed) readable 和 maintainable.\n\nSpace vs time tradeoffs appear frequently. Pre-computing values trades memory 用于 speed. Compressing data trades CPU 用于 storage. Choose based on which resource is more constrained 用于 your use case.\n\nMaintainability vs optimization matters 用于 long-term projects. Aggressive optimizations can introduce bugs 和 make updates difficult. Document why optimizations exist 和 measure their impact.\n\nPremature optimization is a common trap. Profile before optimizing to identify actual bottlenecks. Theoretical optimizations may not match real-world behavior. Focus on algorithmic improvements before micro-optimizations.\n\n安全 must never be sacrificed 用于 性能. Bounds checking, ownership validation, 和 arithmetic safety are non-negotiable. Optimize around 安全, not through it.\n\n## Checklist\n- Profile before optimizing\n- Focus on hot paths\n- Document optimization decisions\n- Balance readability 和 性能\n- Never sacrifice 安全 用于 speed\n\n## Red flags\n- Optimizing without profiling\n- Sacrificing 安全 用于 性能\n- Unreadable code without documentation\n- Optimizing cold paths unnecessarily\n",
            "duration": "45 min"
          },
          "performance-v2-perf-checkpoint": {
            "title": "Checkpoint: Compare before/after + output perf report",
            "content": "# Checkpoint: Compare before/after + output perf report\n\nComplete the optimization lab checkpoint:\n\n- Measure baseline 性能 metrics\n- Apply optimization techniques\n- Verify correctness is preserved\n- Generate 性能 comparison report\n- Output stable JSON 使用 sorted keys\n\nThis validates your ability to optimize while maintaining correctness.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "defi-swap-aggregator": {
    "title": "DeFi Swap Aggregation",
    "description": "Master production swap aggregation on Solana: deterministic quote parsing, route optimization tradeoffs, slippage safety, 和 reliability-aware execution.",
    "duration": "12 hours",
    "tags": [
      "defi",
      "swap",
      "aggregator",
      "jupiter",
      "solana"
    ],
    "modules": {
      "swap-v2-fundamentals": {
        "title": "Swap Fundamentals",
        "description": "Token swap mechanics, slippage protection, route composition, 和 deterministic swap plan construction 使用 transparent tradeoffs.",
        "lessons": {
          "swap-v2-mental-model": {
            "title": "Swap 思维模型: mints, ATAs, decimals, 和 routes",
            "content": "# Swap 思维模型: mints, ATAs, decimals, 和 routes\n\nToken swaps on Solana follow a fundamentally different model than centralized exchanges. Understanding the building blocks — mints, associated token 账户 (ATAs), decimal precision, 和 route composition — is essential before writing any swap code.\n\nEvery SPL token on Solana is defined by a mint 账户. The mint specifies the token's total supply, decimals (0–9), 和 authority. When you swap \"SOL 用于 USDC,\" you are actually swapping wrapped SOL (mint `So11111111111111111111111111111111111111112`) 用于 USDC (mint `EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v`). Native SOL must be wrapped into an SPL token before any program can manipulate it as a standard token.\n\nAssociated Token 账户 (ATAs) are deterministic addresses derived from a 钱包 和 a mint using the Associated Token 账户 program. 用于 every token a 钱包 holds, there must be an ATA. If the recipient does not have an ATA 用于 the output mint, the swap 交易 must include a `createAssociatedTokenAccountIdempotent` 指令 — a common source of 交易 failures when omitted.\n\nDecimal handling is critical. SOL uses 9 decimals (1 SOL = 1,000,000,000 lamports). USDC uses 6 decimals. When displaying \"22.5 USDC,\" the on-chain amount is 22,500,000. Mixing decimals between mints causes catastrophic pricing errors. Always convert human-readable amounts to raw integer amounts early 和 keep all math in integer space until the final display step.\n\nRoutes are the paths a swap takes through liquidity pools. A direct swap (SOL → USDC in a single pool) is the simplest case. When direct liquidity is insufficient or the price is better through an intermediary, the aggregator splits the swap into multiple \"legs\" — 用于 example, SOL → mSOL → USDC. Each leg passes through a different AMM (Automated Market Maker) program like Whirlpool, Raydium, or Orca. The aggregator's job is to find the combination of legs that produces the best output amount after fees.\n\nRoute optimization considers: pool liquidity depth, fee tiers, 价格影响 per leg, 和 the total compute cost of including multiple legs in one 交易. More legs means more 指令, more 账户, 和 higher compute unit consumption — there is a 实战 limit to route complexity within Solana's 交易 size 和 compute budget constraints.\n\n## Execution-quality triangle\n\nEvery route decision balances three competing goals:\n1. better output amount,\n2. lower failure risk (slippage + stale quote exposure),\n3. lower execution overhead (账户 + compute + latency).\n\nStrong aggregators make this tradeoff explicit rather than optimizing only a single metric.\n\n## Checklist\n- Identify input 和 output mints by their full base58 addresses\n- Ensure ATAs exist 用于 both input 和 output tokens before swapping\n- Convert all amounts to raw integer form using the correct decimal places\n- Understand that routes may have multiple legs through different AMM programs\n- Consider compute budget implications of complex routes\n\n## Red flags\n- Mixing decimal scales between different mints\n- Forgetting to create output ATA before the swap 指令\n- Assuming all swaps are single-hop direct routes\n- Ignoring fees charged by 中级 pools in multi-hop routes\n",
            "duration": "45 min"
          },
          "swap-v2-slippage": {
            "title": "Slippage 和 价格影响: protecting swap outcomes",
            "content": "# Slippage 和 价格影响: protecting swap outcomes\n\nSlippage is the difference between the expected output amount at quote time 和 the actual amount received at execution time. In volatile markets 使用 active trading, pool reserves change between when you request a quote 和 when your 交易 lands on-chain. Slippage protection ensures you never receive less than an acceptable minimum.\n\n价格影响 measures how much your swap moves the pool's price. A small swap in a deep liquidity pool has near-zero 价格影响. A large swap in a shallow pool can move the price significantly — you are effectively trading against yourself as each unit you buy makes the next unit more expensive. 价格影响 is calculated at quote time 和 should be displayed to users before they confirm.\n\nThe slippage tolerance is expressed in basis points (bps). 1 bps = 0.01%. A slippage of 50 bps means you accept up to 0.5% less than the quoted output. The minimum output amount is calculated as: minOutAmount = outAmount - (outAmount × slippageBps / 10000). This calculation MUST use integer arithmetic to avoid floating-point rounding errors. Using BigInt in JavaScript ensures exact computation.\n\nSetting slippage too tight (e.g., 1 bps) causes frequent 交易 failures because even minor pool changes exceed the tolerance. Setting it too loose (e.g., 1000 bps = 10%) exposes users to sandwich attacks where a malicious actor front-runs the swap to move the price, then back-runs after execution to profit from the price movement. The optimal range 用于 most swaps is 30–100 bps, 使用 higher values 用于 volatile or low-liquidity pairs.\n\nSandwich attacks exploit predictable slippage tolerances. An attacker observes your pending 交易 in the mempool, submits a 交易 to buy the output token (raising its price), lets your swap execute at the worse price, then sells the output token at profit. Tight slippage limits reduce the attacker's profit margin 和 may cause them to skip your 交易 entirely.\n\nDynamic slippage adjusts the tolerance based on: recent volatility, pool depth, swap size relative to pool reserves, 和 historical 交易 success rates. 高级 aggregators compute recommended slippage per-route to balance execution reliability 使用 protection. When building swap UIs, always show both the quoted output 和 the minimum guaranteed output so users understand their worst-case outcome.\n\n## Checklist\n- Calculate minOutAmount using integer arithmetic (BigInt)\n- Display both expected 和 minimum output amounts to users\n- Use 30–100 bps as default slippage 用于 most pairs\n- Show 价格影响 percentage prominently 用于 large swaps\n- Consider dynamic slippage based on pool conditions\n\n## Red flags\n- Using floating-point math 用于 slippage calculations\n- Setting extremely loose slippage (>500 bps) without user warning\n- Not displaying 价格影响 用于 large swaps\n- Ignoring sandwich attack vectors in slippage 设计\n",
            "duration": "50 min"
          },
          "swap-v2-route-explorer": {
            "title": "Route visualization: understanding swap legs 和 fees",
            "content": "# Route visualization: understanding swap legs 和 fees\n\nSwap routes reveal the path your tokens take through DeFi liquidity. Visualizing routes helps users understand why a multi-hop path might yield more output than a direct swap, 和 where fees are deducted along the way.\n\nA route consists of one or more legs. Each leg represents a swap through a specific AMM pool. The leg includes: the AMM program label (e.g., \"Whirlpool,\" \"Raydium\"), the input 和 output mints 用于 that leg, the fee charged by the pool (denominated in the output token), 和 the percentage of the total input allocated to this leg.\n\nSplit routes divide the input amount across multiple paths. 用于 example, 60% might go through Raydium SOL/USDC 和 40% through Orca SOL/USDC. Splitting across pools reduces 价格影响 because each pool handles a smaller portion of the total swap. The aggregator optimizes the split percentages to maximize total output.\n\nFee accounting is per-leg. Each AMM charges a fee (typically 0.01%–1% depending on the pool's fee tier). In concentrated liquidity pools, fee tiers are explicit (e.g., Orca's 1 bps, 4 bps, 30 bps, 100 bps tiers). The total fee across all legs determines the true cost of the route. A route 使用 lower per-leg fees might still be more expensive if it requires more hops.\n\nWhen rendering route information, show: the overall path (input mint → [中级 mints] → output mint), per-leg details (AMM, fee, percentage), total fees in the output token denomination, 价格影响 as a percentage, 和 the final output amounts (quoted 和 minimum). Color-coding or progress indicators help users quickly understand whether a route is simple (green, single hop) or complex (yellow/orange, multi-hop).\n\nEffective price is calculated as: outputAmount / inputAmount, denominated in output-per-input units. 用于 SOL → USDC, this gives the effective USD price of SOL 用于 this specific swap. Comparing the effective price against oracle or market price reveals the total cost of the swap including all fees 和 价格影响. This \"execution cost\" metric is the most honest summary of swap quality.\n\nRoute caching 和 expiration matter 用于 UX. Quotes from aggregators have a limited validity window (typically 10–30 seconds). If the user takes too long to confirm, the quote expires 和 the route must be re-fetched. The UI should clearly indicate quote freshness 和 automatically re-quote when expired. Stale quotes that execute against current pool state will likely fail or produce worse outcomes.\n\n## Checklist\n- Show each leg 使用 AMM label, mints, fee, 和 split percentage\n- Display total fees summed across all legs\n- Calculate 和 display effective price (output/input)\n- Indicate quote expiration time to users\n- Color-code routes by complexity (hops count)\n\n## Red flags\n- Hiding fees from the user display\n- Not showing 价格影响 用于 large swaps\n- Allowing execution of expired quotes\n- Displaying only the best-case output without minimum\n",
            "duration": "45 min"
          },
          "swap-v2-swap-plan": {
            "title": "Challenge: Build a normalized SwapPlan from a quote",
            "content": "# Challenge: Build a normalized SwapPlan from a quote\n\nParse a raw aggregator quote response 和 produce a normalized SwapPlan:\n\n- Extract input/output mints 和 amounts from the quote\n- Calculate minOutAmount using BigInt slippage arithmetic\n- Map each route leg to a normalized structure 使用 AMM label, mints, fees, 和 percentage\n- Handle zero slippage correctly (minOut equals outAmount)\n- Ensure all amounts remain as string representations of integers\n\nYour SwapPlan must be fully deterministic — same input always produces same output.",
            "duration": "50 min"
          }
        }
      },
      "swap-v2-execution": {
        "title": "Execution & Reliability",
        "description": "State-machine execution, 交易 anatomy, retry/staleness reliability patterns, 和 high-signal swap run reporting.",
        "lessons": {
          "swap-v2-state-machine": {
            "title": "Challenge: Implement swap UI state machine",
            "content": "# Challenge: Implement swap UI state machine\n\nBuild a deterministic state machine 用于 the swap UI flow:\n\n- States: idle → quoting → ready → sending → confirming → success | error\n- Process a sequence of events 和 track all state transitions\n- Invalid transitions should move to the error state 使用 a descriptive message\n- The error state supports RESET (back to idle) 和 RETRY (back to quoting)\n- Track transition history as an array of {from, event, to} objects\n\nThe state machine must be fully deterministic — same event sequence always produces same result.",
            "duration": "45 min"
          },
          "swap-v2-tx-anatomy": {
            "title": "Swap 交易 anatomy: 指令, 账户, 和 compute",
            "content": "# Swap 交易 anatomy: 指令, 账户, 和 compute\n\nA swap 交易 on Solana is a carefully ordered sequence of 指令 that together achieve an atomic token exchange. Understanding each 指令's role, the 账户 list requirements, 和 compute budget considerations is essential 用于 building reliable swap flows.\n\nThe typical swap 交易 contains these 指令 in order: (1) Compute Budget: SetComputeUnitLimit 和 SetComputeUnitPrice to ensure the 交易 has enough compute 和 appropriate priority. (2) Create ATA (if needed): createAssociatedTokenAccountIdempotent 用于 the output token if the user doesn't already have one. (3) Wrap SOL (if input is native SOL): transfer SOL to a temporary WSOL 账户 和 sync its balance. (4) Swap 指令(s): the actual AMM program calls that execute the swap, referencing all required pool 账户. (5) Unwrap WSOL (if output is native SOL): close the temporary WSOL 账户 和 recover SOL.\n\n账户 requirements scale 使用 route complexity. A single-hop swap through Whirlpool requires approximately 12–15 账户 (user 钱包, token 账户, pool state, oracle, tick arrays, etc.). A multi-hop route through two different AMMs can require 25+ 账户, pushing against the 交易 size limit. Address Lookup Tables (ALTs) mitigate this by compressing 账户 references from 32 bytes to 1 byte each, but require a separate setup 交易.\n\nCompute budget estimation is critical. A simple SOL → USDC Whirlpool swap uses roughly 80,000–120,000 compute units. Multi-hop routes can use 200,000–400,000+. Setting the compute limit too low causes the 交易 to fail. Setting it too high wastes the user's priority fee budget (priority fee = CU price × CU limit). Aggregators typically provide a recommended compute unit limit per route.\n\nPriority fees determine 交易 ordering. During high-demand periods (popular mints, volatile markets), 交易 compete 用于 block space. The priority fee (in micro-lamports per compute unit) determines where your 交易 lands in the leader's queue. Too low 和 the 交易 may not be included; too high 和 the user overpays. Dynamic priority fee estimation uses recent block data to suggest competitive rates.\n\n交易 simulation before submission catches many errors: insufficient balance, missing 账户, compute budget exceeded, slippage exceeded. Simulating saves the user from paying 交易 fees on doomed 交易. The simulation result includes compute units consumed, log messages, 和 any error codes — all useful 用于 debugging.\n\nVersioned 交易 (v0) are required when using Address Lookup Tables. Legacy 交易 cannot reference ALTs. Most modern swap aggregators return versioned 交易 messages. 钱包 must support versioned 交易 signing (most do, but some older 钱包 adapters may not).\n\n## Checklist\n- Include compute budget 指令 at the start of the 交易\n- Create output ATA if it doesn't exist\n- Handle SOL wrapping/unwrapping 用于 native SOL swaps\n- Simulate 交易 before submission\n- Use versioned 交易 when ALTs are needed\n\n## Red flags\n- Omitting compute budget 指令 (uses default 200k limit)\n- Not creating output ATA before the swap 指令\n- Forgetting to unwrap WSOL after receiving native SOL output\n- Skipping simulation 和 sending potentially invalid 交易\n",
            "duration": "50 min"
          },
          "swap-v2-reliability": {
            "title": "Reliability patterns: retries, stale quotes, 和 latency",
            "content": "# Reliability patterns: retries, stale quotes, 和 latency\n\nProduction swap flows must handle the reality of network latency, expired quotes, 和 交易 failures. Reliability engineering separates toy swap implementations from production-grade systems that users trust 使用 real money.\n\nQuote staleness is the primary reliability challenge. An aggregator quote reflects pool state at a specific moment. By the time the user reviews the quote, signs the 交易, 和 it lands on-chain, pool reserves may have changed significantly. A quote older than 10–15 seconds should be considered potentially stale. The UI should show a countdown timer 和 automatically re-quote when the timer expires. Never allow users to send 交易 based on quotes older than 30 seconds.\n\nRetry strategies must distinguish between retryable 和 non-retryable failures. Retryable: network timeout, RPC node temporarily unavailable, blockhash expired (re-fetch 和 re-sign), 和 rate limiting (429 responses, back off exponentially). Non-retryable: insufficient balance, invalid 账户 state, slippage exceeded (pool price moved too far, re-quote required), 和 program errors indicating invalid 指令 data.\n\nExponential backoff 使用 jitter prevents retry storms. Base delay of 500ms, multiplied by 2 on each retry, 使用 random jitter of ±25% to prevent synchronized retries from multiple clients. Cap retries at 3–5 attempts. If all retries fail, present a clear error message 使用 actionable options: \"Quote expired — refresh 和 try again\" rather than generic \"交易 failed.\"\n\nBlockhash management affects reliability. A 交易's blockhash must be recent (within ~60 seconds / 150 slots). If a 交易 fails 和 you retry, the blockhash may have expired. The retry flow must: get a fresh blockhash, rebuild the 交易 使用 the new blockhash, re-sign, 和 re-submit. This is why swap 交易 building should be a reusable function that accepts a blockhash parameter.\n\nLatency budgets help set user expectations. Typical Solana 交易 confirmation takes 400ms–2 seconds. However, during congestion, confirmation can take 5–30 seconds or fail entirely. The UI should show progressive states: \"Submitting...\" → \"Confirming...\" 使用 block confirmations. After 30 seconds without confirmation, offer the user a choice: wait longer, retry, or cancel (note: you cannot cancel a submitted 交易, but you can stop polling 和 let the blockhash expire).\n\n交易 status polling should use WebSocket subscriptions (signatureSubscribe) 用于 real-time confirmation rather than polling getTransaction. Polling creates unnecessary RPC load 和 introduces latency. Subscribe immediately after sendTransaction returns a signature, 和 set a timeout 用于 maximum wait time.\n\n## Checklist\n- Show quote freshness countdown 和 auto-refresh\n- Classify errors as retryable vs non-retryable\n- Use exponential backoff 使用 jitter 用于 retries\n- Get fresh blockhash on each retry attempt\n- Use WebSocket subscriptions 用于 confirmation\n\n## Red flags\n- Retrying non-retryable errors (wastes time 和 fees)\n- No retry limit (infinite retry loops)\n- Sending 交易 使用 stale quotes (>30 seconds)\n- Polling getTransaction instead of subscribing\n",
            "duration": "45 min"
          },
          "swap-v2-swap-report": {
            "title": "Checkpoint: Generate a SwapRunReport",
            "content": "# Checkpoint: Generate a SwapRunReport\n\nBuild the final swap run report that combines all 课程 concepts:\n\n- Summarize the route 使用 leg details 和 total fees (using BigInt summation)\n- Compute the effective price as outAmount / inAmount (9 decimal precision)\n- Include the state machine outcome (finalState from the UI flow)\n- Collect all errors from the state result 和 additional error sources\n- Output must be stable JSON 使用 deterministic key ordering\n\nThis checkpoint validates your complete understanding of swap aggregation.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "defi-clmm-liquidity": {
    "title": "CLMM Liquidity Engineering",
    "description": "Master concentrated liquidity engineering on Solana DEXs: tick math, range strategy 设计, fee/IL dynamics, 和 deterministic LP position reporting.",
    "duration": "14 hours",
    "tags": [
      "defi",
      "clmm",
      "liquidity",
      "orca",
      "solana"
    ],
    "modules": {
      "clmm-v2-fundamentals": {
        "title": "CLMM Fundamentals",
        "description": "Concentrated liquidity concepts, tick/price math, 和 range-position behavior needed to reason about CLMM execution.",
        "lessons": {
          "clmm-v2-vs-cpmm": {
            "title": "CLMM vs constant product: why ticks exist",
            "content": "# CLMM vs constant product: why ticks exist\n\nConcentrated Liquidity Market Makers (CLMMs) represent a fundamental evolution in automated market maker 设计. To understand why they exist, we must first understand the limitations of the constant product model 和 then examine how tick-based systems solve those problems on Solana.\n\n## The constant product model 和 its inefficiency\n\nThe original AMM 设计, popularized by Uniswap V2 和 adopted by Raydium V1 on Solana, uses the constant product invariant: x * y = k, where x 和 y are the reserves of two tokens 和 k is a constant. When a trader swaps token A 用于 token B, the product must remain unchanged. This creates a smooth price curve that spans the entire range from zero to infinity.\n\nThe problem 使用 this approach is capital inefficiency. If a SOL/USDC pool holds $10 million in liquidity, 和 SOL trades between $20 和 $30 用于 months, the vast majority of that capital sits idle. Liquidity allocated to price ranges below $1 or above $1000 never participates in trades, earns no fees, yet still dilutes the returns 用于 liquidity providers (LPs). In practice, studies show that less than 5% of liquidity in constant product pools is actively used at any given time.\n\n## Concentrated liquidity: the core insight\n\nCLMMs, pioneered by Uniswap V3 和 implemented on Solana by Orca Whirlpools, Raydium Concentrated Liquidity, 和 Meteora DLMM, allow LPs to allocate capital to specific price ranges. Instead of spreading liquidity across all possible prices, an LP can say: \"I want to provide liquidity only between $20 和 $30 用于 SOL/USDC.\" This concentrates their capital where trades actually happen, dramatically increasing capital efficiency.\n\nThe capital efficiency gain is substantial. An LP providing concentrated liquidity in a narrow range can achieve the same depth as a constant product LP 使用 100x or even 4000x less capital, depending on how tight the range is. This means more fees earned per dollar deployed, which is the fundamental value proposition of CLMMs.\n\n## Why ticks exist\n\nTo implement concentrated liquidity, the price space must be discretized. CLMMs divide the continuous price curve into discrete points called ticks. Each tick represents a specific price, 和 the relationship between tick index 和 price follows the formula: price = 1.0001^tick. This means each tick represents a 0.01% (1 basis point) change in price from the adjacent tick.\n\nTicks serve several critical purposes. First, they provide the boundaries 用于 liquidity positions. When an LP creates a position from tick -1000 to tick 1000, they are defining a price range. Second, ticks are where liquidity transitions happen. As the price crosses a tick boundary, the active liquidity changes because positions that start or end at that tick become active or inactive. Third, ticks enable efficient fee tracking, because the protocol only needs to track fee growth at tick boundaries rather than at every possible price.\n\nTick spacing is an important optimization. Not every tick is usable in every pool. Pools 使用 higher fee tiers use wider tick spacing (e.g., 64 or 128 ticks apart) to reduce gas costs 和 state size. A pool 使用 tick spacing of 64 means LPs can only place position boundaries at tick indices that are multiples of 64. This tradeoff reduces granularity but improves on-chain efficiency, which is especially important on Solana where 账户 sizes 和 compute units matter.\n\n## Solana-specific CLMM considerations\n\nOn Solana, CLMMs face unique architectural challenges. The 账户模型 requires pre-allocated tick arrays that store tick data in contiguous ranges. Orca Whirlpools, 用于 example, uses tick array 账户 that each hold 88 ticks worth of data. The program must load the correct tick array 账户 as 指令, which means swaps that cross many ticks require more 账户 和 more compute units.\n\nThe Solana runtime's 1232-byte 交易 size limit 和 200,000 compute unit default also constrain CLMM operations. Large swaps that cross multiple tick boundaries may need to be split across multiple 交易, 和 position management operations must be carefully optimized to fit within these constraints.\n\n## LP decision framework\n\nBefore opening any CLMM position, answer three questions:\n1. What price regime am I targeting (mean-reverting vs trending)?\n2. How actively can I rebalance when out-of-range?\n3. What failure budget can I tolerate 用于 fees vs IL vs rebalance costs?\n\nCLMM returns come from strategy discipline, not just math formulas.\n\n## Checklist\n- Understand that x*y=k spreads liquidity across all prices, wasting capital\n- CLMMs let LPs concentrate capital in specific price ranges\n- Ticks discretize the price space at 1 basis point intervals\n- Tick spacing varies by pool fee tier 用于 on-chain efficiency\n- Solana CLMMs use tick array 账户 用于 state management\n\n## Red flags\n- Assuming CLMM positions behave like constant product positions\n- Ignoring tick spacing when placing position boundaries\n- Underestimating compute costs 用于 swaps crossing many ticks\n- Forgetting that out-of-range positions earn zero fees\n",
            "duration": "50 min"
          },
          "clmm-v2-price-tick": {
            "title": "Price, tick, 和 sqrtPrice: core conversions",
            "content": "# Price, tick, 和 sqrtPrice: core conversions\n\nThe mathematical foundation of every CLMM rests on three interrelated representations of price: the human-readable price, the tick index, 和 the sqrtPriceX64. Understanding how to convert between these representations is essential 用于 building any CLMM integration on Solana.\n\n## Tick to price conversion\n\nThe fundamental relationship between a tick index 和 price is: price = 1.0001^tick. This formula means that each consecutive tick represents a 0.01% (1 basis point) change in price. Tick 0 corresponds to a price of 1.0. Positive ticks yield prices greater than 1, 和 negative ticks yield prices less than 1.\n\n用于 example, tick 23027 gives a price of approximately 10.0 (since 1.0001^23027 is roughly 10). Tick -23027 gives approximately 0.1. This logarithmic spacing means ticks provide consistent relative precision across all price levels. Whether the price is 0.001 or 1000, adjacent ticks always differ by 0.01%.\n\nThe inverse conversion from price to tick uses the natural logarithm: tick = ln(price) / ln(1.0001). Since tick indices must be integers, this value is typically rounded to the nearest integer. In practice, you also need to align the tick to the pool's tick spacing, which means rounding down to the nearest multiple of the tick spacing value.\n\n## The sqrtPrice representation\n\nCLMMs do not store price directly on-chain. Instead, they store the square root of the price in a fixed-point format called sqrtPriceX64. This representation has two important advantages.\n\nFirst, using the square root simplifies the core AMM math. The amount of token0 in a position is proportional to (1/sqrtPrice_lower - 1/sqrtPrice_upper), 和 the amount of token1 is proportional to (sqrtPrice_upper - sqrtPrice_lower). These linear relationships are much easier to compute on-chain than the original price-based formulas would be.\n\nSecond, the X64 fixed-point format (also called Q64.64) provides high precision without floating-point arithmetic. The sqrtPrice is multiplied by 2^64 和 stored as a 128-bit unsigned integer. This means sqrtPriceX64 = sqrt(price) * 2^64. 用于 tick 0 (price = 1.0), the sqrtPriceX64 is exactly 2^64 = 18446744073709551616.\n\nOn Solana, Orca Whirlpools stores this value as a u128 in the Whirlpool 账户 state. Every swap operation updates this value as the price moves. The sqrt_price field is the canonical source of truth 用于 the current pool price.\n\n## Decimal handling 和 token precision\n\nReal-world tokens have different decimal places. SOL has 9 decimals, USDC has 6 decimals. The tick-to-price formula gives a \"raw\" price that must be adjusted 用于 decimals. If token0 is SOL (9 decimals) 和 token1 is USDC (6 decimals), the human-readable price is: display_price = raw_price * 10^(decimals0 - decimals1) = raw_price * 10^(9-6) = raw_price * 1000.\n\nThis decimal adjustment is critical 和 a common source of bugs. Always track which token is token0 和 which is token1 in the pool, 和 apply the correct decimal scaling when converting between on-chain tick values 和 display prices.\n\n## Tick spacing 和 alignment\n\nNot every tick index is a valid position boundary. Each pool has a tick_spacing parameter that determines which ticks can be used. Common values are: 1 (用于 stable pairs 使用 0.01% fee), 8 (用于 0.04% fee pools), 64 (用于 0.30% fee pools), 和 128 (用于 1.00% fee pools).\n\nTo align a tick to the pool's tick spacing, use: aligned_tick = floor(tick / tick_spacing) * tick_spacing. This always rounds toward negative infinity, ensuring consistent behavior 用于 both positive 和 negative ticks. 用于 example, 使用 tick spacing 64: tick 100 aligns to 64, tick -100 aligns to -128.\n\n## Precision considerations\n\nFloating-point arithmetic introduces rounding errors in tick/price conversions. When converting price to tick 和 back, the result may differ by 1 tick due to floating-point precision limits. 用于 on-chain operations, always use the integer tick index as the source of truth 和 derive the price from it, never the reverse.\n\nThe sqrtPriceX64 computation using BigInt avoids floating-point issues 用于 the final representation, but the 中级 sqrt 和 pow operations still use JavaScript's 64-bit floats. 用于 production systems processing large values, consider using dedicated decimal libraries or performing these computations 使用 higher-precision arithmetic.\n\n## Checklist\n- price = 1.0001^tick 用于 tick-to-price conversion\n- tick = round(ln(price) / ln(1.0001)) 用于 price-to-tick conversion\n- sqrtPriceX64 = BigInt(round(sqrt(price) * 2^64))\n- Align ticks to tick spacing: floor(tick / spacing) * spacing\n- Adjust 用于 token decimals when displaying human-readable prices\n\n## Red flags\n- Ignoring decimal differences between token0 和 token1\n- Using floating-point price as source of truth instead of tick index\n- Forgetting tick spacing alignment when creating positions\n- Overflow in sqrtPriceX64 computation 用于 extreme tick values\n",
            "duration": "50 min"
          },
          "clmm-v2-range-explorer": {
            "title": "Range positions: in-range 和 out-of-range dynamics",
            "content": "# Range positions: in-range 和 out-of-range dynamics\n\nA CLMM position is defined by its lower tick 和 upper tick. These two boundaries determine the price range in which the position is active, earns fees, 和 holds a mix of both tokens. Understanding in-range 和 out-of-range behavior is fundamental to managing concentrated liquidity effectively on Solana.\n\n## Anatomy of a range position\n\nWhen an LP creates a position on Orca Whirlpools (or any Solana CLMM), they specify three parameters: the lower tick index, the upper tick index, 和 the amount of liquidity to provide. The protocol then calculates how much of each token the LP must deposit based on the current price relative to the position's range.\n\nIf the current price is within the range (lower_tick <= current_tick <= upper_tick), the LP deposits both tokens. The ratio depends on where the current price sits within the range. If the price is near the lower bound, the position holds mostly token0. If near the upper bound, it holds mostly token1. This is the direct analog of how a constant product pool holds different ratios at different prices, but concentrated into the LP's chosen range.\n\n## In-range behavior\n\nWhen the current pool price is within a position's range, the position is in-range 和 actively participates in swaps. Every swap that moves the price within this range uses the position's liquidity 和 generates fees 用于 the LP.\n\nThe fee accrual mechanism works as follows: the pool tracks a global fee_growth value 用于 each token. When a swap occurs, the fee (e.g., 0.30% of the swap amount) is distributed proportionally across all in-range liquidity. Each position tracks its own fee_growth snapshot, 和 uncollected fees are the difference between the current global growth 和 the position's snapshot, multiplied by the position's liquidity.\n\nIn-range positions experience impermanent loss (IL) as the price moves. When the price moves up, the position converts token0 into token1 (selling token0 at higher prices). When the price moves down, it converts token1 into token0. This rebalancing is the source of IL, 和 it is more pronounced in CLMMs than in constant product pools because the liquidity is concentrated in a narrower range.\n\n## Out-of-range behavior\n\nWhen the price moves outside a position's range, the position becomes out-of-range. This has critical implications. The position stops earning fees entirely because it no longer participates in swaps. The position holds 100% of one token: if the price moved above the upper tick, the position holds entirely token1 (all token0 was sold as the price rose). If the price moved below the lower tick, the position holds entirely token0 (all token1 was sold as the price fell).\n\nAn out-of-range position is effectively a limit order that has been filled. If you set a range above the current price 和 the price rises through it, your token0 is converted to token1 at prices within your range. This property makes CLMMs useful 用于 implementing range orders 和 dollar-cost averaging strategies.\n\nOut-of-range positions still exist on-chain 和 can be closed or modified at any time. The LP can withdraw their single-sided holdings, or they can wait 用于 the price to return to their range. If the price returns, the position automatically becomes active again 和 starts earning fees.\n\n## Position composition at boundaries\n\nAt the exact lower tick, the position holds 100% token0 和 0% token1. At the exact upper tick, it holds 0% token0 和 100% token1. At any price between, the composition is a function of where the current sqrtPrice sits relative to the range boundaries.\n\nThe token amounts are calculated as: amount0 = liquidity * (1/sqrtPrice_current - 1/sqrtPrice_upper) 和 amount1 = liquidity * (sqrtPrice_current - sqrtPrice_lower). These formulas only apply when the price is in-range. When out-of-range below, amount0 = liquidity * (1/sqrtPrice_lower - 1/sqrtPrice_upper) 和 amount1 = 0. When out-of-range above, amount0 = 0 和 amount1 = liquidity * (sqrtPrice_upper - sqrtPrice_lower).\n\n## Active liquidity 和 the liquidity curve\n\nThe pool's active liquidity at any given price is the sum of all in-range positions at that price. This creates a liquidity distribution curve that can have complex shapes depending on where LPs have placed their positions. Deeper liquidity at the current price means less slippage 用于 traders.\n\nOn Solana, this active liquidity is stored in the Whirlpool 账户's liquidity field 和 is updated whenever the price crosses a tick boundary where positions start or end. The tick array 账户 store the net liquidity change at each initialized tick, allowing the program to efficiently update active liquidity during swaps.\n\n## Checklist\n- In-range positions earn fees 和 hold both tokens\n- Out-of-range positions earn zero fees 和 hold one token\n- Token composition varies continuously within the range\n- Active liquidity is the sum of all in-range positions\n- Fee growth tracking uses global vs position-level snapshots\n\n## Red flags\n- Expecting fees from out-of-range positions\n- Ignoring the single-sided nature of out-of-range holdings\n- Forgetting to 账户 用于 IL in concentrated positions\n- Assuming position composition is static within a range\n",
            "duration": "45 min"
          },
          "clmm-v2-tick-math": {
            "title": "Challenge: Implement tick/price conversion helpers",
            "content": "# Challenge: Implement tick/price conversion helpers\n\nImplement the core tick math functions used in every CLMM integration:\n\n- Convert a tick index to a human-readable price using price = 1.0001^tick\n- Convert the price to sqrtPriceX64 using Q64.64 fixed-point encoding\n- Reverse-convert a price back to the nearest tick index\n- Align a tick index to the pool's tick spacing\n\nYour implementation will be tested against known tick values including tick 0, positive ticks, 和 negative ticks.",
            "duration": "50 min"
          }
        }
      },
      "clmm-v2-positions": {
        "title": "Positions & Risk",
        "description": "Fee accrual simulation, range strategy tradeoffs, precision pitfalls, 和 deterministic position risk reporting.",
        "lessons": {
          "clmm-v2-position-fees": {
            "title": "Challenge: Simulate position fee accrual",
            "content": "# Challenge: Simulate position fee accrual\n\nImplement a fee accrual simulator 用于 a CLMM position over a price path:\n\n- Convert lower 和 upper tick boundaries to prices\n- Walk through each price in the path 和 determine in-range or out-of-range status\n- Accrue fees proportional to trade volume when in-range\n- Compute annualized fee APR\n- Track periods in-range vs out-of-range\n- Determine current status from the final price\n\nThis simulates the real-world behavior of concentrated liquidity positions as prices move.",
            "duration": "50 min"
          },
          "clmm-v2-range-strategy": {
            "title": "Range strategies: tight, wide, 和 rebalancing rules",
            "content": "# Range strategies: tight, wide, 和 rebalancing rules\n\nChoosing the right price range is the most important decision a CLMM liquidity provider makes. The range determines capital efficiency, fee income, impermanent loss exposure, 和 rebalancing frequency. This 课时 covers the major strategies 和 the tradeoffs between them.\n\n## Tight ranges: maximum efficiency, maximum risk\n\nA tight range concentrates all liquidity into a narrow price band. 用于 example, providing liquidity 用于 SOL/USDC within +/- 2% of the current price. The advantages are significant: capital efficiency can be 100x or more compared to a full-range position, 和 the LP earns a proportionally larger share of trading fees.\n\nHowever, tight ranges carry substantial risks. The position goes out-of-range frequently, requiring active monitoring 和 rebalancing. Each time the position goes out-of-range, the LP has fully converted to one token 和 stops earning fees. The LP also realizes impermanent loss on each range crossing, 和 the gas costs of frequent rebalancing can eat into profits.\n\nTight ranges work best 用于 stable pairs (USDC/USDT) where the price rarely deviates significantly, 用于 professional LPs who can automate rebalancing, 和 用于 short-term positions where the LP has a strong directional view.\n\n## Wide ranges: passive 和 resilient\n\nA wide range covers a larger price band, such as +/- 50% or even the full price range. Capital efficiency is lower, but the position stays in-range longer 和 requires less active management. Fee income per dollar is lower, but the position earns fees more consistently.\n\nWide ranges suit passive LPs who cannot actively monitor positions, volatile pairs where the price can swing dramatically, 和 LPs who want to minimize rebalancing costs 和 IL realization events.\n\nThe extreme case is a full-range position covering all ticks. This replicates constant product AMM behavior 和 never goes out-of-range. While capital-inefficient, it provides maximum resilience 和 is appropriate 用于 very volatile or low-liquidity pairs.\n\n## Asymmetric ranges 和 directional bets\n\nLPs can create asymmetric ranges that express a directional view. If you believe SOL will appreciate against USDC, you might set a range from the current price up to 2x the current price. This means you are providing liquidity as SOL appreciates, selling SOL at progressively higher prices. If SOL drops, your position immediately goes out-of-range 和 you hold SOL, preserving your long exposure.\n\nConversely, a range set below the current price acts like a limit buy order. You deposit USDC, 和 if SOL's price drops into your range, your USDC is converted to SOL at your desired prices.\n\n## Rebalancing strategies\n\nRebalancing is the process of closing an out-of-range position 和 opening a new one centered on the current price. The key decisions are: when to rebalance, 和 how to set the new range.\n\nTime-based rebalancing checks the position at fixed intervals (hourly, daily) 和 rebalances if out-of-range. This is simple to implement but may miss optimal timing. Price-based rebalancing uses the current price relative to the range boundaries. A common trigger is rebalancing when the price exits the inner 50% of the range, before it actually goes out-of-range.\n\nThreshold-based rebalancing waits until the IL or missed-fee cost of remaining out-of-range exceeds the cost of rebalancing (gas fees, slippage on swaps needed to rebalance token composition). This is the most capital-efficient approach but requires sophisticated modeling.\n\nOn Solana, rebalancing a Whirlpool position involves three operations: collecting unclaimed fees, closing the old position (withdrawing liquidity 和 burning the position NFT), 和 opening a new position 使用 updated range. These operations typically fit in two to three 交易 depending on the number of 账户 involved.\n\n## Automated vault strategies\n\nSeveral protocols on Solana automate CLMM range management. These vault protocols (such as Kamino Finance) accept LP deposits 和 automatically create, monitor, 和 rebalance concentrated liquidity positions. They use various strategies including mean-reversion, momentum-following, 和 volatility-adjusted range widths.\n\nWhen evaluating automated vaults, consider: the strategy's historical 性能, the management 和 性能 fees, the rebalancing frequency 和 associated costs, 和 the vault's transparency about its position management logic.\n\n## Checklist\n- Tight ranges maximize efficiency but require active management\n- Wide ranges provide resilience at the cost of efficiency\n- Asymmetric ranges can express directional views\n- Rebalancing triggers: time-based, price-based, or threshold-based\n- Consider automated vaults 用于 passive management\n\n## Red flags\n- Using tight ranges without monitoring or automation\n- Rebalancing too frequently, losing fees to gas costs\n- Ignoring the realized IL at each rebalancing event\n- Assuming past APR will predict future returns\n",
            "duration": "50 min"
          },
          "clmm-v2-risk-review": {
            "title": "CLMM risks: rounding, overflow, 和 tick spacing errors",
            "content": "# CLMM risks: rounding, overflow, 和 tick spacing errors\n\nBuilding reliable CLMM integrations requires awareness of precision risks that can cause incorrect calculations, failed 交易, or lost funds. This 课时 catalogs the most common pitfalls in tick math, fee computation, 和 position management on Solana.\n\n## Floating-point rounding in tick conversions\n\nThe tick-to-price formula price = 1.0001^tick 和 its inverse tick = ln(price) / ln(1.0001) both involve floating-point arithmetic. JavaScript's Number type uses IEEE 754 double-precision (64-bit) floats, which provide approximately 15-17 significant decimal digits. 用于 most tick ranges (roughly -443636 to +443636, the valid CLMM range), this precision is sufficient.\n\nHowever, rounding errors accumulate in compound operations. Converting a tick to a price 和 back may yield tick +/- 1 due to floating-point rounding in the logarithm. The safest practice is to always treat the integer tick index as the canonical value. If you need a price, derive it from the tick. If you need a tick from a user-entered price, compute the tick 和 then show the user the exact price that tick represents, so they see the actual boundary rather than an approximation.\n\nThe Math.round function in the priceToTick conversion introduces its own edge cases. When the true tick is exactly X.5, Math.round uses \"round half to even\" (banker's rounding) in some environments. 用于 CLMM math, always round toward the nearest valid tick 和 then align to tick spacing.\n\n## Overflow in sqrtPriceX64 computation\n\nThe sqrtPriceX64 value is stored as a u128 on-chain (128-bit unsigned integer). In JavaScript, this must be handled 使用 BigInt. The 中级 computation sqrt(price) * 2^64 can overflow a 64-bit float 用于 extreme tick values. At the maximum valid tick (443636), the price is approximately 1.34 * 10^19, 和 sqrt(price) * 2^64 is approximately 6.75 * 10^28, which fits in a u128 but exceeds the safe integer range of JavaScript Numbers.\n\nAlways use BigInt 用于 the final sqrtPriceX64 value. 用于 中级 computations at extreme ticks, consider using a high-precision library or performing the computation in Rust (where Solana programs actually run). 用于 client-side JavaScript, the 实战 risk is manageable 用于 common token pairs but must be tested at boundary conditions.\n\n## Tick spacing alignment errors\n\nA frequent bug is creating positions 使用 tick boundaries that are not aligned to the pool's tick spacing. The on-chain program will reject these positions, but the error message may be cryptic. Always align ticks before submitting 交易: aligned = floor(tick / tickSpacing) * tickSpacing.\n\nBe careful 使用 negative ticks: floor(-100 / 64) = floor(-1.5625) = -2, so -100 aligns to -128, not -64. This is correct behavior (rounding toward negative infinity), but developers who expect truncation toward zero will get wrong results. Test 使用 negative ticks explicitly.\n\n## Fee computation precision\n\nFee growth values in CLMMs use 128-bit fixed-point arithmetic (Q64.64 or Q128.128 depending on the implementation). When computing uncollected fees, the formula is: uncollected_fees = (global_fee_growth - position_fee_growth_snapshot) * liquidity.\n\nBoth the subtraction 和 the multiplication can overflow if not handled carefully. On Solana, the program uses checked arithmetic 和 wrapping subtraction (since fee_growth is monotonically increasing 和 wraps around). Client-side code must replicate this wrapping behavior when reading on-chain state.\n\nA common mistake is computing fees 使用 JavaScript Numbers, which lose precision 用于 large BigInt values. Always use BigInt 用于 fee calculations 和 only convert to Number at the final display step, after applying decimal adjustments.\n\n## Decimal mismatch between tokens\n\nDifferent tokens have different decimal places (SOL: 9, USDC: 6, BONK: 5). When computing position values, token amounts, or fee amounts, the decimal places must be consistently applied. A common bug is computing IL in raw amounts without normalizing to the same decimal base, leading to wildly incorrect results.\n\nAlways track the decimal places of both tokens in the pool 和 apply them when converting between raw amounts 和 display amounts. The on-chain CLMM program operates entirely in raw (lamport-level) amounts; all decimal formatting is a client-side responsibility.\n\n## 账户 和 compute unit limits\n\nSolana-specific risks include exceeding compute unit limits during swaps that cross many ticks, requiring too many tick array 账户 (each swap can reference at most a few tick arrays), 和 账户 size limits 用于 position management.\n\nWhen building swap 交易, estimate the number of tick crossings 和 include sufficient tick array 账户. If a swap would cross more ticks than can be accommodated, the 交易 will fail. Splitting large swaps across multiple 交易 or using a routing protocol helps mitigate this risk.\n\n## Checklist\n- Use integer tick index as canonical, derive price from it\n- Use BigInt 用于 sqrtPriceX64 和 all fee computations\n- Always align ticks to tick spacing 使用 floor division\n- Test 使用 negative ticks, zero ticks, 和 extreme ticks\n- Apply correct decimal places 用于 each token in the pool\n\n## Red flags\n- Using JavaScript Number 用于 sqrtPriceX64 or fee amounts\n- Forgetting wrapping subtraction 用于 fee growth deltas\n- Truncating instead of flooring 用于 negative tick alignment\n- Computing IL or fees without matching token decimals\n",
            "duration": "45 min"
          },
          "clmm-v2-position-report": {
            "title": "Checkpoint: Generate a Position Report",
            "content": "# Checkpoint: Generate a Position Report\n\nImplement a comprehensive LP position report generator that combines all CLMM concepts:\n\n- Convert tick boundaries to human-readable prices\n- Determine in-range or out-of-range status from the current price\n- Aggregate fee history into total earned fees per token\n- Compute annualized fee APR\n- Calculate impermanent loss percentage\n- Return a complete, deterministic position report\n\nThis checkpoint validates your understanding of tick math, fee accrual, range dynamics, 和 position analysis.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "defi-lending-risk": {
    "title": "Lending & Liquidation Risk",
    "description": "Master Solana lending risk engineering: utilization 和 rate mechanics, liquidation path analysis, oracle safety, 和 deterministic scenario reporting.",
    "duration": "14 hours",
    "tags": [
      "defi",
      "lending",
      "liquidation",
      "risk",
      "solana"
    ],
    "modules": {
      "lending-v2-fundamentals": {
        "title": "Lending Fundamentals",
        "description": "Lending pool mechanics, utilization-driven rate models, 和 health-factor foundations required 用于 defensible risk analysis.",
        "lessons": {
          "lending-v2-pool-model": {
            "title": "Lending pool model: supply, borrow, 和 utilization",
            "content": "# Lending pool model: supply, borrow, 和 utilization\n\nLending protocols are the backbone of decentralized finance. They enable users to earn yield on idle assets by supplying them to a shared pool, while borrowers draw from that pool by posting collateral. Understanding the mechanics of supply, borrow, 和 utilization is essential before diving into interest rate models or liquidation logic.\n\nA lending pool is a smart contract (or set of 账户 on Solana) that holds a reserve of a single token — 用于 example, USDC. Suppliers deposit tokens into the pool 和 receive interest-bearing receipt tokens in return. On Solana-based protocols like Solend, MarginFi, or Kamino, these receipt tokens track each supplier's proportional share of the growing pool. When a supplier withdraws, they redeem receipt tokens 用于 the underlying asset plus accrued interest.\n\nBorrowers interact 使用 the same pool from the other side. To borrow from the USDC pool, a user must first deposit collateral into one or more other pools (用于 example, SOL). The protocol values the collateral in USD terms 和 allows the user to borrow up to a percentage of that value, determined by the loan-to-value (LTV) ratio. If SOL has an LTV of 75%, depositing $1,000 worth of SOL allows borrowing up to $750 in USDC. The borrowed amount accrues interest over time, increasing the user's debt.\n\nThe utilization ratio is the single most important metric in a lending pool. It is defined as:\n\nutilization = totalBorrowed / totalSupply\n\nwhere totalSupply is the sum of all deposits (including borrowed amounts that are still owed back to the pool). When utilization is 0%, no assets are borrowed — suppliers earn nothing. When utilization is 100%, every deposited asset is lent out — no supplier can withdraw because there is no liquidity available. Healthy protocols target utilization between 60% 和 85%, balancing yield 用于 suppliers against withdrawal liquidity.\n\nThe reserve factor is a protocol-level parameter that skims a percentage of the interest paid by borrowers before distributing the remainder to suppliers. If borrowers pay 10% annual interest 和 the reserve factor is 10%, the protocol retains 1% 和 suppliers receive the effective yield on the remaining 9%. Reserve funds are used 用于 protocol insurance, development, 和 治理 treasury. Understanding the reserve factor is critical because it directly reduces the supply-side APY relative to the borrow-side APR.\n\nPool accounting must be exact. Solana lending protocols typically use a shares-based model: when you deposit 100 USDC into a pool 使用 1,000 USDC total 和 1,000 shares outstanding, you receive 100 shares. As interest accrues, the total USDC in the pool grows (say to 1,100 USDC), but the share count remains 1,100. Your 100 shares are now worth 100 USDC — the value per share increased. This model avoids iterating over every depositor to distribute interest. The same pattern applies to borrow shares, tracking each borrower's proportional debt.\n\nOn Solana specifically, lending pools are represented as program-derived 账户. The reserve 账户 holds the token balance, a reserve config 账户 stores parameters (LTV, liquidation threshold, reserve factor, interest rate model), 和 individual obligation 账户 track each user's deposits 和 borrows. Programs like Solend use the spl-token program 用于 token custody 和 Pyth or Switchboard oracles 用于 price feeds.\n\n## Risk-operator mindset\n\nTreat every pool as a control system, not just a yield product:\n1. utilization controls liquidity stress,\n2. rate model controls borrower behavior,\n3. oracle quality controls collateral truth,\n4. liquidation speed controls solvency recovery.\n\nWhen one control weakens, the others must compensate.\n\n## Checklist\n- Understand the relationship between supply, borrow, 和 utilization\n- Know that utilization = totalBorrowed / totalSupply\n- Recognize that the reserve factor reduces supplier yield\n- Understand share-based accounting 用于 deposits 和 borrows\n- Identify the key on-chain 账户 in a Solana lending pool\n\n## Red flags\n- Utilization at or near 100% (withdrawal liquidity crisis)\n- Missing or zero reserve factor (no protocol safety buffer)\n- Share-price manipulation through donation attacks\n- Pools without oracle-backed price feeds 用于 collateral valuation\n",
            "duration": "50 min"
          },
          "lending-v2-interest-curves": {
            "title": "Interest rate curves 和 the kink model",
            "content": "# Interest rate curves 和 the kink model\n\nInterest rates in lending protocols are not fixed. They adjust dynamically based on pool utilization to balance supply 和 demand 用于 liquidity. The piecewise-linear \"kink\" model is the dominant interest rate 设计 used across DeFi lending protocols, from Compound 和 Aave on Ethereum to Solend 和 MarginFi on Solana.\n\nThe core insight is simple: when utilization is low, borrowing should be cheap to encourage demand. When utilization is high, borrowing should be expensive to discourage further borrowing 和 incentivize new deposits. The kink model achieves this 使用 two linear segments joined at a critical utilization point called the \"kink.\"\n\nThe kink model has four parameters: baseRate, slope1, slope2, 和 kink. The baseRate is the minimum borrow rate when utilization is zero. Slope1 is the rate of increase below the kink — a gentle incline that gradually raises borrow costs as utilization increases. The kink is the target utilization (typically 0.80 or 80%). Slope2 is the steep rate of increase above the kink — a sharp jump that penalizes borrowing when the pool approaches full utilization.\n\nBelow the kink, the borrow rate formula is:\n\nborrowRate = baseRate + (utilization / kink) * slope1\n\nThis creates a gentle linear increase. At 50% utilization 使用 a kink at 80%, baseRate of 2%, 和 slope1 of 10%, the borrow rate would be: 0.02 + (0.50 / 0.80) * 0.10 = 0.02 + 0.0625 = 0.0825 or 8.25%.\n\nAbove the kink, the formula becomes:\n\nborrowRate = baseRate + slope1 + ((utilization - kink) / (1 - kink)) * slope2\n\nThe full slope1 is added (the rate at the kink point), plus a steep increase proportional to how far utilization exceeds the kink. 使用 slope2 = 1.00 (100%), at 90% utilization: 0.02 + 0.10 + ((0.90 - 0.80) / (1 - 0.80)) * 1.00 = 0.02 + 0.10 + 0.50 = 0.62 or 62%. This dramatic jump is intentional — it makes borrowing above 80% utilization extremely expensive, creating strong pressure to restore utilization below the kink.\n\nThe supply rate is derived from the borrow rate, utilization, 和 reserve factor:\n\nsupplyRate = borrowRate * utilization * (1 - reserveFactor)\n\nSuppliers only earn on the portion of the pool that is actively borrowed, 和 the reserve factor takes its cut. At 50% utilization, an 8.25% borrow rate, 和 10% reserve factor: 0.0825 * 0.50 * 0.90 = 0.037125 or 3.71% supply APY.\n\nWhy the kink matters: without the steep slope2, high utilization would only moderately increase rates, potentially leading to a \"liquidity death spiral\" where all assets are borrowed 和 no supplier can withdraw. The kink creates an economic circuit breaker. Protocols tune these parameters through 治理 — adjusting the kink point, slopes, 和 base rate to target different utilization profiles 用于 different assets. Stablecoins typically have higher kinks (85-90%) because their prices are stable, while volatile assets have lower kinks (65-75%) to maintain larger liquidity buffers.\n\nReal-world Solana protocols often extend this model 使用 additional features: rate smoothing (averaging over recent blocks to prevent rapid oscillations), multiple kink points 用于 more granular control, 和 dynamic parameter adjustment based on market conditions. However, the fundamental two-slope kink model remains the foundation.\n\n## Checklist\n- Understand the four parameters: baseRate, slope1, slope2, kink\n- Calculate borrow rate below 和 above the kink\n- Derive supply rate from borrow rate, utilization, 和 reserve factor\n- Recognize why steep slope2 prevents liquidity crises\n- Know that different assets use different kink parameters\n\n## Red flags\n- Slope2 too low (insufficient deterrent 用于 high utilization)\n- Kink set too high (leaves insufficient withdrawal buffer)\n- Base rate at zero (no minimum cost of borrowing)\n- Parameters unchanged despite market condition shifts\n",
            "duration": "50 min"
          },
          "lending-v2-health-explorer": {
            "title": "Health factor monitoring 和 liquidation preview",
            "content": "# Health factor monitoring 和 liquidation preview\n\nThe health factor is the single number that determines whether a lending position is safe or subject to liquidation. Monitoring health factors in real time is essential 用于 both borrowers (to avoid liquidation) 和 liquidators (to identify profitable liquidation opportunities). Understanding how to compute, interpret, 和 react to health factor changes is a core skill 用于 DeFi risk management.\n\nThe health factor formula is:\n\nhealthFactor = (collateralValue * liquidationThreshold) / borrowValue\n\nwhere collateralValue is the total USD value of all deposited collateral, liquidationThreshold is the weighted average threshold across all collateral assets, 和 borrowValue is the total USD value of all outstanding borrows. When the health factor drops below 1.0, the position becomes eligible 用于 liquidation.\n\nThe liquidation threshold is distinct from the loan-to-value (LTV) ratio. LTV determines the maximum amount you can borrow — 用于 example, 75% LTV on SOL means you can borrow up to 75% of your SOL collateral value. The liquidation threshold is higher — say 80% — providing a buffer zone. You can borrow at 75% LTV, 和 you are only liquidated when your effective ratio exceeds 80%. This 5% gap gives borrowers time to add collateral or repay debt before liquidation.\n\nWhen a user has multiple collateral assets, the effective liquidation threshold is a weighted average. If you deposit $1,000 of SOL (threshold 0.80) 和 $500 of ETH (threshold 0.75), the weighted threshold is: (1000 * 0.80 + 500 * 0.75) / 1500 = (800 + 375) / 1500 = 0.7833. This weighted threshold is used in the health factor calculation.\n\nHealth factor interpretation: a value of 2.0 means the position can withstand a 50% decline in collateral value (or 50% increase in borrow value) before liquidation. A value of 1.5 provides a 33% buffer. A value of 1.1 is dangerously close — a 9% adverse price move triggers liquidation. Professional risk managers target health factors of 1.5 or above, 使用 automated alerts below 1.3 和 emergency actions below 1.2.\n\nMonitoring dashboards should display: current health factor 使用 color coding (green above 1.5, yellow 1.2-1.5, red below 1.2), the price change percentage needed to trigger liquidation, estimated liquidation prices 用于 each collateral asset, 和 historical health factor over time. On Solana, health factor data can be derived by reading obligation 账户 和 combining 使用 oracle price feeds from Pyth or Switchboard.\n\nLiquidation preview calculations help users understand their worst-case exposure. The maximum additional borrow is: max(0, collateralValue * effectiveThreshold - currentBorrow). The liquidation shortfall (when health factor < 1.0) is: currentBorrow - collateralValue * effectiveThreshold. This shortfall represents how much additional collateral or debt repayment is needed to restore the position to safety.\n\nPrice scenario analysis extends monitoring to \"what-if\" questions. What happens to the health factor if SOL drops 20%? If both SOL 和 ETH drop 30%? If interest accrues 用于 another month? By computing health factors across a range of price scenarios, borrowers can proactively manage risk before adverse conditions materialize. This scenario-based approach forms the foundation of the risk report challenge later in this 课程.\n\n## Checklist\n- Calculate health factor using weighted liquidation thresholds\n- Distinguish between LTV (borrowing limit) 和 liquidation threshold\n- Compute maximum additional borrow 和 liquidation shortfall\n- Set up monitoring 使用 color-coded health factor alerts\n- Run price scenario analysis before major market events\n\n## Red flags\n- Health factor below 1.2 without active monitoring\n- No alerts configured 用于 health factor changes\n- Ignoring weighted threshold calculations 用于 multi-asset positions\n- Failing to 账户 用于 accruing interest in health factor projections\n",
            "duration": "45 min"
          },
          "lending-v2-interest-rates": {
            "title": "Challenge: Compute utilization-based interest rates",
            "content": "# Challenge: Compute utilization-based interest rates\n\nImplement the kink-based interest rate model used by lending protocols:\n\n- Calculate the utilization ratio from total supply 和 total borrowed\n- Apply the piecewise-linear kink model 使用 baseRate, slope1, slope2, 和 kink\n- Compute the borrow rate using the appropriate formula 用于 below-kink 和 above-kink regions\n- Derive the supply rate from borrow rate, utilization, 和 reserve factor\n- Handle edge cases: zero supply, zero borrows, utilization at exactly the kink\n- Return all values formatted to 6 decimal places\n\nYour implementation must be deterministic — same input always produces same output.",
            "duration": "50 min"
          }
        }
      },
      "lending-v2-risk-management": {
        "title": "Risk Management",
        "description": "Health-factor computation, liquidation mechanics, oracle failure handling, 和 multi-scenario risk reporting 用于 stressed markets.",
        "lessons": {
          "lending-v2-health-factor": {
            "title": "Challenge: Compute health factor 和 liquidation status",
            "content": "# Challenge: Compute health factor 和 liquidation status\n\nImplement the health factor computation 用于 a multi-asset lending position:\n\n- Sum collateral 和 borrow values from an array of position objects\n- Compute weighted average liquidation threshold across all collateral assets\n- Calculate the health factor using the standard formula\n- Determine liquidation eligibility (health factor below 1.0)\n- Calculate maximum additional borrow capacity 和 liquidation shortfall\n- Handle edge cases: no borrows (max health factor), no collateral, single asset\n\nReturn all USD values to 2 decimal places 和 health factor to 4 decimal places.",
            "duration": "50 min"
          },
          "lending-v2-liquidation-mechanics": {
            "title": "Liquidation mechanics: bonus, close factor, 和 bad debt",
            "content": "# Liquidation mechanics: bonus, close factor, 和 bad debt\n\nLiquidation is the enforcement mechanism that keeps lending protocols solvent. When a borrower's health factor falls below 1.0, external actors called liquidators can repay a portion of the debt in exchange 用于 the borrower's collateral at a discount. Understanding liquidation mechanics — the incentive structure, limits, 和 failure modes — is essential 用于 anyone building on or using lending protocols.\n\nThe liquidation bonus (also called the liquidation incentive or discount) is the premium liquidators receive 用于 performing liquidations. If the liquidation bonus is 5%, a liquidator who repays $100 of debt receives $105 worth of collateral. This bonus serves two purposes: it compensates liquidators 用于 gas costs 和 execution risk, 和 it creates competitive pressure to liquidate positions quickly before other liquidators claim the opportunity. On Solana, where 交易 costs are low, liquidation bonuses tend to be smaller (3-8%) compared to Ethereum (5-15%).\n\nThe close factor limits how much of a position can be liquidated in a single 交易. A close factor of 50% means a liquidator can repay at most 50% of the outstanding debt in one liquidation call. This prevents a single liquidator from seizing all collateral in one 交易, giving the borrower a chance to respond. It also distributes liquidation opportunities across multiple liquidators, improving the health of the liquidation market. Some protocols use dynamic close factors — smaller percentages 用于 mildly underwater positions, larger percentages (up to 100%) 用于 deeply underwater positions.\n\nThe liquidation process on Solana follows these steps: (1) a liquidator identifies a position 使用 health factor below 1.0 by scanning obligation 账户, (2) the liquidator calls the liquidation 指令 specifying which debt to repay 和 which collateral to seize, (3) the protocol verifies the position is indeed liquidatable, (4) the debt tokens are transferred from the liquidator to the pool, reducing the borrower's debt, (5) the corresponding collateral (plus bonus) is transferred from the borrower's obligation to the liquidator. The entire process is atomic — it either completes fully or reverts.\n\nBad debt occurs when a position's collateral value (including the liquidation bonus) is insufficient to cover the outstanding debt. This happens during extreme market crashes where prices move faster than liquidators can act, or when the collateral asset experiences a sudden loss of liquidity. When bad debt materializes, the protocol must absorb the loss. Common approaches include: drawing from the reserve fund (accumulated from reserve factors), socializing the loss across all suppliers in the pool (reducing the share price), or using a protocol insurance fund or backstop mechanism.\n\nCascading liquidations are a systemic risk. When many positions use the same collateral (e.g., SOL), a price drop triggers liquidations. Liquidators selling the seized collateral on DEXes further depresses the price, triggering more liquidations. This cascade can drain pool liquidity rapidly. Protocols mitigate this through: conservative LTV ratios, higher liquidation thresholds 用于 volatile assets, liquidation rate limits (maximum liquidation volume per time window), 和 integration 使用 deep liquidity sources.\n\nSolana-specific considerations: liquidation bots on Solana benefit from low latency 和 low 交易 costs. However, they must compete 用于 交易 ordering during volatile periods. MEV (Maximal Extractable Value) on Solana through Jito tips allows liquidators to prioritize their 交易. Protocols must also handle Solana's 账户模型 — each obligation 账户 must be refreshed 使用 current oracle prices before liquidation can proceed, adding 指令 和 compute units to the liquidation 交易.\n\n## Checklist\n- Understand the liquidation bonus incentive structure\n- Know how close factor limits single-交易 liquidation\n- Track the flow of funds during a liquidation event\n- Identify bad debt scenarios 和 protocol mitigation strategies\n- Consider cascading liquidation risks in portfolio construction\n\n## Red flags\n- Liquidation bonus too low (liquidators are not incentivized to act quickly)\n- Close factor at 100% (full liquidation in one shot, no borrower recourse)\n- No reserve fund or insurance mechanism 用于 bad debt\n- Ignoring cascading liquidation risks in concentrated collateral pools\n",
            "duration": "50 min"
          },
          "lending-v2-oracle-risk": {
            "title": "Oracle risk 和 stale pricing in lending",
            "content": "# Oracle risk 和 stale pricing in lending\n\nLending protocols depend entirely on accurate, timely price feeds to compute collateral values, health factors, 和 liquidation eligibility. Oracles — the services that bring off-chain price data on-chain — are the single most critical external dependency. Oracle failures or manipulation can lead to catastrophic losses: incorrect liquidations of healthy positions, failure to liquidate underwater positions, or exploits that drain protocol reserves.\n\nOn Solana, the two dominant oracle providers are Pyth Network 和 Switchboard. Pyth provides high-frequency price feeds sourced directly from market makers, exchanges, 和 trading firms. Pyth publishes price, confidence interval, 和 exponential moving average (EMA) price 用于 each asset. Switchboard is a more general-purpose oracle network that supports custom data feeds 和 verification mechanisms. Most Solana lending protocols integrate both 和 use the more conservative price (lower 用于 collateral, higher 用于 borrows).\n\nStale prices are the most common oracle risk. A price is \"stale\" when it has not been updated within a protocol-defined freshness window — typically 30-120 seconds on Solana. Staleness occurs when: oracle publishers experience downtime, network congestion delays update 交易, or the asset's market enters a period of extreme volatility where publishers disagree on the price. Lending protocols must reject stale prices 和 either pause operations or use fallback pricing. Accepting a stale price during a market crash can mean using a price from minutes ago that is significantly higher than reality — blocking necessary liquidations 和 enabling under-collateralized borrowing.\n\nConfidence intervals quantify price uncertainty. Pyth provides a confidence band around each price — 用于 example, SOL at $25.00 +/- $0.15. A narrow confidence interval indicates strong publisher agreement. A wide confidence interval signals disagreement, low liquidity, or unusual market conditions. Risk-aware protocols use confidence-adjusted prices: 用于 collateral valuation, use (price - confidence) to be conservative; 用于 borrow valuation, use (price + confidence) to 账户 用于 upside risk. This approach prevents protocols from accepting inflated collateral values during uncertain market conditions.\n\nPrice manipulation attacks target the oracle layer. In a classic oracle manipulation, an attacker temporarily moves the price on a low-liquidity market that the oracle reads from, borrows against the inflated collateral value, 和 then lets the price revert — leaving the protocol 使用 under-collateralized debt. Mitigations include: using time-weighted average prices (TWAPs) instead of spot prices, requiring multiple independent sources to agree, capping single-block price changes, 和 implementing borrow/withdrawal delays during high-volatility periods.\n\nSolana-specific oracle considerations: Pyth on Solana uses a pull-based model where price updates are posted to on-chain 账户 that protocols read. Each Pyth price 账户 contains the latest price, confidence, EMA price, publish time, 和 status (Trading, Halted, Unknown). Protocols should check the status field — a \"Halted\" or \"Unknown\" status indicates the feed is unreliable. The publishTime must be compared against the current slot time to detect staleness. Switchboard 账户 have similar freshness 和 confidence metadata.\n\nMulti-oracle strategies improve resilience. A protocol might use Pyth as the primary oracle 和 Switchboard as a fallback. If Pyth's price is stale or has low confidence, the protocol switches to Switchboard. If both are unavailable, the protocol pauses new borrows 和 liquidations rather than operating on unknown prices. This layered approach prevents single points of failure in the oracle infrastructure.\n\nCircuit breakers add an additional safety layer. If an oracle reports a price change exceeding a threshold (e.g., >20% in one update), the protocol should flag this as potentially suspicious 和 either verify against a secondary source or temporarily pause operations. Flash crashes 和 recovery events can produce legitimate large price movements, but the protocol should err on the side of caution.\n\n## Checklist\n- Verify oracle freshness (publishTime within acceptable window)\n- Use confidence intervals 用于 conservative pricing\n- Implement multi-oracle fallback strategies\n- Check oracle status fields (Trading, Halted, Unknown)\n- Set circuit breakers 用于 extreme price movements\n\n## Red flags\n- Single oracle dependency 使用 no fallback\n- No staleness checks on price data\n- Ignoring confidence intervals 用于 collateral valuation\n- Using spot prices without TWAP or time-weighting\n- No circuit breakers 用于 extreme price changes\n",
            "duration": "45 min"
          },
          "lending-v2-risk-report": {
            "title": "Checkpoint: Generate a multi-scenario risk report",
            "content": "# Checkpoint: Generate a multi-scenario risk report\n\nBuild the final risk report that combines all 课程 concepts:\n\n- Evaluate a base case using current position prices\n- Apply price overrides from multiple named scenarios (bull, crash, etc.)\n- Compute collateral value, borrow value, 和 health factor per scenario\n- Identify which scenarios trigger liquidation (health factor < 1.0)\n- Track the worst health factor across all scenarios\n- Count total liquidation scenarios\n- Output must be stable JSON 使用 deterministic key ordering\n\nThis checkpoint validates your complete understanding of lending risk analysis.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "defi-perps-risk-console": {
    "title": "Perps Risk Console",
    "description": "Master perps risk engineering on Solana: precise PnL/funding accounting, margin safety monitoring, liquidation simulation, 和 deterministic console reporting.",
    "duration": "14 hours",
    "tags": [
      "defi",
      "perps",
      "perpetuals",
      "risk",
      "solana"
    ],
    "modules": {
      "perps-v2-fundamentals": {
        "title": "Perps Fundamentals",
        "description": "Perpetual futures mechanics, funding accrual logic, 和 PnL modeling foundations 用于 accurate position diagnostics.",
        "lessons": {
          "perps-v2-mental-model": {
            "title": "Perpetual futures: base positions, entry price, 和 mark vs oracle",
            "content": "# Perpetual futures: base positions, entry price, 和 mark vs oracle\n\nPerpetual futures (perps) are synthetic derivatives that let traders gain exposure to an asset's price movement without holding the underlying token. Unlike traditional futures 使用 expiry dates, perpetual contracts never settle. Instead, a funding rate mechanism keeps the contract price anchored to the spot price over time. Understanding how positions are represented, how entry prices work, 和 the distinction between mark 和 oracle prices is the foundation of every risk calculation that follows.\n\n## Position anatomy\n\nA perpetual futures position is defined by four core fields: side (long or short), size (the quantity of the base asset), entry price (the average cost basis), 和 margin (the collateral deposited). When you open a long position of 10 SOL-PERP at $22.50 使用 $225 margin, you are expressing a bet that SOL's price will rise. The notional value of this position is size multiplied by the current mark price. Notional value changes continuously as the mark price moves, even though your entry price remains fixed until you modify the position.\n\nEntry price is not simply the price at the moment you clicked \"buy.\" If you add to an existing position, the entry price updates to the weighted average of the old 和 new fills. 用于 example, if you hold 5 SOL-PERP at $20 和 buy 5 more at $25, your new entry price becomes (5 * 20 + 5 * 25) / 10 = $22.50. Partial closes do not change the entry price — only additions do. Tracking entry price accurately is critical because every PnL calculation derives from the difference between entry 和 current price.\n\n## Mark price vs oracle price\n\nOn-chain perpetual protocols maintain two distinct prices: the mark price 和 the oracle price. The oracle price reflects the broader market's view of the asset's spot value. Solana protocols commonly use Pyth or Switchboard oracle feeds, which aggregate price data from multiple exchanges 和 publish updates on-chain every 400 milliseconds. The oracle price is the \"truth\" — the real-world value of the underlying asset.\n\nThe mark price is the protocol's internal valuation of the perpetual contract. It is typically derived from the oracle price plus a premium or discount that reflects supply 和 demand imbalance in the perp market itself. When there are more longs than shorts, the mark price trades above the oracle (positive premium). When shorts dominate, the mark trades below (negative premium). The formula varies by protocol but often follows: markPrice = oraclePrice + exponentialMovingAverage(premium).\n\nMark price is used 用于 all PnL calculations 和 liquidation triggers. Using mark price instead of raw trade price prevents manipulation attacks where a single large trade could spike the last-traded price 和 trigger mass liquidations. The mark price moves more smoothly because it incorporates the oracle as a stability anchor.\n\n## Why this matters 用于 risk\n\nEvery risk metric in a perps risk console depends on getting these fundamentals right. Unrealized PnL is computed against the mark price. Margin ratio is computed using notional value at mark price. Liquidation price is derived from the entry price 和 margin. If you confuse mark 和 oracle, or miscalculate entry price after position averaging, every downstream number is wrong.\n\nOn Solana specifically, oracle latency introduces an additional consideration. Pyth oracle updates propagate 使用 slot-level granularity (~400ms). During volatile periods, the oracle price can lag behind actual market moves by several hundred milliseconds. Protocols handle this by including confidence intervals in their oracle reads 和 rejecting prices 使用 excessively wide confidence bands. When building risk dashboards, always display the oracle confidence alongside the price 和 flag stale oracles (timestamps older than a few seconds).\n\n## Console 设计 principle\n\nA useful risk console must separate:\n1. directional 性能 (PnL),\n2. structural cost (funding + fees),\n3. survival risk (margin ratio + liquidation distance).\n\nBlending these into one number hides the decision signals traders actually need.\n\n## Checklist\n- Understand that perpetual futures never expire 和 use funding to track spot\n- Track entry price as a weighted average across all fills\n- Distinguish mark price (PnL, liquidation) from oracle price (funding, reference)\n- Monitor oracle staleness 和 confidence intervals\n- Compute notional value as size * markPrice\n\n## Red flags\n- Using last-traded price instead of mark price 用于 PnL\n- Forgetting to update entry price on position additions\n- Ignoring oracle confidence intervals during volatile markets\n- Assuming mark price equals oracle price (the premium matters)\n",
            "duration": "50 min"
          },
          "perps-v2-funding": {
            "title": "Funding rates: why they exist 和 how they accrue",
            "content": "# Funding rates: why they exist 和 how they accrue\n\nFunding rates are the mechanism that tethers a perpetual contract's price to the underlying spot price. Without funding, the perp price could drift arbitrarily far from reality because the contract never expires. Funding creates a periodic cash flow between longs 和 shorts that incentivizes convergence: when the perp trades above spot, longs pay shorts; when it trades below, shorts pay longs.\n\n## The convergence mechanism\n\nConsider a scenario where heavy demand from leveraged long traders pushes the SOL-PERP mark price to $23 while the SOL oracle price is $22. The premium is $1, or about 4.5%. The funding rate will be positive, meaning long holders pay short holders every funding interval. This payment makes it expensive to hold longs 和 attractive to hold shorts, which naturally pushes the perp price back toward spot. When the perp trades below spot (negative premium), funding flips: shorts pay longs, discouraging shorts 和 encouraging longs.\n\nThe funding rate is typically calculated as: fundingRate = clamp(premium / 24, -maxRate, +maxRate), where the premium is the percentage difference between mark 和 oracle prices, divided by 24 to normalize to an hourly rate. Most protocols on Solana settle funding every hour, though some use shorter intervals (every 8 hours is common on centralized exchanges). The clamp function prevents extreme rates during flash crashes or squeezes.\n\n## How funding accrues\n\nFunding is not a continuous stream — it settles at discrete intervals. At each funding timestamp, the protocol snapshots every open position 和 calculates: fundingPayment = positionSize * entryPrice * fundingRate. 用于 a 10 SOL-PERP position at $25 entry 使用 a funding rate of 0.01% (0.0001), the payment is 10 * 25 * 0.0001 = $0.025 per interval.\n\nThe direction of payment depends on the position side 和 the sign of the funding rate. When the funding rate is positive: longs pay (their margin decreases) 和 shorts receive (their margin increases). When negative: shorts pay 和 longs receive. This is a zero-sum transfer — the total paid by one side exactly equals the total received by the other side, minus any protocol fees.\n\nCumulative funding matters more than any single payment. A position held 用于 24 hours accumulates 24 hourly funding payments (or 3 eight-hour payments, depending on the protocol). During trending markets, cumulative funding can become a significant drag on PnL. A long position in a strongly bullish market might show +$100 unrealized PnL but have paid -$15 in cumulative funding, reducing the real return. Risk dashboards must display both unrealized PnL 和 cumulative funding separately so traders see the full picture.\n\n## Funding on Solana protocols\n\nSolana perps protocols like Drift, Mango Markets, 和 Jupiter Perps each implement funding slightly differently. Drift uses a time-weighted average premium over 1-hour windows. Jupiter Perps uses a simpler hourly mark-to-oracle premium. Mango uses an oracle-based funding model 使用 configurable parameters per market. Despite these differences, the core principle is identical: positive premium means longs pay shorts.\n\nOn-chain funding settlement on Solana happens through cranked 指令. A keeper bot calls a \"settle funding\" 指令 at each interval, which iterates through positions 和 adjusts their realized PnL 账户. Positions that are not explicitly settled may accumulate pending funding payments that are only applied when the position is next touched (opened, closed, or cranked). This lazy evaluation means your displayed margin may not reflect unsettled funding until you interact 使用 the position.\n\n## Impact on risk monitoring\n\n用于 risk console purposes, you must track: (1) the current funding rate 和 whether your position is paying or receiving, (2) cumulative funding paid or received since position open, (3) the net margin impact as a percentage of initial margin, 和 (4) projected funding cost if the current rate persists. A position that looks profitable on a PnL basis might be marginally unprofitable after accounting 用于 funding drag. Always include funding in your total return calculations.\n\n## Checklist\n- Understand that positive funding rate means longs pay shorts\n- Calculate funding payment as size * price * rate per interval\n- Track cumulative funding over the position's lifetime\n- 账户 用于 funding when computing real return (PnL + funding)\n- Monitor 用于 extreme funding rates that signal market imbalance\n\n## Red flags\n- Ignoring funding costs in PnL reporting\n- Confusing funding direction (positive rate = longs pay)\n- Not accounting 用于 lazy settlement on Solana protocols\n- Assuming funding is continuous rather than discrete-interval\n",
            "duration": "50 min"
          },
          "perps-v2-pnl-explorer": {
            "title": "PnL visualization: tracking profit over time",
            "content": "# PnL visualization: tracking profit over time\n\nProfit 和 loss (PnL) tracking in perpetual futures requires careful accounting across multiple dimensions: unrealized PnL from price movement, realized PnL from closed portions, funding payments, 和 trading fees. A well-built PnL visualization shows traders not just where they stand now, but how they arrived there — which is essential 用于 risk management 和 strategy refinement.\n\n## Unrealized vs realized PnL\n\nUnrealized PnL represents the paper profit or loss on your open position. 用于 a long position: unrealizedPnL = size * (markPrice - entryPrice). 用于 a short: unrealizedPnL = size * (entryPrice - markPrice). This number changes 使用 every price tick 和 represents what you would gain or lose if you closed the position right now at the mark price.\n\nRealized PnL is locked in when you close all or part of a position. If you opened 10 SOL-PERP long at $20 和 close 5 contracts at $25, you realize 5 * (25 - 20) = $25 profit. The remaining 5 contracts continue to have unrealized PnL based on the current mark price versus your (unchanged) entry of $20. Realized PnL is permanent — it has already been credited to your margin 账户. Unrealized PnL fluctuates 和 may increase or decrease.\n\nTotal PnL = realized + unrealized + cumulative funding. This is the true measure of position 性能. Displaying all three components separately gives traders insight into whether their profits come from directional moves (unrealized), successful trades (realized), or favorable funding conditions.\n\n## Return on equity (ROE)\n\nROE measures the percentage return relative to the initial margin deposited. ROE = (unrealizedPnL / initialMargin) * 100. A position 使用 $25 unrealized PnL on $225 margin has an ROE of 11.11%. Because perpetual futures are leveraged instruments, ROE can be dramatically higher (or lower) than the percentage price change. 使用 10x leverage, a 5% price move produces approximately 50% ROE.\n\nROE is the primary 性能 metric 用于 comparing positions across different sizes 和 leverage levels. A $10 profit on $100 margin (10% ROE) represents better capital efficiency than $10 profit on $1000 margin (1% ROE), even though the dollar PnL is identical. Risk consoles should display ROE prominently alongside raw PnL.\n\n## Time-series visualization\n\nPlotting PnL over time reveals patterns invisible in a single snapshot. Key elements of a PnL time series: (1) The unrealized PnL curve, moving 使用 each mark price update. (2) Step changes when partial closes realize PnL. (3) Small periodic steps from funding payments. (4) The cumulative total line combining all components.\n\n用于 Solana protocols, PnL snapshots can be captured at each slot (~400ms) or aggregated into minute/hour candles 用于 longer timeframes. Real-time WebSocket feeds from RPC nodes provide mark price updates, 和 funding payments appear as on-chain events at each settlement interval. A production risk console typically polls mark prices every 1-5 seconds 和 updates the PnL display accordingly.\n\n## Break-even analysis\n\nThe break-even price 账户 用于 all costs: trading fees, funding payments, 和 slippage. 用于 a long position: breakEvenPrice = entryPrice + (totalFees + cumulativeFundingPaid) / size. If you entered at $22.50 使用 $0.50 in total costs on a 10-unit position, your break-even is $22.55. Displaying the break-even line on the PnL chart gives traders a clear target — the position is only truly profitable when the mark price exceeds this line.\n\n## Visualization 最佳实践\n\nEffective PnL dashboards use color coding consistently: green 用于 positive PnL, red 用于 negative. The zero line should be visually prominent. Hover tooltips should show the exact PnL at any point in time. Consider showing both absolute dollar PnL 和 percentage ROE on dual axes. Include funding annotations as small markers on the time axis so traders can see when funding events impacted their PnL curve.\n\n## Checklist\n- Separate unrealized, realized, 和 funding components in the display\n- Calculate ROE relative to initial margin, not current margin\n- Include break-even price accounting 用于 all costs\n- Update PnL in near-real-time using mark price feeds\n- Annotate funding events on the PnL time series\n\n## Red flags\n- Showing only unrealized PnL without funding impact\n- Computing ROE against notional value instead of margin\n- Not distinguishing realized from unrealized PnL\n- Updating PnL using oracle price instead of mark price\n",
            "duration": "45 min"
          },
          "perps-v2-pnl-calc": {
            "title": "Challenge: Calculate perpetual futures PnL",
            "content": "# Challenge: Calculate perpetual futures PnL\n\nImplement a PnL calculator 用于 perpetual futures positions:\n\n- Compute unrealized PnL based on entry price vs mark price\n- Handle both long 和 short positions correctly\n- Calculate notional value as size * markPrice\n- Compute ROE (return on equity) as a percentage of initial margin\n- Format all outputs 使用 appropriate decimal precision\n\nYour calculator must be deterministic — same input always produces the same output.",
            "duration": "50 min"
          },
          "perps-v2-funding-accrual": {
            "title": "Challenge: Simulate funding rate accrual",
            "content": "# Challenge: Simulate funding rate accrual\n\nBuild a funding accrual simulator that processes discrete funding intervals:\n\n- Iterate through an array of funding rates 和 compute the payment 用于 each period\n- Longs pay (subtract from balance) when the funding rate is positive\n- Shorts receive (add to balance) when the funding rate is positive\n- Track cumulative funding, average rate, 和 net margin impact\n- Handle negative funding rates where the direction reverses\n\nThe simulator must be deterministic — same inputs always produce the same result.",
            "duration": "50 min"
          }
        }
      },
      "perps-v2-risk": {
        "title": "Risk & Monitoring",
        "description": "Margin 和 liquidation monitoring, implementation bug traps, 和 deterministic risk-console outputs 用于 production observability.",
        "lessons": {
          "perps-v2-margin-liquidation": {
            "title": "Margin ratio 和 liquidation thresholds",
            "content": "# Margin ratio 和 liquidation thresholds\n\nMargin is the collateral that backs a leveraged position. When the margin falls below a critical threshold relative to the position's notional value, the protocol forcibly closes the position to prevent the trader from owing more than they deposited. Understanding margin mechanics, the maintenance margin threshold, 和 how liquidation prices are calculated is essential 用于 risk monitoring.\n\n## Initial margin 和 leverage\n\nInitial margin is the collateral deposited when opening a position. The leverage multiple is: leverage = notionalValue / initialMargin. A position 使用 $250 notional value 和 $25 margin is 10x leveraged. Higher leverage amplifies both gains 和 losses. At 10x, a 10% adverse price move wipes out 100% of the margin. At 20x, only a 5% move is needed to reach zero.\n\nSolana perps protocols typically allow leverage up to 20x or even 50x on major pairs (SOL, BTC, ETH) 和 lower leverage (5x-10x) on altcoins 使用 thinner liquidity. The maximum leverage is governed by the maintenance margin rate — a lower maintenance margin rate allows higher maximum leverage.\n\n## Maintenance margin\n\nThe maintenance margin rate (MMR) is the minimum margin ratio a position must maintain to avoid liquidation. If the MMR is 5% (0.05), the effective margin must be at least 5% of the notional value at all times. Effective margin 账户 用于 unrealized PnL 和 funding: effectiveMargin = initialMargin + unrealizedPnL + cumulativeFunding. The margin ratio is: marginRatio = effectiveMargin / notionalValue.\n\nWhen the margin ratio drops below the MMR, the position is eligible 用于 liquidation. Protocols don't wait 用于 the margin to reach exactly zero — the maintenance buffer ensures there is still some collateral left to cover liquidation fees, slippage, 和 potential bad debt. If a position's losses exceed its margin entirely, the deficit becomes \"bad debt\" that must be absorbed by an insurance fund or socialized across other traders.\n\n## Liquidation price calculation\n\nThe liquidation price is the mark price at which the margin ratio exactly equals the maintenance margin rate. 用于 a long position: liquidationPrice = entryPrice - (margin + cumulativeFunding - notional * MMR) / size. 用于 a short: liquidationPrice = entryPrice + (margin + cumulativeFunding - notional * MMR) / size.\n\nThis formula 账户 用于 the fact that as the mark price moves against you, both the unrealized PnL (reducing effective margin) 和 the notional value (the denominator of margin ratio) change simultaneously. The liquidation price is not simply \"entry price minus margin per unit\" — the maintenance margin requirement means liquidation triggers before your margin is fully depleted.\n\n用于 example, consider a 10 SOL-PERP long at $22.50 使用 $225 margin 和 5% MMR. The notional at entry is 10 * 22.50 = $225. Liquidation triggers when effectiveMargin / notional = 0.05, which solves to a mark price near $2.05 in this well-margined case. 使用 higher leverage (less margin), the liquidation price would be much closer to entry.\n\n## Cascading liquidations\n\nDuring sharp market moves, many positions hit their liquidation prices simultaneously. Liquidation engines close these positions by selling into the order book (or AMM pools), which pushes the price further in the adverse direction, triggering more liquidations. This cascade effect — also called a \"liquidation spiral\" — can cause prices to move far beyond what fundamentals justify.\n\nOn Solana, liquidation is performed by keeper bots that submit liquidation 交易. These bots compete 用于 liquidation opportunities because protocols offer a liquidation fee (typically 0.5-2% of the position's notional) as an incentive. During cascades, keeper bots may face congestion issues as many liquidation 交易 compete 用于 block space. Partial liquidation — closing only enough of a position to restore the margin ratio above MMR — helps reduce cascade severity by keeping some of the position alive.\n\n## Risk monitoring thresholds\n\nA production risk console should alert at multiple thresholds: (1) WARNING when the margin ratio drops below 1.5x the MMR (e.g., 7.5% when MMR is 5%), (2) CRITICAL when below the MMR itself (liquidation imminent), 和 (3) INFO when unrealized PnL exceeds a significant percentage of margin (positive or negative). These alerts give traders time to add margin, reduce position size, or close entirely before forced liquidation.\n\n## Checklist\n- Calculate effective margin including unrealized PnL 和 funding\n- Compute margin ratio as effectiveMargin / notionalValue\n- Derive liquidation price from entry price, margin, 和 MMR\n- Set warning thresholds above the MMR to give early alerts\n- 账户 用于 liquidation fees in worst-case scenarios\n\n## Red flags\n- Computing liquidation price without accounting 用于 the maintenance buffer\n- Ignoring funding in effective margin calculations\n- Not alerting traders before they reach the liquidation threshold\n- Assuming the mark price at liquidation equals the execution price (slippage exists)\n",
            "duration": "50 min"
          },
          "perps-v2-common-bugs": {
            "title": "Common bugs: sign errors, units, 和 funding direction",
            "content": "# Common bugs: sign errors, units, 和 funding direction\n\nPerpetual futures implementations are mathematically straightforward — the formulas are basic arithmetic. Yet sign errors, unit mismatches, 和 funding direction bugs are among the most frequent 和 costly mistakes in DeFi development. A single flipped sign can turn profits into losses, liquidate healthy positions, or drain insurance funds. This 课时 catalogs the most common pitfalls 和 how to avoid them.\n\n## Sign errors in PnL calculations\n\nThe most fundamental bug: getting the sign wrong on PnL 用于 short positions. Long PnL = size * (markPrice - entryPrice). Short PnL = size * (entryPrice - markPrice). Note that short PnL is NOT size * (markPrice - entryPrice) 使用 a negated size. The size is always positive — it represents the quantity of contracts. The direction is captured in the formula itself. A common mistake is storing size as negative 用于 shorts 和 using a single formula: pnl = size * (markPrice - entryPrice). While mathematically equivalent when size is negative, this representation causes bugs everywhere else: notional value calculations, funding payments, margin ratios, 和 liquidation prices all need absolute size.\n\nRule: Keep size always positive. Branch on the side field to select the correct formula. Never rely on sign conventions embedded in other fields.\n\n## Unit 和 decimal mismatches\n\nSolana token amounts are raw integers (lamports, token base units). Prices from oracles are typically fixed-point numbers 使用 specific exponents. Mixing these without proper conversion produces catastrophically wrong values.\n\nExample: SOL has 9 decimals on-chain. If a position size is stored as 10_000_000_000 (10 SOL in lamports) 和 you multiply by a price of 22.50 (a floating-point dollar value), you get 225,000,000,000 — which might look like a notional value, but it is in lamports-times-dollars, a nonsensical unit. You must either convert size to human-readable units first (divide by 10^9), or keep everything in integer space 使用 a consistent exponent.\n\nRule: Define a canonical unit convention at the start of your project. Either work entirely in human-readable floats (acceptable 用于 display/simulation code) or entirely in integer base units 使用 explicit scaling factors (required 用于 on-chain code). Never mix the two.\n\n## Funding direction confusion\n\nThe funding direction rule is: \"positive funding rate means longs pay shorts.\" This is universal across all major protocols. Yet developers frequently implement it backwards, especially when reasoning about \"who benefits.\" When the rate is positive, the market is bullish (more longs than shorts). Longs pay to discourage the imbalance. Shorts receive as compensation 用于 providing the other side.\n\nIn code, the mistake looks like this:\n- WRONG: if (side === \"long\") totalFunding += payment;\n- RIGHT: if (side === \"long\") totalFunding -= payment;\n\nWhen the funding rate is positive 和 the side is long, the payment reduces the trader's balance. When negative 和 long, the payment increases the balance (longs receive). Test every combination: positive rate + long, positive rate + short, negative rate + long, negative rate + short.\n\n## Liquidation price off-by-one\n\nThe liquidation price formula must 账户 用于 the maintenance margin requirement. A common bug is computing the price at which margin equals zero rather than the price at which margin equals the maintenance requirement. This results in a liquidation price that is too aggressive — the position would be liquidated later than expected, potentially accumulating bad debt.\n\nAnother variant: forgetting to include cumulative funding in the liquidation price calculation. If a long position has paid $5 in funding, its effective margin is $5 less than the initial deposit, 和 the liquidation price is correspondingly closer to the entry price.\n\n## Margin ratio denominator\n\nMargin ratio = effectiveMargin / notionalValue. The notional value must use the current mark price, not the entry price. Using entry price 用于 notional gives an incorrect ratio because the actual exposure changes as the mark price moves. A position 使用 $225 entry notional that has moved to $250 mark notional has a lower margin ratio than the entry-price calculation suggests — the position has grown while the margin remains fixed.\n\n## Integer overflow in funding accumulation\n\nWhen accumulating funding over hundreds or thousands of periods, floating-point precision errors can compound. Each period adds a small number (e.g., 0.025), 和 after thousands of additions, the accumulated error can become material. Using fixed-point arithmetic or rounding at each step (使用 a consistent rounding convention) prevents drift. In JavaScript, toFixed() at the final output step is sufficient 用于 display, but 中级 calculations should preserve full precision.\n\n## 测试 strategy\n\nEvery perps calculation should have test cases covering: (1) Long 使用 profit, (2) Long 使用 loss, (3) Short 使用 profit, (4) Short 使用 loss, (5) Positive funding rate 用于 both sides, (6) Negative funding rate 用于 both sides, (7) Zero funding rate, (8) Zero-margin edge case. If any single combination is missing from your test suite, the corresponding bug can ship undetected.\n\n## Checklist\n- Use separate formulas 用于 long 和 short PnL, not sign-encoded size\n- Define 和 enforce a canonical unit convention (human-readable vs base units)\n- Test all four combinations of funding direction (2 sides x 2 rate signs)\n- Include maintenance margin in liquidation price calculations\n- Use mark price (not entry price) 用于 notional value in margin ratio\n\n## Red flags\n- Negative position sizes used to encode short direction\n- Mixing lamport-scale 和 dollar-scale values in the same calculation\n- Funding payment that adds to long balances when the rate is positive\n- Liquidation price computed at zero margin instead of maintenance margin\n- Margin ratio using entry-price notional instead of mark-price notional\n",
            "duration": "45 min"
          },
          "perps-v2-risk-console-report": {
            "title": "Checkpoint: Generate a Risk Console Report",
            "content": "# Checkpoint: Generate a Risk Console Report\n\nBuild the comprehensive risk console report that integrates all 课程 concepts:\n\n- Calculate unrealized PnL 和 ROE 用于 the position\n- Accumulate funding payments across all provided funding rate intervals\n- Compute effective margin (initial + PnL + funding) 和 margin ratio\n- Derive the liquidation price accounting 用于 maintenance margin 和 funding\n- Generate severity-tiered alerts (CRITICAL, WARNING, INFO) based on thresholds\n- Output must be stable JSON 使用 deterministic structure\n\nThis checkpoint validates your complete understanding of perpetual futures risk management.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "defi-tx-optimizer": {
    "title": "DeFi 交易 Optimizer",
    "description": "Master Solana DeFi 交易 optimization: compute/fee tuning, ALT strategy, reliability patterns, 和 deterministic send-strategy planning.",
    "duration": "12 hours",
    "tags": [
      "defi",
      "transactions",
      "optimization",
      "compute",
      "solana"
    ],
    "modules": {
      "txopt-v2-fundamentals": {
        "title": "交易 Fundamentals",
        "description": "交易 failure diagnosis, compute budget mechanics, priority-fee strategy, 和 fee estimation foundations.",
        "lessons": {
          "txopt-v2-why-fail": {
            "title": "Why DeFi 交易 fail: CU limits, size, 和 blockhash expiry",
            "content": "# Why DeFi 交易 fail: CU limits, size, 和 blockhash expiry\n\nDeFi 交易 on Solana fail 用于 three primary reasons: compute budget exhaustion, 交易 size overflow, 和 blockhash expiry. Understanding each failure mode is essential before attempting any optimization, because the fix 用于 each is fundamentally different. Misdiagnosing the failure category leads to wasted effort 和 frustrated users.\n\n## Compute budget exhaustion\n\nEvery Solana 交易 executes within a compute budget measured in compute units (CUs). The default budget is 200,000 CUs per 交易, which is sufficient 用于 simple transfers but far too low 用于 complex DeFi operations. A single AMM swap through a concentrated liquidity pool can consume 100,000-200,000 CUs. Multi-hop routes, flash loans, or 交易 that interact 使用 multiple protocols easily exceed 400,000 CUs. When a 交易 exceeds its compute budget, the runtime aborts execution 和 returns a `ComputeBudgetExceeded` error. The 交易 fee is still charged because the 验证者 performed work before the limit was hit.\n\nThe solution is the `SetComputeUnitLimit` 指令 from the Compute Budget Program. This 指令 must be the first 指令 in the 交易 (by convention) 和 tells the runtime exactly how many CUs to allocate. Setting the limit too low causes failures; setting it too high wastes priority fee budget because priority fees are calculated per CU requested (not consumed). The optimal approach is to simulate the 交易 first, observe the actual CU consumption, add a 10% safety margin, 和 use that as the limit.\n\n## 交易 size limits\n\nSolana 交易 have a hard size limit of 1,232 bytes when serialized. This limit applies to the entire 交易 packet including signatures, message header, 账户 keys, recent blockhash, 和 指令 data. Each 账户 key consumes 32 bytes. A 交易 referencing 30 unique 账户 uses 960 bytes 用于 账户 keys alone, leaving very little room 用于 指令 data 和 signatures.\n\nDeFi 交易 are particularly 账户-heavy. A single Raydium CLMM swap requires the user 钱包, input token 账户, output token 账户, pool state, AMM config, observation state, token vaults (x2), tick arrays (up to 3), oracle, 和 program IDs. Chaining multiple swaps in a single 交易 can easily push the 账户 count past 40, which exceeds the 1,232-byte limit 使用 standard 账户 encoding. This is where Address Lookup Tables (ALTs) become essential, compressing each 账户 reference from 32 bytes to just 1 byte 用于 账户 stored in the lookup table.\n\n## Blockhash expiry\n\nEvery Solana 交易 includes a recent blockhash that serves as a replay protection mechanism 和 a timestamp. A blockhash is valid 用于 approximately 60 seconds (roughly 150 slots at 400ms per slot). If a 交易 is not included in a block before the blockhash expires, it becomes permanently invalid 和 can never be processed. The 交易 simply disappears without any on-chain error record.\n\nBlockhash expiry is the most insidious failure mode because it produces no error message. The 交易 is silently dropped. This happens frequently during network congestion when 交易 queue 用于 longer than expected, or when users take too long to review 和 approve a 交易 in their 钱包. The correct handling is to monitor 用于 confirmation 使用 a timeout, 和 if the 交易 is not confirmed within 30 seconds, fetch a new blockhash, rebuild 和 re-sign the 交易, 和 resubmit.\n\n## Interaction between failure modes\n\nThese three failure modes often interact. A developer might add more 指令 to avoid multiple 交易 (reducing blockhash expiry risk), but this increases both CU consumption 和 交易 size. Optimizing 用于 one dimension can worsen another. The art of 交易 optimization is finding the right balance: enough CU budget to complete execution, compact enough to fit in 1,232 bytes, 和 fast enough submission to land before the blockhash expires.\n\n## Production triage rule\n\nDiagnose 交易 failures in strict order:\n1. did it fit 和 simulate,\n2. did it propagate 和 include,\n3. did it confirm before expiry.\n\nThis sequence prevents noisy fixes 和 reduces false assumptions during incidents.\n\n## Diagnostic checklist\n- Check 交易 logs 用于 `ComputeBudgetExceeded` when CU is the issue\n- Check serialized 交易 size against the 1,232-byte limit\n- Monitor confirmation status to detect silent blockhash expiry\n- Simulate 交易 before sending to catch CU 和 账户 issues early\n- Track failure rates by category to identify systemic problems\n",
            "duration": "50 min"
          },
          "txopt-v2-compute-budget": {
            "title": "Compute budget 指令 和 priority fee strategy",
            "content": "# Compute budget 指令 和 priority fee strategy\n\nThe Compute Budget Program provides two critical 指令 that every serious DeFi 交易 should include: `SetComputeUnitLimit` 和 `SetComputeUnitPrice`. Together, they control how much computation your 交易 can perform 和 how much you are willing to pay 用于 priority inclusion in a block.\n\n## SetComputeUnitLimit\n\nThis 指令 sets the maximum number of compute units the 交易 can consume. The value must be between 1 和 1,400,000 (the per-交易 maximum on Solana). The 指令 takes a single u32 parameter representing the CU limit. When omitted, the runtime uses the default of 200,000 CUs.\n\nChoosing the right limit requires profiling. Use `simulateTransaction` on an RPC node to execute the 交易 without landing it on-chain. The simulation response includes `unitsConsumed`, which tells you exactly how many CUs the 交易 used. Add a 10% safety margin to this value: `Math.ceil(unitsConsumed * 1.1)`. This margin 账户 用于 minor variations in CU consumption between simulation 和 actual execution (e.g., different slot, slightly different 账户 state).\n\nSetting the limit exactly to the simulated value is risky because CU consumption can vary slightly between simulation 和 execution. Setting it 2x or 3x higher is wasteful because your priority fee is calculated against the requested limit, not the consumed amount. The 10% margin provides a good balance between safety 和 cost efficiency.\n\n## SetComputeUnitPrice\n\nThis 指令 sets the priority fee in micro-lamports per compute unit. A micro-lamport is one millionth of a lamport (1 lamport = 0.000000001 SOL). The priority fee is calculated as: `priorityFee = ceil(computeUnitLimit * computeUnitPrice / 1,000,000)` lamports.\n\n用于 example, 使用 a CU limit of 200,000 和 a CU price of 5,000 micro-lamports: `ceil(200,000 * 5,000 / 1,000,000) = ceil(1,000) = 1,000 lamports`. This is added on top of the base fee of 5,000 lamports per signature (typically one signature 用于 user 交易).\n\n## Priority fee market dynamics\n\nSolana 验证者 order 交易 within a block by priority fee (micro-lamports per CU). During low-congestion periods, even a CU price of 1 micro-lamport is sufficient. During high-demand events (popular NFT mints, volatile market moments, new token launches), competitive CU prices can reach 100,000+ micro-lamports.\n\nThe priority fee market is highly dynamic. Strategies 用于 choosing the right price include: (1) Static pricing: set a fixed CU price based on the expected congestion level. Simple but often suboptimal. (2) Recent-fee sampling: query `getRecentPrioritizationFees` from the RPC to see what fees landed in recent blocks. Use the median or 75th percentile as your price. (3) Percentile targeting: decide what probability of inclusion you want (e.g., 90% chance of landing in the next block) 和 price accordingly.\n\n## Fee calculation formula\n\nThe total 交易 fee follows this formula:\n\n```\nbaseFee = 5000 lamports (per signature)\npriorityFee = ceil(computeUnitLimit * computeUnitPrice / 1_000_000) lamports\ntotalFee = baseFee + priorityFee\n```\n\nWhen building a 交易 planner, these calculations must use integer arithmetic to match on-chain behavior. Floating-point rounding differences can cause fee estimate mismatches that confuse users.\n\n## 指令 ordering\n\nCompute budget 指令 must appear before any other 指令 in the 交易. The runtime processes them during 交易 validation, before executing program 指令. Placing them after other 指令 is technically allowed but violates convention 和 may cause issues 使用 some tools 和 钱包.\n\n## 实战 recommendations\n- Always include both SetComputeUnitLimit 和 SetComputeUnitPrice\n- Simulate first, then set CU limit to ceil(consumed * 1.1)\n- Sample recent fees 和 use the 75th percentile 用于 reliable inclusion\n- Display the total fee estimate to users before they sign\n- Cap the CU limit at 1,400,000 (Solana maximum per 交易)\n",
            "duration": "50 min"
          },
          "txopt-v2-cost-explorer": {
            "title": "交易 cost estimation 和 fee planning",
            "content": "# 交易 cost estimation 和 fee planning\n\nAccurate fee estimation is the foundation of a good DeFi user experience. Users need to know what a 交易 will cost before they sign it. 验证者 need sufficient fees to prioritize your 交易. Getting fee estimation right means understanding the components, profiling real 交易, 和 adapting to market conditions.\n\n## Components of 交易 cost\n\nA Solana 交易's cost has three components: (1) the base fee, which is 5,000 lamports per signature 和 is fixed by protocol; (2) the priority fee, which is variable 和 determined by the compute unit price you set; 和 (3) the rent cost 用于 any new 账户 created by the 交易 (e.g., creating an Associated Token 账户 costs approximately 2,039,280 lamports in rent-exempt minimum balance).\n\n用于 DeFi 交易 that do not create new 账户, the cost is simply base fee plus priority fee. 用于 交易 that create ATAs or other 账户, the rent deposits significantly increase the total cost 和 should be displayed separately in the UI since rent is recoverable when the 账户 is closed.\n\n## CU profiling\n\nProfiling compute unit consumption across different operation types builds an estimation model. Common DeFi operations 和 their typical CU ranges:\n\n- SOL transfer: 2,000-5,000 CUs\n- SPL token transfer: 4,000-8,000 CUs\n- Create ATA (idempotent): 25,000-35,000 CUs\n- Simple AMM swap (constant product): 60,000-120,000 CUs\n- CLMM swap (concentrated liquidity): 100,000-200,000 CUs\n- Multi-hop route (2 legs): 200,000-400,000 CUs\n- Flash loan + swap: 300,000-600,000 CUs\n\nThese ranges vary based on pool state, tick array crossings in CLMM pools, 和 program version. Profiling your specific use case 使用 simulation produces much more accurate estimates than using generic ranges.\n\n## Fee market analysis\n\nThe priority fee market fluctuates based on network demand. During quiet periods (off-peak hours, low volatility), median priority fees hover around 1-100 micro-lamports per CU. During peak events, fees can spike to 10,000-1,000,000+ micro-lamports per CU.\n\nFetching recent fee data from `getRecentPrioritizationFees` returns fee levels from the last 150 slots. Computing percentiles (25th, 50th, 75th, 90th) from this data provides a fee distribution that informs pricing strategy:\n- 25th percentile: economy — may take multiple blocks to land\n- 50th percentile: standard — lands in 1-2 blocks under normal conditions\n- 75th percentile: fast — high probability of next-block inclusion\n- 90th percentile: urgent — nearly guaranteed next-block inclusion\n\n## Fee tiers 用于 user selection\n\nPresent fee estimates at multiple priority levels so users can choose their urgency. A typical tier structure:\n\n- Low priority: 100 micro-lamports/CU — suitable 用于 non-urgent operations\n- Medium priority: 1,000 micro-lamports/CU — standard DeFi operations\n- High priority: 10,000 micro-lamports/CU — time-sensitive trades\n\nEach tier produces a different total fee: `baseFee + ceil(cuLimit * tierPrice / 1,000,000)`. Display all three alongside estimated confirmation times to help users make informed decisions.\n\n## Dynamic fee adjustment\n\nProduction systems should adjust fee tiers based on real-time market data rather than using static values. Query recent fees every 10-30 seconds 和 update the tier prices to reflect current conditions. During congestion spikes, automatically increase the default tier to ensure 交易 land. During quiet periods, reduce fees to save users money.\n\n## Cost display 最佳实践\n- Show total fee in both lamports 和 SOL equivalent\n- Separate base fee, priority fee, 和 rent deposits\n- Indicate the priority level 和 expected confirmation time\n- Update fee estimates in real-time as market conditions change\n- Warn users when fees are unusually high compared to recent averages\n",
            "duration": "45 min"
          },
          "txopt-v2-tx-plan": {
            "title": "Challenge: Build a 交易 plan 使用 compute budgeting",
            "content": "# Challenge: Build a 交易 plan 使用 compute budgeting\n\nBuild a 交易 planning function that analyzes a set of 指令 和 produces a complete 交易 plan:\n\n- Sum estimatedCU from all 指令 和 add a 10% safety margin (ceiling)\n- Cap the compute unit limit at 1,400,000 (Solana maximum)\n- Calculate priority fee: ceil(computeUnitLimit * computeUnitPrice / 1,000,000)\n- Calculate total fee: base fee (5,000 lamports) + priority fee\n- Count unique 账户 keys across all 指令\n- Add 2 to 指令 count 用于 SetComputeUnitLimit 和 SetComputeUnitPrice\n- Flag needsVersionedTx when unique 账户 exceed 35\n\nYour plan must be fully deterministic -- same input always produces same output.",
            "duration": "50 min"
          }
        }
      },
      "txopt-v2-optimization": {
        "title": "Optimization & Strategy",
        "description": "Address Lookup Table planning, reliability/retry patterns, actionable error UX, 和 full send-strategy reporting.",
        "lessons": {
          "txopt-v2-lut-planner": {
            "title": "Challenge: Plan Address Lookup Table usage",
            "content": "# Challenge: Plan Address Lookup Table usage\n\nBuild a function that determines the optimal Address Lookup Table strategy 用于 a 交易:\n\n- Collect all unique 账户 keys across 指令\n- Check which keys exist in available LUTs\n- Calculate 交易 size: base overhead (200 bytes) + keys * 32 bytes each\n- 使用 LUT: non-LUT keys cost 32 bytes, LUT keys cost 1 byte each\n- Recommend \"legacy\" if the 交易 fits in 1,232 bytes without LUT\n- Recommend \"use-existing-lut\" if LUT keys make it fit\n- Recommend \"create-new-lut\" if it still does not fit even 使用 available LUTs\n- Return byte savings from LUT usage\n\nYour planner must be fully deterministic -- same input always produces same output.",
            "duration": "50 min"
          },
          "txopt-v2-reliability": {
            "title": "Reliability patterns: retry, re-quote, resend vs rebuild",
            "content": "# Reliability patterns: retry, re-quote, resend vs rebuild\n\nProduction DeFi applications must handle 交易 failures gracefully. The difference between a frustrating 和 a reliable experience comes down to retry strategy: knowing when to resend the same 交易, when to rebuild 使用 fresh parameters, 和 when to abort 和 inform the user.\n\n## Failure classification\n\n交易 failures fall into two categories: retryable 和 non-retryable. Correct classification is the foundation of any retry strategy.\n\nRetryable failures include: (1) blockhash expired -- the 交易 was not included in time, re-fetch blockhash 和 resend; (2) network timeout -- the RPC node did not respond, try again or switch nodes; (3) rate limiting (HTTP 429) -- back off 和 retry after the specified delay; (4) node behind -- the RPC node's slot is behind the cluster, try a different node; 和 (5) 交易 not found after send -- may need to resend.\n\nNon-retryable failures include: (1) insufficient funds -- user does not have enough balance; (2) slippage exceeded -- pool price moved beyond tolerance, must re-quote; (3) 账户 does not exist -- expected 账户 is missing; (4) program error 使用 specific error code -- the program logic rejected the 交易; 和 (5) invalid 指令 data -- the 交易 was constructed incorrectly.\n\n## Resend vs rebuild\n\nResending means submitting the exact same signed 交易 bytes again. This is safe because Solana deduplicates 交易 by signature -- if the original 交易 was already processed, the resend is ignored. Resending is appropriate when: the 交易 was sent but confirmation timed out, the RPC node returned a transient error, or you suspect the 交易 was not propagated to the leader.\n\nRebuilding means constructing a new 交易 from scratch 使用 fresh parameters: new blockhash, possibly updated 账户 state, re-simulated CU estimate, 和 new signature. Rebuilding is necessary when: the blockhash expired (cannot resend 使用 stale blockhash), slippage was exceeded (pool state changed, need fresh quote), or 账户 state changed (e.g., ATA was created by another 交易 in the meantime).\n\nThe decision tree is: if the failure is a network/delivery issue, resend; if the failure indicates stale state, rebuild; if the failure indicates a permanent problem (insufficient balance, invalid 指令), abort 使用 a clear error.\n\n## Exponential backoff 使用 jitter\n\nRetry timing must use exponential backoff to avoid overwhelming the network during congestion. The formula is:\n\n```\ndelay = baseDelay * (backoffMultiplier ^ attemptNumber) + random jitter\n```\n\n使用 a base delay of 500ms 和 a 2x multiplier: attempt 1 waits ~500ms, attempt 2 waits ~1,000ms, attempt 3 waits ~2,000ms. Adding random jitter of +/-25% prevents synchronized retries from many clients hitting the same RPC endpoint simultaneously.\n\nCap retries at 3 attempts 用于 user-initiated 交易. More retries introduce unacceptable latency (users do not want to wait 10+ seconds). 用于 backend/automated 交易, higher retry counts (5-10) may be acceptable.\n\n## Blockhash refresh on retry\n\nEvery retry that involves rebuilding must fetch a fresh blockhash. Using the same blockhash across retries is dangerous because the blockhash may have already expired or be close to expiry. The retry flow is: (1) fetch new blockhash, (2) rebuild 交易 message 使用 new blockhash, (3) re-sign 使用 user 钱包 (or programmatic keypair), (4) simulate the rebuilt 交易, (5) send if simulation succeeds.\n\n用于 钱包-connected applications, re-signing requires another user interaction (钱包 popup). To minimize this friction, some applications use durable nonces instead of blockhashes. Durable nonces do not expire, eliminating the need to re-sign on retry. However, durable nonces have their own complexity 和 are not universally supported.\n\n## User-facing retry UX\n\nPresent retry progress clearly: show the attempt number, what went wrong, 和 what is happening next. Example states: \"Sending 交易...\" -> \"交易 not confirmed, retrying (2/3)...\" -> \"Refreshing quote...\" -> \"Success!\" or \"Failed after 3 attempts. [Try Again] [Cancel]\". Never retry silently -- users should always know what is happening 使用 their 交易.\n\n## Checklist\n- Classify every failure as retryable or non-retryable\n- Use exponential backoff (500ms base, 2x multiplier) 使用 jitter\n- Cap retries at 3 用于 user-initiated 交易\n- Refresh blockhash on every rebuild attempt\n- Distinguish resend (same bytes) from rebuild (new 交易)\n- Show retry progress in the UI 使用 clear status messages\n",
            "duration": "50 min"
          },
          "txopt-v2-ux-errors": {
            "title": "UX: actionable error messages 用于 交易 failures",
            "content": "# UX: actionable error messages 用于 交易 failures\n\nRaw Solana error messages are cryptic. \"交易 simulation failed: Error processing 指令 2: custom program error: 0x1771\" tells a developer something but tells a user nothing. Mapping program errors to clear, actionable messages is essential 用于 DeFi application quality.\n\n## Error taxonomy\n\nSolana 交易 errors fall into several categories, each requiring different user-facing treatment:\n\n钱包 errors: insufficient SOL balance, insufficient token balance, 钱包 disconnected, user rejected signature request. These are the most common 和 simplest to handle. The message should state what is missing 和 how to fix it: \"Insufficient SOL balance. You need at least 0.05 SOL to cover 交易 fees. Current balance: 0.01 SOL.\"\n\nProgram errors: these are custom error codes from on-chain programs. Each program defines its own error codes. 用于 example, Jupiter aggregator might return error 6001 用于 \"slippage tolerance exceeded,\" while Raydium returns a different code 用于 the same concept. Maintaining a mapping from program ID + error code to human-readable messages is necessary 用于 each protocol you integrate 使用.\n\nNetwork errors: RPC node unavailable, connection timeout, rate limited. These are transient 和 should be presented 使用 automatic retry: \"Network temporarily unavailable. Retrying in 3 seconds...\" The user should not need to take action unless all retries fail.\n\nCompute errors: compute budget exceeded, 交易 too large. These indicate the 交易 was constructed incorrectly (from the user's perspective). The message should explain the situation 和 offer a solution: \"交易 too complex 用于 a single submission. Splitting into two 交易...\"\n\n## Mapping program errors\n\nThe most important error mappings 用于 DeFi applications:\n\nSlippage exceeded: \"Price moved beyond your tolerance of X%. The swap would give you less than your minimum output of Y tokens. Tap 'Refresh Quote' to get an updated price.\" This is actionable -- the user can refresh 和 try again.\n\nInsufficient liquidity: \"Not enough liquidity in the pool 用于 this swap size. Try reducing the swap amount or using a different route.\" This tells the user what to do.\n\nStale oracle: \"Price oracle data is outdated. This can happen during high volatility. Please wait a moment 和 try again.\" This sets expectations.\n\n账户 not initialized: \"Your token 账户 用于 [TOKEN] needs to be created first. This will cost approximately 0.002 SOL in rent.\" This explains the additional cost.\n\n## Error message principles\n\nGood error messages follow these principles: (1) State what happened in plain language. Not \"Error 0x1771\" but \"The swap price changed too much.\" (2) Explain why it happened. \"Prices move quickly during high volatility.\" (3) Tell the user what to do. \"Tap Refresh to get an updated quote, or increase your slippage tolerance.\" (4) Provide technical details in a collapsible section 用于 power users: \"Program: JUP6LkbZbjS1jKKwapdHNy74zcZ3tLUZoi5QNyVTaV4, Error: 6001 (SlippageToleranceExceeded).\"\n\n## Error recovery flows\n\nEach error category should have a defined recovery flow:\n\nBalance errors: show current balance, required balance, 和 a link to fund the 钱包 or swap 用于 the needed token. Pre-calculate the exact shortfall.\n\nSlippage errors: automatically re-quote 使用 the same parameters. If the new quote is acceptable, present it 使用 a \"Swap at new price\" button. If the price moved significantly, warn the user before proceeding.\n\nTimeout errors: show a 交易 explorer link so the user can verify whether the 交易 actually succeeded. Include a \"Check Status\" button that polls the signature. Many apparent failures are actually successes where the confirmation was slow.\n\nSimulation errors: catch these before sending. If simulation fails, do not prompt the user to sign. Instead, show the mapped error 和 recovery action. This saves users from paying fees on doomed 交易.\n\n## Logging 和 monitoring\n\nLog every error 使用 full context: timestamp, 钱包 address (anonymized), 交易 signature (if available), program ID, error code, mapped message, 和 recovery action taken. This data drives improvements: if 80% of errors are slippage-related, you need better default slippage settings or dynamic adjustment. If compute errors spike, your CU estimation model needs tuning.\n\n## Checklist\n- Map all known program error codes to human-readable messages\n- Include actionable recovery steps in every error message\n- Provide technical details in a collapsible section\n- Automatically re-quote on slippage failures\n- Log all errors 使用 full context 用于 monitoring\n",
            "duration": "45 min"
          },
          "txopt-v2-send-strategy": {
            "title": "Checkpoint: Generate a send strategy report",
            "content": "# Checkpoint: Generate a send strategy report\n\nBuild the final send strategy report that combines all 课程 concepts into a comprehensive 交易 optimization plan:\n\n- Build a tx plan: sum CU estimates 使用 10% margin (capped at 1,400,000), calculate priority fee, count unique 账户 和 total 指令 (+2 用于 compute budget)\n- Plan LUT strategy: calculate sizes 使用 和 without LUT, recommend legacy / use-existing-lut / create-new-lut\n- Generate fee estimates at three priority tiers: low (100 uL/CU), medium (1,000 uL/CU), high (10,000 uL/CU)\n- Include a fixed retry policy: 3 retries, 500ms base delay, 2x backoff, always refresh blockhash\n- Preserve the input timestamp in the output\n\nThis checkpoint validates your complete understanding of 交易 optimization.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "solana-mobile-signing": {
    "title": "Solana Mobile Signing",
    "description": "Master production mobile 钱包 signing on Solana: Android MWA sessions, iOS deep-link constraints, resilient retries, 和 deterministic session telemetry.",
    "duration": "12 hours",
    "tags": [
      "mobile",
      "signing",
      "wallet",
      "mwa",
      "solana"
    ],
    "modules": {
      "mobilesign-v2-fundamentals": {
        "title": "Mobile Signing Fundamentals",
        "description": "Platform constraints, connection UX patterns, signing timeline behavior, 和 typed request construction across Android/iOS.",
        "lessons": {
          "mobilesign-v2-reality-check": {
            "title": "Mobile signing reality check: Android vs iOS constraints",
            "content": "# Mobile signing reality check: Android vs iOS constraints\n\nMobile 钱包 signing on Solana is fundamentally different from browser-based 钱包 interactions. The constraints imposed by Android 和 iOS operating systems shape every 设计 decision, from session management to error handling. Understanding these platform differences is essential before writing any signing code.\n\n## Android 和 Mobile 钱包 Adapter (MWA)\n\nOn Android, the Solana Mobile 钱包 Adapter (MWA) protocol provides a persistent communication channel between dApps 和 钱包 applications. MWA leverages Android's ability to run foreground services, which means the 钱包 application can maintain an active session while the user interacts 使用 the dApp. The protocol uses a WebSocket-like association mechanism: the dApp sends an association intent, the 钱包 responds 使用 a session token, 和 subsequent sign requests flow over this persistent channel.\n\nThe key advantage of MWA on Android is session continuity. Once a user authorizes a dApp, the 钱包 maintains an active session that can handle multiple sign requests without requiring the user to switch applications. The foreground service keeps the communication channel alive even when the 钱包 is not in the foreground. This enables flows like batch signing, sequential 交易 approval, 和 real-time status updates.\n\nAndroid MWA sessions have a lifecycle tied to the association. The dApp initiates an association via an Android intent, receives a session object, 和 can then issue authorize, sign_transactions, sign_messages, 和 sign_and_send_transactions requests. Sessions persist until explicitly deauthorized, the 钱包 terminates them, or the session TTL expires. Typical TTL values range from 5 minutes to 24 hours depending on the 钱包 implementation.\n\nHowever, Android is not without constraints. The user must have a compatible MWA 钱包 installed (Phantom, Solflare, or other MWA-compatible 钱包). The association intent may fail if no compatible 钱包 is found, requiring graceful fallback. Additionally, Android battery optimization 和 Doze mode can interrupt foreground services on some manufacturer-modified Android builds (Samsung, Xiaomi), requiring careful handling of session interruption.\n\n## iOS limitations 和 deep link patterns\n\niOS presents a fundamentally different challenge. Apple does not allow arbitrary background processes or persistent inter-app communication channels. There is no equivalent to Android's foreground service pattern. When a user switches from a dApp (typically a web view or native app) to a 钱包 app, the dApp's execution context is suspended. There is no way to maintain a WebSocket or persistent channel between the two applications.\n\nOn iOS, 钱包 interactions rely on deep links 和 universal links. The dApp constructs a signing request, encodes it into a URL, 和 opens the 钱包 via a deep link. The 钱包 processes the request, 和 returns the result via a callback deep link back to the dApp. Each sign request requires a full app switch: dApp to 钱包, user approval, 钱包 back to dApp.\n\nThis round-trip app switching has significant UX implications. Each signature requires 2-4 seconds of visual context switching. Users see the iOS app transition animation, must locate the approve button in the 钱包, 和 then return to the dApp. Batch signing is particularly painful because each 交易 in the batch requires a separate app switch (unless the 钱包 supports batch approval in a single deep link payload).\n\nSession persistence on iOS is effectively impossible in the traditional sense. The dApp cannot know if the 钱包 is still running, whether the user closed it, or if iOS terminated it 用于 memory pressure. Every request must be treated as potentially the first request in a new session. This means encoding all necessary context (app identity, cluster, authorization state) into every deep link request.\n\n## What actually works in production\n\nProduction mobile dApps adopt a hybrid strategy. On Android, they detect MWA support 和 use the persistent session model. On iOS, they fall back to deep link patterns 使用 aggressive local caching to minimize the data that must be re-transmitted on each request. Cross-platform frameworks like the Solana Mobile SDK abstract some of these differences, but developers must still handle platform-specific edge cases.\n\nFallback patterns include: QR code-based WalletConnect sessions (works on both platforms but adds latency), embedded browser 钱包 (avoid app switching but sacrifice 安全), 和 progressive web app approaches 使用 browser extension 钱包. Each fallback has trade-offs in 安全, UX, 和 feature completeness.\n\nThe most robust approach is capability detection at runtime: check 用于 MWA support, fall back to deep links, 和 ultimately offer QR-based connection as a universal fallback. Each path should provide appropriate UX feedback so users understand why the experience differs across devices.\n\n## Shipping principle 用于 mobile signing\n\n设计 用于 interruption by default. Assume app switches, OS suspension, network drops, 和 钱包 restarts are normal events. A resilient signing flow recovers state quickly 和 keeps users informed at each step.\n\n## Checklist\n- Detect MWA availability on Android before attempting association\n- Implement deep link fallback 用于 iOS 和 non-MWA Android\n- Handle session interruption from OS-level process management\n- Cache session state locally 用于 faster reconnection\n- Provide clear UX 用于 each connection method\n\n## Red flags\n- Assuming MWA works identically on iOS 和 Android\n- Not handling foreground service termination on Android\n- Ignoring deep link callback failures on iOS\n- Hardcoding a single 钱包 without fallback detection\n",
            "duration": "50 min"
          },
          "mobilesign-v2-connection-ux": {
            "title": "钱包 connection UX patterns: connect, reconnect, 和 recovery",
            "content": "# 钱包 connection UX patterns: connect, reconnect, 和 recovery\n\n钱包 connection on mobile is the first interaction users have 使用 your dApp. A smooth connection flow builds trust; a broken one drives users away. This 课时 covers the connection lifecycle, automatic reconnection strategies, network mismatch handling, 和 user-friendly error states.\n\n## Initial connection flow\n\nThe connection flow begins 使用 capability detection. Before presenting any 钱包 UI, your dApp should determine what connection methods are available. On Android, check 用于 installed MWA-compatible 钱包 by attempting to resolve the MWA association intent. On iOS, check 用于 registered deep link handlers. If neither is available, offer a QR code or WalletConnect fallback.\n\nOnce a connection method is selected, the authorization flow begins. 用于 MWA on Android, this involves sending an authorize request 使用 your app identity (name, URI, icon). The 钱包 displays a consent screen showing your dApp's identity 和 requested permissions. Upon approval, the 钱包 returns an auth token 和 the user's public key. Store both: the public key 用于 display 和 交易 building, the auth token 用于 session resumption.\n\n用于 deep link connections on iOS, the flow is: construct an authorize deep link 使用 your app identity 和 callback URI, open the 钱包, wait 用于 the callback deep link 使用 the auth result, 和 parse the response. The response includes the public key 和 optionally a session token 用于 subsequent requests.\n\nConnection state should be persisted locally. Store the 钱包 address, connection method, auth token, 和 timestamp. This enables automatic reconnection on app restart without requiring the user to re-authorize. Use secure storage (Keychain on iOS, EncryptedSharedPreferences on Android) 用于 auth tokens.\n\n## Automatic reconnection\n\nWhen the dApp restarts or returns from background, attempt silent reconnection before showing any 钱包 UI. The reconnection flow checks: is there a stored auth token? Is it still valid (not expired)? Can we re-establish the communication channel?\n\nOn Android 使用 MWA, reconnection involves re-associating 使用 the 钱包 using the stored auth token. If the 钱包 accepts the token, the session resumes transparently. If the token is expired or revoked, fall back to a fresh authorization flow. The key is making this check fast (under 500ms) so the user does not see a loading state.\n\nOn iOS, reconnection is simpler but less reliable. Check if the stored 钱包 address is still valid by verifying the 账户 exists on-chain. The auth token from the previous deep link session may or may not be accepted by the 钱包 on the next interaction. Optimistically display the stored 钱包 address 和 handle re-authorization lazily when the first sign request fails.\n\n## Network mismatch handling\n\nNetwork mismatches occur when the dApp expects one cluster (e.g., mainnet-beta) but the 钱包 is configured 用于 another (e.g., devnet). This is a common source of confusing errors: 交易 build correctly but fail on submission because they reference 账户 that do not exist on the 钱包's configured cluster.\n\nDetection strategies include: requesting the 钱包's current cluster during authorization, comparing the cluster in sign responses against expectations, 和 catching specific RPC errors that indicate cluster mismatch (e.g., 账户 not found 用于 well-known program addresses).\n\nWhen a mismatch is detected, present a clear error message: \"Your 钱包 is connected to devnet, but this dApp requires mainnet-beta. Please switch your 钱包's network 和 reconnect.\" Avoid technical jargon. Some 钱包 support programmatic cluster switching via the MWA protocol; use this when available.\n\n## User-friendly error states\n\nError states must be actionable. Users should always know what happened 和 what to do next. Common error states 和 their UX patterns:\n\n钱包 not found: \"No compatible 钱包 detected. Install Phantom or Solflare to continue.\" Include direct links to app stores.\n\nAuthorization denied: \"钱包 connection was declined. Tap Connect to try again.\" Do not repeatedly prompt; wait 用于 user action.\n\nSession expired: \"Your 钱包 session has expired. Tap to reconnect.\" Attempt silent reconnection first; only show this if silent reconnection fails.\n\nNetwork error: \"Unable to reach the Solana network. Check your internet connection 和 try again.\" Distinguish between local network issues 和 RPC endpoint failures.\n\n钱包 disconnected: \"Your 钱包 was disconnected. This can happen if the 钱包 app was closed. Tap to reconnect.\" On Android, this may indicate the foreground service was killed.\n\n## Recovery patterns\n\nRecovery should be automatic when possible 和 manual when necessary. Implement a connection state machine 使用 states: disconnected, connecting, connected, reconnecting, 和 error. Transitions between states should be deterministic 和 logged 用于 debugging.\n\nThe reconnecting state is critical. When a connected session fails (e.g., the 钱包 app crashes), transition to reconnecting 和 attempt up to 3 silent reconnection attempts 使用 exponential backoff (1s, 2s, 4s). If all attempts fail, transition to error 和 present the manual reconnection UI.\n\n## Checklist\n- Detect available connection methods before showing 钱包 UI\n- Store auth tokens securely 用于 automatic reconnection\n- Handle network mismatch 使用 clear user messaging\n- Implement connection state machine 使用 deterministic transitions\n- Provide actionable error states 使用 recovery options\n\n## Red flags\n- Showing raw error codes to users\n- Repeatedly prompting 用于 authorization after denial\n- Not persisting connection state across app restarts\n- Ignoring network mismatch silently\n",
            "duration": "50 min"
          },
          "mobilesign-v2-timeline-explorer": {
            "title": "Signing session timeline: request, 钱包, 和 response flow",
            "content": "# Signing session timeline: request, 钱包, 和 response flow\n\nUnderstanding the complete lifecycle of a mobile signing request is essential 用于 building reliable dApps. Every sign request passes through multiple stages, each 使用 its own failure modes 和 timing constraints. This 课时 traces a request from construction to final response.\n\n## Request construction phase\n\nThe signing flow begins in the dApp when user action triggers a 交易. The dApp constructs the 交易: fetching a recent blockhash, building 指令, setting the fee payer, 和 serializing the 交易 into a byte array. On mobile, this construction phase must be fast because the user is waiting 用于 the 钱包 to appear.\n\nKey timing constraint: the recent blockhash has a limited validity window (typically 60-90 seconds on mainnet, determined by the slots-per-epoch configuration). If 交易 construction takes too long (e.g., due to slow RPC responses), the blockhash may expire before the 钱包 even sees the 交易. Production dApps pre-fetch blockhashes 和 refresh them periodically.\n\nThe constructed 交易 is encoded (typically base64 用于 MWA, or URL-safe base64 用于 deep links) 和 wrapped in a sign request object. The sign request includes metadata: the app identity, requested cluster, 和 a unique request ID 用于 tracking. On MWA, this is sent over the session channel. On iOS deep links, it is encoded into the URL.\n\n## 钱包-side processing\n\nOnce the 钱包 receives the sign request, it enters its own processing pipeline. The 钱包 decodes the 交易, simulates it (if the 钱包 supports simulation), extracts human-readable information 用于 the approval screen, 和 presents the 交易 details to the user.\n\nSimulation is a critical step. 钱包 like Phantom simulate 交易 before showing them to users, detecting potential failures, extracting token transfer amounts, 和 identifying program interactions. Simulation adds 1-3 seconds to the 钱包-side processing time but significantly improves the user experience by showing accurate fee estimates 和 transfer amounts.\n\nThe approval screen shows: the requesting dApp's identity (name, icon, URI), the 交易 type (transfer, swap, mint, etc.), amounts being transferred, estimated fees, 和 any warnings (e.g., interaction 使用 unverified programs). The user can approve or reject. The time spent on this screen is unpredictable 和 depends entirely on the user.\n\n## Response handling\n\nAfter the user approves (or rejects), the 钱包 constructs 和 returns a response. 用于 approved 交易, the response contains the signed 交易 bytes (the original 交易 使用 the 钱包's signature appended). 用于 rejected 交易, the response contains an error code 和 message.\n\nOn MWA, the response arrives over the same session channel. The dApp receives a callback 使用 the signed 交易 or error. On iOS deep links, the 钱包 opens the dApp's callback URL 使用 the response encoded in the URL parameters or fragment.\n\nResponse parsing must be defensive. Check that the response contains a valid signature, that the 交易 bytes match the original request (to detect tampering), 和 that the response corresponds to the correct request ID. 钱包 may return responses out of order if multiple requests were queued.\n\n## Timeout scenarios\n\nTimeouts are the most challenging failure mode in mobile signing. A timeout can occur at multiple points: during request delivery (the 钱包 never received the request), during user decision (the user walked away), during response delivery (the 钱包 signed but the response was lost), or during submission (the signed 交易 was sent but confirmation timed out).\n\nEach timeout requires a different recovery strategy. Request delivery timeout: retry the request. User decision timeout: show a \"waiting 用于 钱包\" UI 使用 a cancel option. Response delivery timeout: check on-chain 用于 the 交易 signature before retrying (to avoid double-signing). Submission timeout: poll 用于 交易 status before resubmitting.\n\nA reasonable timeout configuration 用于 mobile: 30 seconds 用于 the complete round-trip (request to response), 使用 a 60-second grace period 用于 user decision on the 钱包 side. If the MWA session itself times out, re-associate before retrying. If the deep link callback never arrives, present a manual \"I've approved in my 钱包\" button that triggers a status check.\n\n## The complete timeline\n\nA typical successful signing flow takes 3-8 seconds on Android MWA 和 6-15 seconds on iOS deep links. The breakdown: 交易 construction (0.5-2s), request delivery (0.1-0.5s on MWA, 1-3s on deep link), 钱包 simulation (1-3s), user approval (variable), response delivery (0.1-0.5s on MWA, 1-3s on deep link), 和 交易 submission (0.5-2s).\n\n## Checklist\n- Pre-fetch blockhashes to minimize construction time\n- Include unique request IDs 用于 response correlation\n- Handle all timeout scenarios 使用 appropriate recovery\n- Parse responses defensively 使用 signature validation\n- Provide real-time status feedback during the signing flow\n\n## Red flags\n- Using stale blockhashes that expire during signing\n- Not correlating responses 使用 request IDs\n- Treating all timeouts identically\n- Missing the case where a 交易 was signed but the response was lost\n",
            "duration": "45 min"
          },
          "mobilesign-v2-sign-request": {
            "title": "Challenge: Build a typed sign request",
            "content": "# Challenge: Build a typed sign request\n\nImplement a sign request builder 用于 Mobile 钱包 Adapter:\n\n- Validate the payload type (交易 or message)\n- Validate payload data (base64 用于 交易, non-empty string 用于 messages)\n- Set session metadata (app identity 使用 name, URI, 和 icon)\n- Validate the cluster (mainnet-beta, devnet, or testnet)\n- Generate a request ID if not provided\n- Return a structured SignRequest 使用 validation results\n\nYour implementation will be tested against valid requests, message signing requests, 和 invalid inputs 使用 multiple errors.",
            "duration": "50 min"
          }
        }
      },
      "mobilesign-v2-production": {
        "title": "Production Patterns",
        "description": "Session persistence, 交易-review safety, retry state machines, 和 deterministic session reporting 用于 production mobile apps.",
        "lessons": {
          "mobilesign-v2-session-persist": {
            "title": "Challenge: Session persistence 和 restoration",
            "content": "# Challenge: Session persistence 和 restoration\n\nImplement a session persistence manager 用于 mobile 钱包 sessions:\n\n- Process a sequence of actions: save, restore, clear, 和 expire_check\n- Track 钱包 address 和 last sign request ID across actions\n- Handle session expiry based on TTL 和 timestamps\n- Return the final session state 使用 a complete action log\n\nEach action modifies the session state. Save establishes a session, restore checks if it is still valid, clear removes it, 和 expire_check verifies TTL bounds.",
            "duration": "50 min"
          },
          "mobilesign-v2-review-screens": {
            "title": "Mobile 交易 review: what users need to see",
            "content": "# Mobile 交易 review: what users need to see\n\n交易 review screens are the last line of defense between a user 和 a potentially harmful 交易. On mobile, screen real estate is limited 和 user attention is fragmented. Designing effective review screens requires understanding what information matters, how to present it, 和 what simulation results to surface.\n\n## Human-readable 交易 summaries\n\nRaw 交易 data is meaningless to most users. A 交易 containing a SystemProgram.transfer 指令 should display \"Send 1.5 SOL to 7Y4f...T6aY\" rather than showing serialized 指令 bytes. The translation from on-chain 指令 to human-readable summaries is one of the most important UX challenges in mobile 钱包 development.\n\nSummary generation involves: identifying the program being called (System Program, Token Program, a known DeFi protocol), decoding the 指令 data according to the program's IDL or known layout, extracting the relevant parameters (amounts, addresses, token mints), 和 formatting them 用于 display. Unknown programs should show a warning: \"Interaction 使用 unverified program: Prog1111...\".\n\nAddress formatting on mobile requires truncation. Full Solana addresses (32-44 characters) do not fit on mobile screens. The standard pattern is showing the first 4 和 last 4 characters 使用 an ellipsis: \"7Y4f...T6aY\". Always provide a way to view the full address (tap to expand or copy). 用于 known addresses (well-known programs, token mints), show the human-readable name instead: \"USDC Token Program\" rather than \"EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v\".\n\nToken amounts must include decimals 和 symbols. A raw amount of 1500000 用于 a USDC transfer should display as \"1.50 USDC\", not \"1500000 lamports\". This requires knowing the token's decimal places 和 symbol, which can be fetched from the token mint's metadata or a local registry of known tokens.\n\n## Fee display 和 estimation\n\n交易 fees on Solana are low but not zero. Users should see the estimated fee before approving. The base fee (currently 5000 lamports or 0.000005 SOL) plus any priority fee should be displayed clearly. If the 交易 includes compute budget 指令 that set a custom fee, extract 和 display the total.\n\nFee estimation can use simulation results. The Solana RPC simulateTransaction method returns the compute units consumed, which combined 使用 the priority fee rate gives an accurate fee estimate. Display fees in both SOL 和 the user's preferred fiat currency if possible.\n\n用于 交易 that interact 使用 DeFi protocols, additional costs may apply: swap fees, protocol fees, slippage impact. These should be itemized separately from the network 交易 fee. A swap review screen might show: \"Swap 10 USDC 用于 ~0.05 SOL | Network fee: 0.000005 SOL | Protocol fee: 0.01 USDC | 价格影响: 0.1%\".\n\n## Simulation results\n\n交易 simulation is the most powerful tool 用于 交易 review. Before showing the approval screen, simulate the 交易 和 extract: balance changes (SOL 和 token 账户), new 账户 that will be created, 账户 that will be closed, 和 any errors or warnings.\n\nBalance change summaries are the most intuitive way to present 交易 effects. Show a list of changes: \"-1.5 SOL from your 钱包\", \"+150 USDC to your 钱包\", \"-0.000005 SOL (network fee)\". Color-code decreases (red) 和 increases (green) 用于 quick visual scanning.\n\nSimulation can detect potential issues: insufficient balance, 账户 ownership conflicts, program errors, 和 excessive compute usage. Surface these as warnings before the user approves. A warning like \"This 交易 will fail: insufficient SOL balance\" saves the user from paying a fee 用于 a failed 交易.\n\n## Approval UX patterns\n\nThe approve 和 reject buttons must be unambiguous. Use distinct colors (green 用于 approve, red/grey 用于 reject), sufficient spacing to prevent accidental taps, 和 clear labels (\"Approve\" 和 \"Reject\", not \"OK\" 和 \"Cancel\"). Consider requiring a deliberate gesture (swipe to approve) 用于 high-value 交易.\n\nBiometric confirmation adds 安全 用于 high-value 交易. After the user taps approve, prompt 用于 fingerprint or face recognition before signing. This prevents unauthorized 交易 if the device is unlocked but unattended. Make biometric confirmation optional 和 configurable.\n\nLoading states during signing should show progress: \"Signing 交易...\", \"Submitting to network...\", \"Waiting 用于 confirmation...\". Never show a blank screen or spinner without context. If the process takes longer than expected, show a message: \"This is taking longer than usual. Your 交易 is still processing.\"\n\n## Checklist\n- Translate 指令 to human-readable summaries\n- Truncate addresses 使用 first 4 和 last 4 characters\n- Show token amounts 使用 correct decimals 和 symbols\n- Display simulation-based fee estimates\n- Surface balance changes 使用 color coding\n- Require deliberate approval gestures 用于 high-value 交易\n\n## Red flags\n- Showing raw 指令 bytes to users\n- Displaying token amounts without decimal conversion\n- Missing fee information on approval screens\n- No simulation before 交易 approval\n- Approve 和 reject buttons too close together\n",
            "duration": "45 min"
          },
          "mobilesign-v2-retry-patterns": {
            "title": "One-tap retry: handling offline, rejected, 和 timeout states",
            "content": "# One-tap retry: handling offline, rejected, 和 timeout states\n\nMobile environments are inherently unreliable. Users move between WiFi 和 cellular, enter tunnels, close apps mid-交易, 和 钱包 crash. A robust retry system is not optional; it is a core requirement 用于 production mobile dApps. This 课时 covers retry state machines, offline detection, user-initiated retry, 和 mobile-appropriate backoff strategies.\n\n## Retry state machine\n\nEvery sign request in a mobile dApp should be managed by a state machine 使用 well-defined states 和 transitions. The core states are: idle, pending, signing, submitted, confirmed, failed, 和 retrying. Each state has specific allowed transitions 和 associated UI.\n\nIdle: no active request. Transition to pending when the user initiates an action.\n\nPending: the request is being constructed (fetching blockhash, building 交易). Transition to signing when the request is sent to the 钱包, or to failed if construction fails (e.g., RPC unreachable).\n\nSigning: waiting 用于 钱包 response. Transition to submitted if the 钱包 returns a signed 交易, to failed if the 钱包 rejects, or to retrying if the signing times out.\n\nSubmitted: the signed 交易 has been sent to the network. Transition to confirmed when the 交易 is finalized, or to failed if submission fails or confirmation times out.\n\nConfirmed: terminal success state. Display success UI 和 clean up.\n\nFailed: non-terminal failure state. Analyze the failure reason 和 determine if retry is appropriate. Transition to retrying if the failure is retryable, or remain in failed if it is terminal (e.g., user explicitly rejected).\n\nRetrying: preparing to retry. Refresh stale data (new blockhash, updated balances), wait 用于 backoff period, then transition back to pending.\n\n## Offline detection\n\nMobile offline detection is more nuanced than checking navigator.onLine. That property only indicates whether the device has a network interface active, not whether the Solana RPC endpoint is reachable. Implement a multi-layer detection strategy.\n\nLayer 1: Network interface status. Use the device's network state API to detect complete disconnection (airplane mode, no signal). This is instant 和 covers the most obvious case.\n\nLayer 2: RPC health check. Periodically ping the Solana RPC endpoint 使用 a lightweight request (getHealth or getSlot). If this fails but the network interface is up, the issue is likely RPC-specific. Try a fallback RPC endpoint before declaring offline status.\n\nLayer 3: 交易-level detection. If a 交易 submission returns a network error, mark the request as failed-offline rather than failed-permanent. This distinction drives the retry logic: offline failures should be retried when connectivity returns, while permanent failures (insufficient funds, invalid 交易) should not.\n\nWhen offline is detected, queue pending sign requests locally. Display an offline banner: \"You are offline. Your 交易 will be submitted when connectivity returns.\" When connectivity is restored, process the queue in order, refreshing blockhashes 用于 any queued 交易 (they will have expired).\n\n## User-initiated retry\n\nNot all retries should be automatic. When a 交易 fails, present the user 使用 context 和 a clear retry option. The retry button should be prominent (primary action), 和 the error context should be concise.\n\n用于 钱包 rejection: \"交易 was declined in your 钱包. [Try Again]\". The retry re-opens the 钱包 使用 the same request. Do not automatically retry rejected 交易; respect the user's decision 和 only retry on explicit user action.\n\n用于 timeout: \"钱包 did not respond in time. This may happen if the 钱包 app was closed. [Retry] [Cancel]\". Before retrying, check if the 交易 was already signed 和 submitted (to avoid double-signing).\n\n用于 network errors: \"Could not reach the Solana network. [Retry When Online]\". This button should be disabled while offline 和 automatically trigger when connectivity returns.\n\n用于 submission failures: \"交易 could not be confirmed. [Retry 使用 New Blockhash]\". This re-constructs the 交易 使用 a fresh blockhash 和 re-submits. Show the previous failure reason to build user confidence.\n\n## Exponential backoff on mobile\n\nMobile backoff must be more aggressive than server-side backoff because users are waiting 和 watching. Start 使用 a 1-second delay, double on each retry, 和 cap at 8 seconds. After 3 failed retries, stop automatic retrying 和 present a manual retry option.\n\nThe backoff sequence 用于 automatic retries: 1s, 2s, 4s, then stop. 用于 user-initiated retries, do not apply backoff (the user explicitly chose to retry, so execute immediately). 用于 offline queue processing, use a 2-second delay between queued items to avoid overwhelming the RPC endpoint when connectivity returns.\n\nJitter is important even on mobile. Add a random 0-500ms offset to each retry delay to prevent thundering herd problems when many users come back online simultaneously (e.g., after a widespread network outage).\n\nDisplay retry progress to the user: \"Retrying in 3... 2... 1...\" or \"Attempt 2 of 3\". Never retry silently; users should always know the dApp is working on their behalf.\n\n## Checklist\n- Implement a state machine 用于 every sign request lifecycle\n- Detect offline state at network, RPC, 和 交易 levels\n- Queue 交易 locally when offline\n- Refresh blockhashes before retrying queued 交易\n- Use mobile-appropriate backoff: 1s, 2s, 4s, then manual\n- Show retry progress 和 attempt counts to users\n\n## Red flags\n- Automatically retrying user-rejected 交易\n- Using server-side backoff timing (30s+) on mobile\n- Retrying 使用 stale blockhashes\n- Silently retrying without user visibility\n- Not checking 用于 already-submitted 交易 before retry\n",
            "duration": "50 min"
          },
          "mobilesign-v2-session-report": {
            "title": "Checkpoint: Generate a session report",
            "content": "# Checkpoint: Generate a session report\n\nImplement a session report generator that summarizes a complete mobile signing session:\n\n- Count total requests, successful signs, 和 failed signs\n- Sum retry attempts across all requests\n- Calculate session duration from start 和 end timestamps\n- Break down requests by type (交易 vs message)\n- Produce deterministic JSON output 用于 consistent reporting\n\nThis checkpoint validates your understanding of session lifecycle, request tracking, 和 deterministic output generation.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "solana-pay-commerce": {
    "title": "Solana Pay Commerce",
    "description": "Master Solana Pay commerce integration: robust URL encoding, QR/payment tracking workflows, confirmation UX, 和 deterministic POS reconciliation artifacts.",
    "duration": "12 hours",
    "tags": [
      "solana-pay",
      "commerce",
      "payments",
      "qr",
      "solana"
    ],
    "modules": {
      "solanapay-v2-foundations": {
        "title": "Solana Pay Foundations",
        "description": "Solana Pay specification, URL encoding rigor, transfer request anatomy, 和 deterministic builder/encoder patterns.",
        "lessons": {
          "solanapay-v2-mental-model": {
            "title": "Solana Pay 思维模型 和 URL encoding rules",
            "content": "# Solana Pay 思维模型 和 URL encoding rules\n\nSolana Pay is an open specification 用于 encoding payment requests into URLs that 钱包 can parse 和 execute. Unlike traditional payment processors that rely on centralized intermediaries, Solana Pay enables direct peer-to-peer value transfer by embedding all the information a 钱包 needs into a single URI string. Understanding this specification deeply is the foundation 用于 building any commerce integration on Solana.\n\nThe Solana Pay specification defines two distinct request types: transfer requests 和 交易 requests. Transfer requests are the simpler of the two — they encode a recipient address, an amount, 和 optional metadata directly in the URL. The 钱包 parses the URL, constructs a standard SOL or SPL token transfer 交易, 和 submits it to the network. 交易 requests are more powerful — the URL points to an API endpoint that returns a serialized 交易 用于 the 钱包 to sign. This allows the merchant server to build arbitrarily complex 交易 (multi-指令, program interactions, etc.) while the 钱包 simply signs what it receives.\n\nThe URL format follows a strict schema. A transfer request URL takes the form: `solana:<recipient>?amount=<amount>&spl-token=<mint>&reference=<ref>&label=<label>&message=<msg>&memo=<memo>`. The scheme is always `solana:` (not `solana://`). The recipient is a base58-encoded Solana public key placed immediately after the colon 使用 no slashes. Query parameters encode the payment details.\n\nEach parameter has specific encoding rules. The `amount` is a decimal string representing the number of tokens (not lamports or raw units). 用于 native SOL, `amount=1.5` means 1.5 SOL. 用于 SPL tokens, the amount is in the token's human-readable units respecting its decimals. The `spl-token` parameter is optional — when absent, the transfer is native SOL. When present, it must be the base58-encoded mint address of the SPL token. The `reference` parameter is one or more base58 public keys that are added as non-signer keys in the transfer 指令, enabling 交易 discovery via `getSignaturesForAddress`. The `label` identifies the merchant or payment recipient in a human-readable format. The `message` provides a description of the payment purpose. Both `label` 和 `message` must be URL-encoded using percent-encoding (spaces become `%20`, special characters like `#` become `%23`).\n\nWhen should you use transfer requests versus 交易 requests? Transfer requests are ideal 用于 simple point-of-sale payments where the merchant only needs to receive a fixed amount of a single token. They work entirely client-side — no server needed. 交易 requests are necessary when the payment involves multiple 指令 (e.g., creating an associated token 账户, interacting 使用 a program, splitting payments among multiple recipients, or including on-chain metadata). 交易 requests require a server endpoint that the 钱包 calls to fetch the 交易.\n\nURL encoding correctness is critical. A malformed URL will be rejected by compliant 钱包. Common mistakes include: using `solana://` instead of `solana:`, encoding the recipient address incorrectly, omitting percent-encoding 用于 special characters in labels, 和 providing amounts in raw token units instead of human-readable decimals. The specification requires that all base58 values are valid Solana public keys (32 bytes when decoded), 和 that amounts are non-negative finite decimal numbers.\n\nThe reference key mechanism is what makes Solana Pay 实战 用于 commerce. By generating a unique keypair per 交易 和 including its public key as a reference, the merchant can poll `getSignaturesForAddress(reference)` to detect when the payment arrives. This eliminates the need 用于 webhooks or push notifications — the merchant simply polls until the reference appears in a confirmed 交易, then verifies the transfer details match the expected payment.\n\n## Commerce operator rule\n\nThink in terms of order-state guarantees, not just payment detection:\n1. request created,\n2. payment observed,\n3. payment validated,\n4. fulfillment released.\n\nEach step needs explicit checks so fulfillment never races ahead of verification.\n\n## Checklist\n- Use `solana:` scheme (no double slashes)\n- Place the recipient base58 address directly after the colon\n- Encode label 和 message 使用 encodeURIComponent\n- Use human-readable decimal amounts, not raw lamport values\n- Generate a unique reference keypair per payment 用于 tracking\n\n## Red flags\n- Using `solana://` instead of `solana:`\n- Sending raw lamport amounts in the amount field\n- Forgetting to URL-encode label 和 message parameters\n- Reusing reference keys across multiple payments\n",
            "duration": "50 min"
          },
          "solanapay-v2-transfer-anatomy": {
            "title": "Transfer request anatomy: recipient, amount, reference, 和 labels",
            "content": "# Transfer request anatomy: recipient, amount, reference, 和 labels\n\nA Solana Pay transfer request URL contains everything a 钱包 needs to construct 和 submit a payment 交易. Each component of the URL serves a specific purpose in the payment flow. Understanding the anatomy of these requests — 和 how each field maps to on-chain behavior — is essential 用于 building reliable commerce integrations.\n\nThe recipient address is the most critical field. It appears immediately after the `solana:` scheme 和 must be a valid base58-encoded Solana public key. 用于 native SOL transfers, this is the 钱包 address that will receive the SOL. 用于 SPL token transfers, this is the 钱包 address whose associated token 账户 (ATA) will receive the tokens. The 钱包 application is responsible 用于 deriving the correct ATA from the recipient address 和 the SPL token mint. If the recipient's ATA does not exist, the 钱包 must create it as part of the 交易 (using `createAssociatedTokenAccountIdempotent`). A malformed or invalid recipient address will cause the 钱包 to reject the payment request entirely.\n\nThe amount parameter specifies how much to transfer in human-readable decimal form. 用于 native SOL, `amount=2.5` means 2.5 SOL (2,500,000,000 lamports internally). 用于 USDC (6 decimals), `amount=10.50` means 10.50 USDC (10,500,000 raw units). The 钱包 handles the conversion from decimal to raw units based on the token's decimal configuration. This 设计 keeps the URL readable by humans 和 consistent across tokens 使用 different decimal places. The amount must be a positive finite number — zero, negative, or infinite values are invalid.\n\nThe spl-token parameter distinguishes SOL transfers from SPL token transfers. When omitted, the transfer is native SOL. When present, it must be the base58-encoded mint address of the SPL token to transfer. Common examples include USDC (`EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v`), USDT (`Es9vMFrzaCERmJfrF4H2FYD8hX5F4f1mUQ4v8mBfgsYx`), 和 any other SPL token. The 钱包 validates that the mint exists 和 that the sender has a sufficient balance before constructing the 交易.\n\nThe reference parameter is what makes Solana Pay viable 用于 real-time commerce. A reference is a base58-encoded public key that gets added as a non-signer 账户 in the transfer 指令. After the 交易 confirms, anyone can call `getSignaturesForAddress(reference)` to find the 交易 containing this reference. The merchant generates a unique reference keypair 用于 each payment request, encodes the public key in the URL, 和 then polls the Solana RPC to detect when a matching 交易 appears. Multiple references can be included by repeating the parameter: `reference=<key1>&reference=<key2>`. This is useful when multiple parties need to independently track the same payment.\n\nThe label parameter identifies the merchant or payment recipient. It appears in the 钱包's confirmation dialog so the user knows who they are paying. 用于 example, `label=Sunrise%20Coffee` tells the user they are paying \"Sunrise Coffee.\" The label must be URL-encoded — spaces become `%20`, ampersands become `%26`, 和 other special characters use standard percent-encoding. Keeping labels concise (under 50 characters) ensures they display properly across different 钱包 implementations.\n\nThe message parameter provides additional context about the payment. It might include an order number, item description, or other merchant-specific information. Like the label, it must be URL-encoded. Example: `message=Order%20%23157%20-%202x%20Espresso`. Some 钱包 display the message in a secondary line below the label, while others may truncate long messages. The memo parameter (not to be confused 使用 message) adds an on-chain memo 指令 to the 交易, creating a permanent on-chain record. Use message 用于 display purposes 和 memo 用于 data that must be recorded on-chain.\n\nThe complete flow works as follows: (1) the merchant generates a unique reference keypair, (2) constructs the Solana Pay URL 使用 all parameters, (3) encodes the URL into a QR code or deep link, (4) the customer scans/clicks 和 their 钱包 parses the URL, (5) the 钱包 constructs the transfer 交易 including the reference as a non-signer 账户, (6) the customer approves 和 the 钱包 submits the 交易, (7) the merchant polls `getSignaturesForAddress(reference)` until it finds the confirmed 交易, (8) the merchant verifies the 交易 details match the expected payment.\n\n## Checklist\n- Validate recipient is a proper base58 public key (32-44 characters)\n- Use human-readable decimal amounts matching the token's precision\n- Generate a fresh reference keypair 用于 every payment request\n- URL-encode label 和 message 使用 encodeURIComponent\n- Include spl-token only when transferring SPL tokens, not native SOL\n\n## Red flags\n- Reusing the same reference across multiple payment requests\n- Providing amounts in raw lamports or smallest token units\n- Forgetting URL encoding on label or message (breaks parsing)\n- Not validating the recipient address format before URL construction\n",
            "duration": "50 min"
          },
          "solanapay-v2-url-explorer": {
            "title": "URL builder: live preview of Solana Pay URLs",
            "content": "# URL builder: live preview of Solana Pay URLs\n\nBuilding Solana Pay URLs correctly requires understanding how each parameter contributes to the final encoded string. In this 课时, we walk through the construction process step by step, examining how different combinations of parameters produce different URLs 和 how encoding rules affect the output.\n\nThe base URL always starts 使用 the `solana:` scheme followed by the recipient address. There are no slashes, no host, no path segments — just the scheme colon 和 the base58 address. 用于 example: `solana:7Y4f3Taf6YKqz3Y3h2Hj9uV4UT2Df6gKGfE6q8SxT6aY`. This alone is a valid Solana Pay URL, though it lacks an amount 和 would prompt the 钱包 to request the amount from the user.\n\nAdding query parameters transforms the base URL into a complete payment request. The first parameter is separated from the recipient by `?`, 和 subsequent parameters are separated by `&`. Parameter order does not affect validity, but convention places amount first 用于 readability. A SOL transfer 用于 1.5 SOL: `solana:7Y4f3Taf6YKqz3Y3h2Hj9uV4UT2Df6gKGfE6q8SxT6aY?amount=1.5`.\n\nAdding an SPL token changes the transfer type. Including `spl-token=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v` tells the 钱包 this is a USDC transfer, not a SOL transfer. The amount is still in human-readable form — `amount=10` means 10 USDC, not 10 raw units. The 钱包 reads the mint's decimal configuration from the chain 和 converts accordingly.\n\nThe reference parameter enables payment detection. Each payment should include a unique reference public key. In practice, you generate a Keypair, extract its public key as a base58 string, 和 include it: `reference=Ref1111111111111111111111111111111111111111`. After the customer pays, you poll `getSignaturesForAddress` 使用 this reference to find the 交易. Multiple references can be included 用于 multi-party tracking.\n\nURL encoding 用于 labels 和 messages follows standard percent-encoding rules. The JavaScript function `encodeURIComponent` handles this correctly. Spaces become `%20`, the hash symbol becomes `%23`, ampersands become `%26`, 和 so on. 用于 example, a label \"Joe's Coffee & Tea\" encodes to `label=Joe's%20Coffee%20%26%20Tea`. Failing to encode these characters breaks the URL parser — an unencoded `&` in a label would be interpreted as a parameter separator, splitting the label 和 creating an invalid parameter.\n\nLet us trace through a complete example. A coffee shop wants to charge 4.25 USDC 用于 order number 157. The shop's 钱包 address is `7Y4f3Taf6YKqz3Y3h2Hj9uV4UT2Df6gKGfE6q8SxT6aY`. They generate a reference key `Ref1111111111111111111111111111111111111111`. The resulting URL: `solana:7Y4f3Taf6YKqz3Y3h2Hj9uV4UT2Df6gKGfE6q8SxT6aY?amount=4.25&spl-token=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v&reference=Ref1111111111111111111111111111111111111111&label=Sunrise%20Coffee&message=Order%20%23157`.\n\nValidation before encoding catches errors early. Before building the URL, verify: the recipient is a valid base58 string of 32-44 characters, the amount is a positive finite number, the spl-token (if provided) is a valid base58 string, 和 the reference (if provided) is a valid base58 string. Emitting clear error messages 用于 each validation failure helps developers debug integration issues quickly.\n\nEdge cases to handle: (1) amounts 使用 many decimal places — truncate to the token's decimal precision, (2) empty or whitespace-only labels — omit the parameter entirely rather than including an empty value, (3) extremely long messages — some 钱包 truncate at 256 characters, (4) Unicode characters in labels — encodeURIComponent handles UTF-8 encoding correctly, but test 使用 your target 钱包.\n\n## Checklist\n- Start 使用 `solana:` followed immediately by the recipient address\n- Use `?` before the first parameter 和 `&` between subsequent ones\n- Apply encodeURIComponent to label 和 message values\n- Validate all base58 fields before building the URL\n- Test generated URLs 使用 multiple 钱包 implementations\n\n## Red flags\n- Including raw unencoded special characters in labels or messages\n- Building URLs 使用 invalid or unvalidated recipient addresses\n- Using fixed reference keys instead of generating unique ones per payment\n- Omitting the spl-token parameter 用于 SPL token transfers\n",
            "duration": "45 min"
          },
          "solanapay-v2-encode-transfer": {
            "title": "Challenge: Encode a Solana Pay transfer request URL",
            "content": "# Challenge: Encode a Solana Pay transfer request URL\n\nBuild a function that encodes a Solana Pay transfer request URL from input parameters:\n\n- Validate the recipient address (must be 32-44 characters of valid base58)\n- Validate the amount (must be a positive finite number)\n- Construct the URL 使用 the `solana:` scheme 和 query parameters\n- Apply encodeURIComponent to label 和 message fields\n- Include spl-token 和 reference only when provided\n- Return validation errors when inputs are invalid\n\nYour encoder must be fully deterministic — same input always produces the same URL.",
            "duration": "50 min"
          }
        }
      },
      "solanapay-v2-implementation": {
        "title": "Tracking & Commerce",
        "description": "Reference tracking state machines, confirmation UX, failure handling, 和 deterministic POS receipt generation.",
        "lessons": {
          "solanapay-v2-reference-tracker": {
            "title": "Challenge: Track payment references through confirmation states",
            "content": "# Challenge: Track payment references through confirmation states\n\nBuild a reference tracking state machine that processes payment events:\n\n- States flow: pending -> found -> confirmed -> finalized (or pending -> expired)\n- The \"found\" event transitions from pending 和 records the 交易 signature\n- The \"confirmation\" event increments the confirmation counter 和 transitions from found to confirmed\n- The \"finalized\" event transitions from confirmed to finalized\n- The \"timeout_check\" event expires the reference if still pending after expiryTimeout seconds\n- Record every state transition in a history array 使用 from, to, 和 timestamp\n\nYour tracker must be fully deterministic — same event sequence always produces the same result.",
            "duration": "50 min"
          },
          "solanapay-v2-confirmation-ux": {
            "title": "Confirmation UX: pending, confirmed, 和 expired states",
            "content": "# Confirmation UX: pending, confirmed, 和 expired states\n\nThe user experience during payment confirmation is the most critical moment in any Solana Pay integration. Between the customer scanning the QR code 和 the merchant acknowledging receipt, there is a window of uncertainty that must be managed 使用 clear visual feedback, appropriate timeouts, 和 graceful error handling. Getting this right determines whether customers trust your payment system.\n\nThe confirmation lifecycle follows a well-defined state machine. After the QR code is displayed, the system enters the **pending** state — waiting 用于 the customer to scan 和 submit the 交易. The merchant's system continuously polls `getSignaturesForAddress(reference)` looking 用于 a matching 交易. When a signature appears, the system transitions to the **found** state. The 交易 has been submitted but may not yet be confirmed. The system then calls `getTransaction(signature)` to verify the payment details (recipient, amount, token) match the expected values. Once the 交易 reaches sufficient confirmations, the state moves to **confirmed**. After the 交易 is finalized (maximum commitment level, irreversible), the state reaches **finalized** 和 the merchant can safely release goods or services.\n\nEach state requires distinct visual treatment. In the **pending** state, display the QR code prominently 使用 a scanning animation or subtle pulse effect. Show a countdown timer indicating how long the payment request remains valid (typically 2-5 minutes). Include the amount, token, 和 merchant name so the customer can verify before scanning. A \"Waiting 用于 payment...\" message 使用 a spinner keeps the customer informed.\n\nThe **found** state is brief but important. When the 交易 is detected, immediately replace the QR code 使用 a checkmark or success animation. Display \"Payment detected — confirming...\" to signal progress. This instant visual feedback is critical — customers need to know their payment was received even before it confirms. Show the 交易 signature (abbreviated, e.g., \"sig: abc1...xyz9\") 用于 reference. If you have a Solana Explorer link, provide it.\n\nThe **confirmed** state means the 交易 has at least one confirmation. 用于 low-value 交易 (coffee, small merchandise), this is sufficient to complete the sale. Display a prominent green checkmark, the confirmed amount, 和 the 交易 reference. Print or display a receipt. 用于 high-value 交易, you may want to wait 用于 finalized status before releasing goods.\n\nThe **finalized** state is the strongest guarantee — the 交易 is part of a rooted slot 和 cannot be reverted. This takes roughly 6-12 seconds after initial confirmation. 用于 most point-of-sale applications, waiting 用于 finalized is unnecessary 和 adds friction. However, 用于 digital goods delivery, API key provisioning, or any irreversible fulfillment, finalized is the safe threshold.\n\nThe **expired** state handles the timeout case. If no matching 交易 appears within the expiry window (e.g., 120 seconds), the payment request expires. Display \"Payment request expired\" 使用 an option to generate a new QR code. Never silently expire — the customer may have just scanned 和 needs to know the request is no longer valid. The expiry timeout should be generous enough 用于 the customer to open their 钱包, review the 交易, 和 approve it (60-120 seconds minimum).\n\nError states require careful messaging. \"交易 not found after timeout\" suggests the customer did not complete the payment. \"交易 found but details mismatch\" indicates a potential issue — the amount or recipient does not match expectations. \"Network error during polling\" should trigger automatic retries before displaying an error to the user. Always provide actionable next steps: \"Try again,\" \"Generate new QR,\" or \"Contact support.\"\n\nPolling strategy affects both UX responsiveness 和 RPC load. Start polling immediately after displaying the QR code. Use a 1-second interval 用于 the first 30 seconds (fast detection), then slow to 2-3 seconds 用于 the remainder of the window. After detecting the 交易, switch to polling `getTransaction` 使用 increasing commitment levels: processed -> confirmed -> finalized. Use exponential backoff if the RPC returns errors.\n\nAccessibility considerations 用于 payment confirmation: (1) Do not rely solely on color to indicate state — use icons, text labels, 和 animations. (2) Provide audio feedback (a subtle chime on confirmation) 用于 environments where the screen may not be visible. (3) Ensure the QR code has sufficient contrast 和 size 用于 scanning from a reasonable distance (at least 300x300 pixels). (4) Support both light 和 dark themes 用于 the confirmation UI.\n\n## Checklist\n- Show distinct visual states: pending, found, confirmed, finalized, expired\n- Display a countdown timer during the pending state\n- Provide instant visual feedback when the 交易 is detected\n- Implement appropriate expiry timeouts (60-120 seconds)\n- Offer actionable next steps on expiry or error\n\n## Red flags\n- No visual feedback between QR display 和 confirmation\n- Silent expiry without notifying the customer\n- Waiting 用于 finalized on low-value point-of-sale 交易\n- Polling too aggressively (every 100ms) 和 overloading the RPC\n",
            "duration": "45 min"
          },
          "solanapay-v2-error-handling": {
            "title": "Error handling 和 edge cases in payment flows",
            "content": "# Error handling 和 edge cases in payment flows\n\nProduction payment systems encounter a wide range of failure modes that must be handled gracefully. Solana Pay integrations face challenges unique to blockchain payments: network congestion, RPC failures, partial 交易 visibility, 和 edge cases around token 账户. Building robust error handling separates demo-quality code from production-grade commerce systems.\n\nRPC connectivity failures are the most common operational issue. The merchant's polling loop depends on a reliable connection to a Solana RPC endpoint. When the RPC is unreachable (network outage, rate limiting, endpoint downtime), the polling loop must not crash or silently stop. Implement retry logic 使用 exponential backoff: first retry after 500ms, second after 1 second, third after 2 seconds, capping at 5 seconds between retries. After 5 consecutive failures, display a degraded-mode warning to the operator (\"Network connectivity issue — payment detection may be delayed\") while continuing to retry in the background. Never abandon polling due to transient RPC errors.\n\nRate limiting from RPC providers is a specific failure mode. Free-tier RPC endpoints (including the public Solana RPC) enforce request limits. A polling loop that fires every second generates 60+ requests per minute per active payment session. If you have 10 concurrent payment sessions, that is 600+ requests per minute. Solutions: use a dedicated RPC provider 使用 higher limits, batch reference checks where possible, implement client-side request deduplication, 和 cache negative results (reference not found) 用于 a short window before re-checking.\n\n交易 mismatch errors occur when a 交易 is found via the reference but its details do not match expectations. This can happen if: (1) someone accidentally or maliciously sent a 交易 that includes the reference key but 使用 wrong amounts, (2) the customer used a different 钱包 that interpreted the URL differently, or (3) there is a bug in the URL encoding that produced incorrect parameters. When a mismatch is detected, log the full 交易 details 用于 debugging, display a clear error to the merchant (\"Payment detected but amount does not match — expected 10 USDC, received 5 USDC\"), 和 do not mark the payment as complete.\n\nInsufficient balance errors are caught by the customer's 钱包 before submission, but the merchant has no visibility into this. From the merchant's perspective, it looks like the customer scanned the QR but never submitted the 交易. The timeout/expiry mechanism handles this case — after the expiry window passes, offer to regenerate the QR code. Consider displaying a message like \"If you are having trouble, please ensure you have sufficient balance.\"\n\nAssociated token 账户 (ATA) creation failures can occur when the customer's 钱包 does not automatically create the recipient's ATA 用于 the SPL token being transferred. This is primarily a concern 用于 less common SPL tokens where the recipient may not have an existing ATA. Modern 钱包 handle this by including a `createAssociatedTokenAccountIdempotent` 指令, but older 钱包 versions may not. The merchant can mitigate this by pre-creating ATAs 用于 all tokens they accept.\n\nDouble-payment detection is essential. If the polling loop detects two 交易 使用 the same reference, this indicates either a 钱包 bug or a user submitting the payment twice. The system should only process the first valid 交易 和 flag any subsequent ones 用于 manual review. Track processed references in a database to prevent duplicate fulfillment.\n\nNetwork congestion causes delayed 交易 confirmation. During high-traffic periods, 交易 may take 10-30 seconds to confirm instead of the usual 400ms-2 seconds. The payment UI should handle this gracefully: extend the visual \"confirming\" state, show a message like \"Network is busy — confirmation may take longer than usual,\" 和 never time out a 交易 that has been detected but not yet confirmed. The timeout should only apply to the initial pending state (waiting 用于 any 交易 to appear), not to the confirmation stage.\n\nPartial visibility is a subtle edge case. Due to RPC node propagation delays, one RPC node may see a 交易 while another does not. If your system uses multiple RPC endpoints (用于 redundancy), you may detect a 交易 on one endpoint 和 fail to fetch its details from another. Solution: when a signature is found, retry `getTransaction` against the same endpoint that returned the signature, 使用 retries 和 backoff, before falling back to alternative endpoints.\n\nMemo 和 metadata validation should verify that any on-chain memo matches the expected payment metadata. If the merchant includes a `memo` parameter in the Solana Pay URL, the confirmed 交易 should contain a corresponding memo 指令. Mismatches may indicate URL tampering.\n\n## Checklist\n- Implement exponential backoff 用于 RPC failures (500ms, 1s, 2s, 5s cap)\n- Verify 交易 details match expected payment parameters\n- Handle double-payment detection 使用 reference deduplication\n- Distinguish between pending timeout 和 confirmation timeout\n- Pre-create ATAs 用于 all accepted SPL tokens\n\n## Red flags\n- Crashing the polling loop on a single RPC error\n- Marking payments complete without verifying amount 和 recipient\n- Not handling network congestion gracefully (premature timeout)\n- Ignoring double-payment scenarios\n",
            "duration": "50 min"
          },
          "solanapay-v2-pos-receipt": {
            "title": "Checkpoint: Generate a POS receipt",
            "content": "# Checkpoint: Generate a POS receipt\n\nBuild the final POS receipt generator that combines all 课程 concepts:\n\n- Reconstruct the Solana Pay URL from payment data (recipient, amount, spl-token, reference, label)\n- Generate a deterministic receipt ID from the reference suffix 和 timestamp\n- Determine currency type: \"SPL\" if splToken is present, otherwise \"SOL\"\n- Include merchant name from the payment label\n- Include the tracking status from the reference tracker\n- Output must be stable JSON 使用 deterministic key ordering\n\nThis checkpoint validates your complete understanding of Solana Pay commerce integration.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "wallet-ux-engineering": {
    "title": "钱包 UX Engineering",
    "description": "Master production 钱包 UX engineering on Solana: deterministic connection state, network safety, RPC resilience, 和 measurable reliability patterns.",
    "duration": "12 hours",
    "tags": [
      "wallet",
      "ux",
      "connection",
      "rpc",
      "solana"
    ],
    "modules": {
      "walletux-v2-fundamentals": {
        "title": "Connection Fundamentals",
        "description": "钱包 connection 设计, network gating, 和 deterministic state-machine architecture 用于 predictable onboarding 和 reconnect paths.",
        "lessons": {
          "walletux-v2-connection-design": {
            "title": "Connection UX that doesn't suck: a 设计 checklist",
            "content": "# Connection UX that doesn't suck: a 设计 checklist\n\n钱包 connection is the first interaction a user has 使用 any Solana dApp. If this experience is slow, confusing, or error-prone, most users will leave before they ever reach your core product. Connection UX deserves the same engineering rigor as any critical user flow, yet most teams treat it as an afterthought. This 课时 establishes the 设计 patterns, failure modes, 和 recovery strategies that separate professional 钱包 integration from broken prototypes.\n\n## The connection lifecycle\n\nA 钱包 connection progresses through a predictable sequence: idle (no 钱包 detected), detecting (scanning 用于 installed adapters), ready (adapter found, user has not yet approved), connecting (approval dialog shown, waiting 用于 user action), connected (public key received, session active), 和 disconnected (user or app terminated the session). Each state must have a distinct visual representation so users always know what is happening 和 what they need to do next.\n\nAuto-connect is the single most impactful UX optimization. When a user has previously connected a specific 钱包, the dApp should attempt to reconnect silently on page load without showing a 钱包 selection modal. The Solana 钱包 adapter standard supports this via the `autoConnect` flag. However, auto-connect must be gated: only attempt it if the user previously granted permission (stored in localStorage), 和 set a timeout of 3-5 seconds. If auto-connect fails silently, fall back to showing the connect button without an error message. Users should never see an error 用于 a background reconnection attempt they did not initiate.\n\n## Loading states 和 skeleton UI\n\nDuring the connecting phase, display a skeleton version of the 钱包-dependent UI rather than a blank screen or spinner. If your app shows a token balance after connection, render a shimmer placeholder in that exact layout position. This technique, called \"optimistic layout reservation,\" prevents jarring content shifts when the connection resolves. The connect button itself should transition to a loading state (disabled, 使用 a subtle animation) to prevent double-click issues.\n\nConnection timeouts need explicit handling. If the 钱包 adapter does not respond within 10 seconds, assume the user closed the approval dialog or the 钱包 extension is unresponsive. Transition to an error state 使用 a clear message: \"Connection timed out. Please try again or check your 钱包 extension.\" Never leave the UI in an indefinite loading state. Implement a deterministic timeout using setTimeout 和 clear it if the connection resolves.\n\n## Error recovery patterns\n\nConnection errors fall into three categories: user-rejected (the user clicked \"Cancel\" in the 钱包 dialog), adapter errors (the 钱包 extension crashed or is not installed), 和 network errors (the RPC endpoint is unreachable after connection). Each category requires a different recovery path.\n\nUser-rejected connections should return to the idle state quietly. Do not show an error toast or modal 用于 a deliberate user action. Simply reset the connect button to its default state. If you want to provide a nudge, a subtle inline message like \"Connect your 钱包 to continue\" is sufficient.\n\nAdapter errors require actionable guidance. If no 钱包 is detected, show a \"Get a 钱包\" link that opens the Phantom or Solflare installation page. If the adapter throws an unexpected error, display the error message 使用 a \"Try Again\" button. Log the error details to your analytics system 用于 debugging, but keep the user-facing message simple.\n\nNetwork errors after connection are particularly tricky because the 钱包 is technically connected (you have the public key) but the app cannot fetch on-chain data. Display a degraded state: show the connected 钱包 address 使用 a warning badge, disable 交易 buttons, 和 provide a \"Check Connection\" button that re-tests the RPC endpoint. Do not disconnect the 钱包 just because the RPC is temporarily unreachable.\n\n## Multi-钱包 support\n\nModern Solana dApps must support multiple 钱包 adapters. The 钱包 selection modal should display installed 钱包 prominently (使用 a green \"Detected\" badge) 和 list popular uninstalled 钱包 below 使用 \"Install\" links. Sort installed 钱包 by most recently used. Remember the user's last 钱包 choice 和 pre-select it on subsequent visits.\n\nWhen the user switches 钱包 (disconnects one, connects another), all cached data tied to the previous 钱包 address must be invalidated. Token balances, 交易 history, 和 program-derived 账户 states are all 钱包-specific. Failing to clear this cache causes data leakage between 账户, which is both a UX bug 和 a potential 安全 issue.\n\n## The checklist\n\n- Implement auto-connect 使用 a 3-5 second timeout 用于 returning users\n- Show skeleton UI during the connecting phase to prevent layout shift\n- Set a 10-second hard timeout on connection attempts\n- Handle user-rejected connections silently (no error state)\n- Provide \"Get a 钱包\" links when no adapter is detected\n- Display degraded UI (not disconnect) when RPC fails post-connection\n- Invalidate all 钱包-specific caches on 账户 switch\n- Remember the user's preferred 钱包 adapter between sessions\n- Disable 交易 buttons during connecting 和 error states\n- Log connection errors to analytics 用于 monitoring adapter reliability\n\n## Reliability principle\n\n钱包 UX is reliability UX. Users judge trust by whether connect, reconnect, 和 recovery behave predictably under stress, not by visual polish alone.\n",
            "duration": "50 min"
          },
          "walletux-v2-network-gating": {
            "title": "Network gating 和 wrong-network recovery",
            "content": "# Network gating 和 wrong-network recovery\n\nSolana has multiple clusters: mainnet-beta, devnet, testnet, 和 localnet. Unlike EVM chains where the 钱包 controls the network 和 emits chain-change events, Solana's network selection is typically controlled by the dApp, not the 钱包. This architectural difference creates a unique set of UX challenges around network mismatch, gating, 和 recovery.\n\n## The network mismatch problem\n\nWhen a dApp targets mainnet-beta but a user's 钱包 or the app's RPC endpoint points to devnet, 交易 will fail silently or produce confusing results. 账户 addresses are the same across clusters, but 账户 state differs entirely. A token 账户 that holds 1000 USDC on mainnet might not exist on devnet. If your app fetches the balance from devnet while the user expects mainnet, they see zero balance 和 assume the app is broken or their funds are gone.\n\nNetwork mismatch is not always obvious. The 钱包 might report a successful signature, but the 交易 was submitted to a different cluster than the one your app is reading from. This creates phantom 交易: the user sees \"交易 confirmed\" but no state change in the UI. Debugging this requires checking which cluster the 交易 was submitted to versus which cluster the app is polling.\n\n## Detecting the current network\n\nThe primary detection method is to check your RPC endpoint's genesis hash. Each Solana cluster has a unique genesis hash. Call `getGenesisHash()` on your connection 和 compare it to known values: mainnet-beta's genesis hash is `5eykt4UsFv8P8NJdTREpY1vzqKqZKvdpKuc147dw2N9d`, devnet is `EtWTRABZaYq6iMfeYKouRu166VU2xqa1wcaWoxPkrZBG`, 和 testnet is `4uhcVJyU9pJkvQyS88uRDiswHXSCkY3zQawwpjk2NsNY`. If the genesis hash does not match your expected cluster, the RPC endpoint is misconfigured.\n\n用于 钱包-side detection, some 钱包 adapters expose network information, but this is not standardized. The most reliable approach is to perform a lightweight RPC call (getGenesisHash or getEpochInfo) immediately after connection 和 compare the response against your expected cluster configuration.\n\n## Network gating patterns\n\nNetwork gating prevents users from performing actions on the wrong network. There are two levels of gating: soft gating 和 hard gating.\n\nSoft gating shows a warning banner but allows the user to continue. This is appropriate 用于 development tools, block explorers, 和 apps that intentionally support multiple clusters. The banner should clearly state the current network, use color coding (green 用于 mainnet, yellow 用于 devnet, red 用于 testnet/localnet), 和 be persistent (not dismissible) so the user always sees it.\n\nHard gating blocks all interactions until the network matches the expected cluster. This is appropriate 用于 production DeFi applications where operating on the wrong network could cause real financial loss. Hard gating should display a full-screen overlay or modal 使用 a clear message: \"This app requires Mainnet Beta. Your connection is currently pointing to Devnet.\" Include a button to switch the RPC endpoint if your app supports runtime endpoint switching.\n\n## Recovery strategies\n\nWhen a network mismatch is detected, the recovery flow depends on who controls the network selection. In most Solana dApps, the app controls the RPC endpoint, so recovery means updating the app's connection object to point to the correct cluster. This can be done automatically (if the correct endpoint is known) or manually (presenting the user 使用 a network selector).\n\nIf recovery requires the user to change their 钱包's network setting (less common on Solana but possible 使用 some 钱包), provide step-by-step 指令 specific to the detected 钱包 adapter. 用于 Phantom: \"Open Phantom > Settings > Developer Settings > Change Network.\" Include screenshots or a link to the 钱包's documentation.\n\nAfter network switching, all cached data must be invalidated. 账户 states, token balances, 交易 history, 和 program-derived addresses may differ across clusters. Implement a `networkChanged` event handler that: clears all cached RPC responses, resets the connection state machine, re-fetches critical 账户 data, 和 updates the UI to reflect the new network.\n\n## Multi-network development workflow\n\n用于 developers building on Solana, supporting seamless network switching during development is essential. Store the selected network in localStorage so it persists across page reloads. Provide a developer-only network switcher (hidden behind a feature flag or only visible in non-production builds) that allows quick toggling between mainnet, devnet, 和 localnet.\n\nWhen switching networks programmatically, create a new Connection object rather than mutating the existing one. This prevents race conditions where in-flight requests on the old network collide 使用 new requests on the new network. The connection switch should be atomic: update the connection reference, clear all caches, 和 trigger a full data refresh in a single synchronous operation.\n\n## Checklist\n- Check genesis hash immediately after RPC connection to verify the cluster\n- Use color-coded persistent banners to indicate the current network\n- Hard-gate production DeFi apps to the expected cluster\n- Invalidate all caches when the network changes\n- Create new Connection objects instead of mutating existing ones\n- Store network preference in localStorage 用于 persistence\n- Provide 钱包-specific 指令 用于 network switching\n\n## Red flags\n- Allowing 交易 on the wrong network without any warning\n- Caching data across network switches (stale cross-network data)\n- Mutating the Connection object during network switch (race conditions)\n- Assuming 钱包 和 dApp are always on the same cluster\n",
            "duration": "50 min"
          },
          "walletux-v2-state-explorer": {
            "title": "Connection state machine: states, events, 和 transitions",
            "content": "# Connection state machine: states, events, 和 transitions\n\n钱包 connection logic in most dApps is implemented as a tangle of boolean flags, useEffect hooks, 和 conditional renders. This approach leads to impossible states (loading 和 error simultaneously), missed transitions (forgetting to clear the error when retrying), 和 race conditions (two connection attempts running in parallel). A finite state machine (FSM) eliminates these problems by making every possible state 和 transition explicit.\n\n## Why state machines 用于 钱包 connections\n\nA state machine defines a finite set of states, a finite set of events, 和 a deterministic transition function that maps (currentState, event) to nextState. At any point in time, the system is in exactly one state. This guarantees that impossible combinations (connected 和 disconnected) cannot occur. Every event is either handled by the current state or explicitly rejected, eliminating silent failures.\n\n用于 钱包 connections, the core states are: `disconnected` (no active session), `connecting` (waiting 用于 钱包 approval or RPC confirmation), `connected` (session active, public key available), 和 `error` (something went wrong). Each state maps to a specific UI presentation, specific allowed user actions, 和 specific side effects.\n\n## Defining the transition table\n\nThe transition table is the heart of the state machine. It specifies which events are valid in which states 和 what the resulting state should be:\n\n```\ndisconnected + CONNECT       → connecting\nconnecting   + CONNECTED     → connected\nconnecting   + CONNECTION_ERROR → error\nconnecting   + TIMEOUT       → error\nconnected    + DISCONNECT    → disconnected\nconnected    + NETWORK_CHANGE → connected (with updated network)\nconnected    + ACCOUNT_CHANGE → connected (with updated address)\nconnected    + CONNECTION_LOST → error\nerror        + RETRY         → connecting\nerror        + DISCONNECT    → disconnected\n```\n\nAny event not listed 用于 a given state is invalid. Invalid events should transition to the error state 使用 a descriptive message rather than being silently ignored. This makes debugging straightforward: every unexpected event is captured 和 logged.\n\n## Side effects 和 context\n\nState transitions carry context (also called \"extended state\" or \"context\"). The connection state machine tracks: `walletAddress` (set on CONNECTED 和 ACCOUNT_CHANGE events), `network` (set on CONNECTED 和 NETWORK_CHANGE events), `errorMessage` (set when entering the error state), 和 `transitions` (a log of all state transitions 用于 debugging).\n\nSide effects are actions triggered by transitions, not by states. 用于 example, the transition from `connecting` to `connected` should trigger: fetching the initial 账户 balance, subscribing to 账户 change notifications, 和 logging the connection event to analytics. The transition from `connected` to `disconnected` should trigger: clearing all cached data, unsubscribing from notifications, 和 resetting the UI to the idle layout.\n\n## Implementation patterns\n\nIn React applications, the state machine can be implemented using `useReducer` 使用 the transition table as the reducer logic. The reducer receives the current state 和 an event (action), looks up the transition in the table, 和 returns the new state 使用 updated context. This approach is testable (pure function), predictable (no side effects in the reducer), 和 composable (multiple components can read the state without duplicating logic).\n\n用于 more complex scenarios, libraries like XState provide first-class support 用于 statecharts (hierarchical state machines 使用 guards, actions, 和 services). XState's visualizer can render the state machine as a diagram, making it easy to verify that all states 和 transitions are covered. However, 用于 钱包 connection logic, a simple transition table in a useReducer is usually sufficient.\n\nThe transition history array is invaluable 用于 debugging. When a user reports a connection issue, the transition log shows exactly what happened: which events fired, in what order, 和 what states resulted. This is far more useful than a single boolean flag or an error message captured at an arbitrary point.\n\n## 测试 state machines\n\nState machines are inherently testable because they are pure functions. Given a starting state 和 a sequence of events, the output is completely deterministic. Test cases should cover: the happy path (disconnected → connecting → connected), error recovery (connecting → error → retry → connecting → connected), 账户 switching (connected → ACCOUNT_CHANGE → connected 使用 new address), 和 invalid events (connected + CONNECT should transition to error, not silently ignored).\n\nEdge cases to test: rapid event sequences (CONNECT followed immediately by DISCONNECT before the connection resolves), duplicate events (two CONNECTED events in a row), 和 state persistence (does the machine correctly restore state from localStorage on page reload?).\n\n## Checklist\n- Define all states explicitly: disconnected, connecting, connected, error\n- Map every valid (state, event) pair to a next state\n- Handle invalid events by transitioning to error 使用 a descriptive message\n- Track transition history 用于 debugging\n- Implement the state machine as a pure reducer function\n- Clear context data (钱包 address, network) on disconnect\n- Clear error message on retry\n",
            "duration": "45 min"
          },
          "walletux-v2-connection-state": {
            "title": "Challenge: Implement 钱包 connection state machine",
            "content": "# Challenge: Implement 钱包 connection state machine\n\nBuild a deterministic state machine 用于 钱包 connection management:\n\n- States: disconnected, connecting, connected, error\n- Process a sequence of events 和 track all state transitions\n- CONNECTED 和 ACCOUNT_CHANGE events carry a walletAddress; CONNECTED 和 NETWORK_CHANGE carry a network\n- Error state stores the error message; disconnected clears all session data\n- Invalid events force transition to error state 使用 a descriptive message\n- Track transition history as an array of {from, event, to} objects\n\nThe state machine must be fully deterministic — same event sequence always produces same result.",
            "duration": "50 min"
          }
        }
      },
      "walletux-v2-production": {
        "title": "Production Patterns",
        "description": "Cache invalidation, RPC resilience 和 health monitoring, 和 measurable 钱包 UX quality reporting 用于 production operations.",
        "lessons": {
          "walletux-v2-cache-invalidation": {
            "title": "Challenge: Cache invalidation on 钱包 events",
            "content": "# Challenge: Cache invalidation on 钱包 events\n\nBuild a cache invalidation engine that processes 钱包 events 和 invalidates the correct cache entries:\n\n- Cache entries have tags: \"账户\" (钱包-specific data), \"network\" (cluster-specific data), \"global\" (persists across everything)\n- ACCOUNT_CHANGE invalidates all entries tagged \"账户\"\n- NETWORK_CHANGE invalidates entries tagged \"network\" 和 \"账户\" (network change means all 账户 data is stale)\n- DISCONNECT invalidates all non-\"global\" entries\n- Track per-event invalidation counts in an event log\n- Return the final cache state, total invalidated count, 和 retained count\n\nThe invalidation logic must be deterministic — same input always produces same output.",
            "duration": "50 min"
          },
          "walletux-v2-rpc-caching": {
            "title": "RPC reads 和 caching strategy 用于 钱包 apps",
            "content": "# RPC reads 和 caching strategy 用于 钱包 apps\n\nEvery interaction in a Solana 钱包 application ultimately depends on RPC calls: fetching balances, loading token 账户, reading program state, 和 confirming 交易. Without a caching strategy, your app hammers the RPC endpoint 使用 redundant requests, drains rate limits, 和 delivers a sluggish user experience. A well-designed cache layer transforms 钱包 apps from painfully slow to instantly responsive while keeping data fresh enough 用于 financial accuracy.\n\n## The RPC cost problem\n\nSolana RPC calls are not free. Public endpoints like those provided by Solana Foundation have aggressive rate limits (typically 40 requests per 10 seconds 用于 free tiers). Premium providers (Helius, QuickNode, Triton) charge per request or by compute units consumed. A naive 钱包 app that re-fetches every piece of data on every render can easily exceed 100 requests per minute 用于 a single user. Multiply by thousands of concurrent users 和 costs become significant.\n\nBeyond cost, latency kills UX. A `getTokenAccountsByOwner` call takes 200-800ms depending on the endpoint 和 账户 complexity. If the user switches tabs 和 returns, re-fetching everything from scratch creates a noticeable loading delay. Caching eliminates this delay 用于 data that has not changed.\n\n## Cache taxonomy\n\nNot all RPC data has the same freshness requirements. Categorize cache entries by their volatility:\n\n**Immutable data** (cache indefinitely): mint metadata (name, symbol, decimals, logo URI), program 账户 structures, 和 historical 交易 details. Once fetched, this data never changes. Store it in an in-memory Map 使用 no expiration.\n\n**Semi-stable data** (cache 用于 30-60 seconds): token balances, staking positions, 治理 votes, 和 NFT ownership. This data changes infrequently 用于 most users. A 30-second TTL (time to live) provides a good balance between freshness 和 efficiency. Use a cache key that includes the 钱包 address 和 network to prevent cross-账户 contamination.\n\n**Volatile data** (cache 用于 5-10 seconds or not at all): recent 交易 confirmations, real-time price feeds, 和 active swap quotes. This data changes constantly 和 becomes stale quickly. Short TTLs or no caching at all is appropriate. 用于 交易 confirmations, use WebSocket subscriptions instead of polling.\n\n## Cache key 设计\n\nCache keys must uniquely identify the request parameters 和 the context. A good cache key 用于 a balance query includes: the RPC method name, the 账户 address, the commitment level, 和 the network cluster. 用于 example: `getBalance:7xKXp...abc:confirmed:mainnet-beta`. Including the network in the key prevents a critical bug: returning devnet data when the user has switched to mainnet.\n\n用于 `getTokenAccountsByOwner`, the key should include the owner address 和 the program filter (TOKEN_PROGRAM_ID or TOKEN_2022_PROGRAM_ID). Different token programs return different 账户 sets, 和 caching them under the same key returns incorrect results.\n\n## Invalidation triggers\n\nCache invalidation is triggered by three 钱包 events: 账户 change, network change, 和 disconnect. These events were covered in the previous challenge, but the caching layer adds nuance.\n\n账户 change invalidates all entries keyed by the 钱包 address. Token balances, 交易 history, 和 program-derived 账户 states are all 钱包-specific. Global data (mint metadata, program IDL) survives an 账户 change.\n\nNetwork change invalidates everything except truly global, network-independent data (UI preferences, theme settings). Even mint metadata should be invalidated because a mint address might exist on mainnet but not on devnet, or have different state.\n\nUser-initiated refresh is the escape hatch. Provide a \"Refresh\" button that clears the entire cache 和 re-fetches all visible data. Users expect this when they know an external action (a transfer from another device) has changed their state but the cache has not expired yet.\n\n## Stale-while-revalidate pattern\n\nThe most effective caching strategy 用于 钱包 apps is stale-while-revalidate (SWR). When a cache entry is requested: if fresh (within TTL), return it immediately. If stale (past TTL but within a grace period, e.g., 2x TTL), return the stale value immediately 和 trigger a background re-fetch. When the re-fetch completes, update the cache 和 notify the UI. If expired (past grace period), block 和 re-fetch before returning.\n\nThis pattern ensures the UI always responds instantly 使用 the best available data while keeping it fresh in the background. Libraries like SWR (用于 React) 和 TanStack Query implement this pattern out of the box 使用 configurable TTL, grace periods, 和 background refetch intervals.\n\n## Checklist\n- Categorize RPC data by volatility: immutable, semi-stable, volatile\n- Include 钱包 address 和 network in all cache keys\n- Invalidate 账户-tagged caches on 钱包 switch\n- Invalidate all non-global caches on network switch\n- Implement stale-while-revalidate 用于 semi-stable data\n- Provide a manual refresh button as an escape hatch\n- Monitor cache hit rates to validate your TTL configuration\n\n## Red flags\n- Caching without network in the key (cross-network data leakage)\n- Not invalidating on 账户 switch (showing previous 钱包's data)\n- Setting TTLs too long 用于 financial data (stale balance display)\n- Re-fetching everything on every render (defeats the purpose of caching)\n",
            "duration": "45 min"
          },
          "walletux-v2-rpc-health": {
            "title": "RPC health monitoring 和 graceful degradation",
            "content": "# RPC health monitoring 和 graceful degradation\n\nRPC endpoints are the lifeline of every Solana 钱包 application. When they go down, become slow, or return stale data, your app becomes unusable. Production 钱包 apps must continuously monitor RPC health 和 degrade gracefully when issues are detected, rather than showing cryptic errors or silently displaying stale data. This 课时 covers the engineering patterns 用于 building resilient RPC connectivity.\n\n## Why RPC endpoints fail\n\nSolana RPC endpoints experience several failure modes. Rate limiting is the most common: free-tier endpoints enforce strict per-IP 和 per-second limits, 和 exceeding them results in HTTP 429 responses. Latency spikes occur during high network activity (NFT mints, token launches) when 验证者 are under heavy load 和 RPC nodes queue requests. Stale data happens when an RPC node falls behind the cluster's tip slot, returning 账户 states that are several slots (or seconds) old. Complete outages, while rare 用于 premium providers, do happen 和 can last minutes to hours.\n\nEach failure mode requires a different response. Rate limiting needs request throttling 和 backoff. Latency spikes need timeout management 和 user communication. Stale data needs detection 和 provider rotation. Complete outages need failover to a backup endpoint.\n\n## Health check implementation\n\nImplement a periodic health check that runs every 15-30 seconds while the app is active. The health check should measure three metrics: latency (round-trip time 用于 a `getSlot` call), freshness (compare the returned slot against the expected tip slot from a secondary source or the previous check), 和 error rate (percentage of failed requests in the last N calls).\n\nA healthy endpoint has latency under 500ms, slot freshness within 5 slots of the expected tip, 和 an error rate below 5%. An unhealthy endpoint has latency over 2000ms, slot freshness more than 50 slots behind, or an error rate above 20%. The 中级 range (degraded) triggers warnings without failover.\n\nStore health check results in a rolling window (last 10-20 checks). A single slow response should not trigger failover, but 3 consecutive slow responses should. This smoothing prevents flapping between endpoints due to transient network issues.\n\n## Failover strategies\n\nPrimary-secondary failover is the simplest pattern. Configure a primary RPC endpoint (your preferred provider) 和 one or more secondaries (different providers 用于 diversity). When the primary becomes unhealthy, route all requests to the secondary. Periodically re-check the primary (every 60 seconds) 和 switch back when it recovers. This prevents all your traffic from permanently migrating to the secondary.\n\nRound-robin 使用 health weighting distributes requests across multiple endpoints based on their current health scores. A healthy endpoint gets a weight of 1.0, a degraded endpoint gets 0.3, 和 an unhealthy endpoint gets 0.0. This approach provides better throughput than single-endpoint strategies 和 automatically adapts to changing conditions.\n\n用于 critical 交易 (swaps, transfers), always use the endpoint 使用 the lowest latency 和 highest freshness. 交易 submission is latency-sensitive: a stale blockhash from a behind-the-tip node will cause the 交易 to be rejected. 用于 read operations (balance queries), slightly stale data is acceptable if it means faster responses.\n\n## Graceful degradation in the UI\n\nWhen RPC health degrades, the UI should communicate the situation clearly without panic. Display a small status indicator (green dot, yellow dot, red dot) near the network name or in the status bar. Clicking it should show detailed health information: current latency, last successful request time, 和 the number of failed requests.\n\nDuring degraded mode, disable or add warnings to 交易 buttons. A yellow warning like \"Network may be slow — 交易 might take longer than usual\" is better than letting users submit 交易 that will likely time out. During a full outage, disable all 交易 features 和 show a clear message: \"Unable to reach the Solana network. Your funds are safe. We'll reconnect automatically.\"\n\nNever hide the degradation. Users who submit 交易 during an outage 和 see \"交易 failed\" without explanation will assume their funds are at risk. Proactive communication (\"The network is experiencing delays\") builds trust even when the experience is suboptimal.\n\n## Request retry 和 throttling\n\nWhen an RPC request fails, classify the error before deciding whether to retry. HTTP 429 (rate limited): back off exponentially starting at 1 second, retry up to 3 times. HTTP 5xx (server error): retry once after 2 seconds, then failover to secondary endpoint. Network timeout: retry once 使用 a shorter timeout (the request may have succeeded but the response was lost), then failover. HTTP 4xx (client error): do not retry, the request is malformed.\n\nImplement a request queue 使用 concurrency limits. Most RPC providers allow 10-40 concurrent requests. If your app tries to fire 50 requests simultaneously (common during initial data loading), queue the excess 和 process them as earlier requests complete. This prevents self-inflicted rate limiting.\n\nDebounce user-triggered requests. If the user rapidly clicks \"Refresh\" or types in a search field that triggers RPC lookups, debounce the requests to at most one per 500ms. This is simple to implement 和 dramatically reduces unnecessary RPC traffic.\n\n## Monitoring 和 alerting\n\nLog all RPC metrics to your observability system: request count, error count, latency percentiles (p50, p95, p99), 和 cache hit rate. Set alerts 用于: error rate exceeding 10% over 5 minutes, p95 latency exceeding 3 seconds, 和 cache hit rate dropping below 50% (indicates a cache invalidation bug or a change in access patterns).\n\nTrack per-endpoint metrics separately. If your primary endpoint's error rate spikes while the secondary is healthy, the failover logic should handle it automatically. But if both endpoints degrade simultaneously, it likely indicates a Solana network-wide issue rather than a provider problem, 和 the alerting should reflect that distinction.\n\n## Checklist\n- Run health checks every 15-30 seconds measuring latency, freshness, 和 error rate\n- Implement primary-secondary failover 使用 automatic recovery\n- Display RPC health status in the UI (green/yellow/red indicator)\n- Disable 交易 features during outages 使用 clear messaging\n- Classify errors before retrying (429 vs 5xx vs 4xx)\n- Implement request queue 使用 concurrency limits\n- Debounce user-triggered RPC requests\n- Monitor 和 alert on error rate, latency, 和 cache hit rate\n\n## Red flags\n- No failover endpoints (single point of failure)\n- Retrying 4xx errors (wastes requests on malformed input)\n- Hiding RPC failures from the user (builds distrust)\n- No concurrency limits (self-inflicted rate limiting)\n",
            "duration": "50 min"
          },
          "walletux-v2-ux-report": {
            "title": "Checkpoint: Generate a 钱包 UX Report",
            "content": "# Checkpoint: Generate a 钱包 UX Report\n\nBuild the final 钱包 UX quality report that combines all 课程 concepts:\n\n- Count connection attempts (CONNECT events) 和 successful connections (CONNECTED events)\n- Calculate success rate as a percentage 使用 2 decimal places\n- Compute average connection time from CONNECTED events' durationMs\n- Count ACCOUNT_CHANGE 和 NETWORK_CHANGE events\n- Calculate cache hit rate from cacheStats (hits / total * 100, 2 decimal places)\n- Calculate RPC health score from rpcChecks (healthy / total * 100, 2 decimal places)\n- Include the timestamp from input\n\nThis checkpoint validates your complete understanding of 钱包 UX engineering.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "sign-in-with-solana": {
    "title": "Sign-In 使用 Solana",
    "description": "Master production SIWS authentication on Solana: standardized inputs, strict verification invariants, replay-resistant nonce lifecycle, 和 audit-ready reporting.",
    "duration": "12 hours",
    "tags": [
      "siws",
      "authentication",
      "wallet",
      "session",
      "solana"
    ],
    "modules": {
      "siws-v2-fundamentals": {
        "title": "SIWS Fundamentals",
        "description": "SIWS rationale, strict input-field semantics, 钱包 rendering behavior, 和 deterministic sign-in input construction.",
        "lessons": {
          "siws-v2-why-exists": {
            "title": "Why SIWS exists: replacing connect-和-signMessage",
            "content": "# Why SIWS exists: replacing connect-和-signMessage\n\nBefore Sign-In 使用 Solana (SIWS) became a standard, dApps authenticated 钱包 holders using a two-step pattern: connect the 钱包, then call `signMessage` 使用 an arbitrary string. The user would see a raw byte blob in their 钱包's approval screen, sign it, 和 the server would verify the signature against the expected public key. This worked, but it was fragile, inconsistent, 和 dangerous.\n\n## The problems 使用 raw signMessage\n\nThe fundamental issue 使用 raw `signMessage` authentication is that 钱包 cannot distinguish between a benign sign-in request 和 a malicious payload. When a 钱包 displays \"Sign this message: 0x48656c6c6f\" or even a human-readable string like \"Please sign in to example.com at 2024-01-15T10:30:00Z,\" the 钱包 has no structured way to parse, validate, or warn about the content. The user must trust that the dApp is honest about what it is asking them to sign.\n\nThis creates several attack vectors. A malicious dApp could present a sign-in prompt that actually contains a serialized 交易. If the 钱包 treats `signMessage` payloads as opaque bytes (which most do), the user signs what they believe is a login but is actually an authorization 用于 a token transfer. Even without outright fraud, the lack of structure means different dApps format their sign-in messages differently. Users see inconsistent approval screens across applications, eroding trust 和 making it harder to identify legitimate requests.\n\nReplay attacks are another critical weakness. If a dApp asks the user to sign \"Log in to example.com\" without a nonce or timestamp, the resulting signature is valid forever. An attacker who intercepts this signature (via a compromised server log, a man-in-the-middle proxy, or a leaked database) can replay it indefinitely to impersonate the user. Adding a nonce or timestamp to the message helps, but without a standard format, each dApp implements its own scheme — some correctly, many not.\n\n## What SIWS standardizes\n\nSign-In 使用 Solana defines a structured message format that 钱包 can parse, validate, 和 display in a human-readable, predictable way. The SIWS standard specifies exactly which fields a sign-in request must contain 和 how 钱包 should render them. This moves authentication from an opaque byte-signing operation to a semantically meaningful, 钱包-aware protocol.\n\nThe core fields of a SIWS sign-in input are: **domain** (the requesting site's origin, displayed prominently by the 钱包), **address** (the expected signer's public key), **nonce** (a unique, server-generated value that prevents replay attacks), **issuedAt** (ISO 8601 timestamp marking when the request was created), **expirationTime** (optional deadline after which the sign-in is invalid), **statement** (human-readable description of what the user is approving), **chainId** (the Solana cluster, e.g., mainnet-beta), 和 **resources** (optional URIs that the sign-in grants access to).\n\nWhen a 钱包 receives a SIWS request, it knows the structure. It can display the domain prominently so the user can verify they are signing in to the correct site. It can show the expiration time so the user knows the request is time-limited. It can warn if the domain in the request does not match the domain the 钱包 was connected from. This structured rendering is a massive UX improvement over displaying raw bytes.\n\n## UX improvements 用于 end users\n\n使用 SIWS, 钱包 approval screens become consistent 和 informative. Instead of seeing an arbitrary string, users see a formatted display: the requesting domain, the statement explaining the action, the nonce (often hidden from the user but validated by the 钱包), 和 time bounds. This consistency across dApps builds user confidence — they 学习 to recognize what a legitimate sign-in request looks like.\n\n钱包 can also implement automatic safety checks. If the domain in the SIWS input does not match the origin of the connecting dApp, the 钱包 can show a warning or block the request entirely. If the issuedAt timestamp is far in the past or the expirationTime has already passed, the 钱包 can reject the request before the user even sees it. These checks are impossible 使用 raw `signMessage` because the 钱包 has no way to parse the content.\n\n## Server-side benefits\n\n用于 backend developers, SIWS provides a predictable verification flow. The server generates a nonce, sends the SIWS input to the client, receives the signed output, 和 verifies: (1) the signature is valid 用于 the claimed address, (2) the domain matches the server's domain, (3) the nonce matches the one the server issued, (4) the timestamps are within acceptable bounds, 和 (5) the address matches the expected signer. Each check is explicit 和 auditable, unlike ad-hoc string parsing.\n\nThe nonce mechanism is particularly important. The server stores issued nonces (in memory, Redis, or a database) 和 marks them as consumed after successful verification. Any attempt to reuse a nonce is rejected as a replay attack. This provides cryptographic proof of freshness that raw signMessage authentication lacks unless the developer explicitly implements it — 和 history shows most developers do not.\n\n## The path forward\n\nSIWS aligns Solana's authentication story 使用 Ethereum's Sign-In 使用 Ethereum (SIWE / EIP-4361) 和 other chain-specific standards. Cross-chain dApps can implement a unified authentication flow 使用 chain-specific signing backends. The 钱包-side rendering, nonce management, 和 verification logic are consistent patterns regardless of the underlying blockchain.\n\n## Operator mindset\n\nTreat SIWS as a protocol contract, not a UI prompt. If nonce lifecycle, domain checks, 和 time bounds are not enforced as strict invariants, authentication becomes signature theater.\n\n## Checklist\n- Understand why raw signMessage is insufficient 用于 authentication\n- Know the core SIWS fields: domain, address, nonce, issuedAt, expirationTime, statement\n- Recognize that SIWS enables 钱包-side validation 和 consistent UX\n- Understand the server-side nonce flow: generate, issue, verify, consume\n\n## Red flags\n- Using raw signMessage 用于 authentication without structured format\n- Omitting nonce from sign-in messages (enables replay attacks)\n- Not validating domain match between SIWS input 和 connecting origin\n- Allowing sign-in messages without expiration times\n",
            "duration": "50 min"
          },
          "siws-v2-input-fields": {
            "title": "SIWS input fields 和 安全 rules",
            "content": "# SIWS input fields 和 安全 rules\n\nThe Sign-In 使用 Solana input is a structured object that defines every parameter of an authentication request. Each field has specific validation rules, 安全 implications, 和 rendering expectations. Understanding every field deeply is essential 用于 building a correct 和 secure SIWS implementation.\n\n## domain\n\nThe `domain` field identifies the requesting application. It must be a valid domain name without protocol prefix — \"example.com\", not \"https://example.com\". The domain serves as the primary trust anchor: when the 钱包 displays the sign-in request, the domain is shown prominently so the user can verify they are interacting 使用 the intended site.\n\n安全 rule: the server must verify that the domain in the signed output matches its own domain exactly. If a user signs a SIWS message 用于 \"evil.com\" 和 submits it to \"example.com\", the server must reject it. The domain check prevents cross-site authentication relay attacks where an attacker presents their own domain to the user but submits the signed result to a different server. Domain validation should be case-insensitive (domains are case-insensitive per RFC 4343) 和 must reject domains containing protocol prefixes, paths, ports, or query strings.\n\n## address\n\nThe `address` field contains the Solana public key (base58-encoded) of the 钱包 that will sign the request. On Solana, public keys are 32 bytes encoded in base58, resulting in strings of 32-44 characters. The address must match the actual signer of the SIWS output — if the address in the input says \"Wallet111\" but \"Wallet222\" actually signs the message, verification must fail.\n\n安全 rule: always validate address format before sending the request to the 钱包. A malformed address will cause downstream verification failures. Check that the address is 32-44 characters long 和 consists only of valid base58 characters (no 0, O, I, or l — these are excluded from base58 to avoid visual ambiguity). On the server side, verify that the address in the signed output matches the address you expected (typically the address of the connected 钱包).\n\n## nonce\n\nThe `nonce` is the single most important 安全 field in SIWS. It is a server-generated, unique, unpredictable string that ties the sign-in request to a specific authentication attempt. The nonce must be at least 8 characters long 和 should be alphanumeric. In production, nonces are typically 16-32 character random strings generated using a cryptographically secure random number generator.\n\n安全 rule: nonces must be generated server-side, never client-side. If the client generates its own nonce, an attacker can reuse a previously valid nonce-signature pair. The server must store the nonce (使用 a TTL matching the sign-in expiration window) 和 check it during verification. After successful verification, the nonce must be permanently invalidated (deleted or marked as consumed). The nonce storage must be atomic — a race condition where two requests verify the same nonce simultaneously would defeat the replay protection entirely.\n\nNonce storage options include: in-memory maps (suitable 用于 single-server deployments), Redis 使用 TTL (suitable 用于 distributed systems), 和 database tables 使用 unique constraints. Whatever storage is used, the invalidation must be atomic: check-和-delete in a single operation, not check-then-delete in two steps.\n\n## issuedAt\n\nThe `issuedAt` field is an ISO 8601 timestamp indicating when the sign-in request was created. It provides temporal context 用于 the authentication attempt. The server sets this value when generating the sign-in input.\n\n安全 rule: during verification, the server must check that `issuedAt` is not in the future (allowing a small clock skew tolerance of 1-2 minutes). A sign-in request 使用 a future issuedAt timestamp is suspicious — it may indicate clock manipulation or request fabrication. The server should also reject sign-in requests where issuedAt is too far in the past, even if the expirationTime has not passed. A reasonable maximum age 用于 issuedAt is 10-15 minutes.\n\n## expirationTime\n\nThe `expirationTime` field is an optional ISO 8601 timestamp indicating when the sign-in request becomes invalid. If present, it must be strictly after `issuedAt`. If absent, the sign-in request has no explicit expiration (though the server should still enforce a maximum age based on issuedAt).\n\n安全 rule: if expirationTime is present, the server must verify that the current time is before the expiration. Expired sign-in requests must be rejected regardless of signature validity. Setting short expiration windows (5-15 minutes) reduces the window 用于 replay attacks 和 limits the useful lifetime of intercepted sign-in requests. Production systems should always set expirationTime rather than relying solely on nonce expiration.\n\n## statement\n\nThe `statement` field is a human-readable string that the 钱包 displays to the user, describing what they are approving. If not provided by the dApp, a sensible default is \"Sign in to <domain>\". The statement should be concise, clear, 和 accurately describe the action.\n\n安全 rule: the statement is informational 和 should not contain sensitive data. It is included in the signed message, so it is visible to anyone who can see the signature. Do not include session tokens, API keys, or other secrets in the statement. The 钱包 renders the statement as-is, so avoid HTML, markdown, or other formatting that might be misinterpreted.\n\n## chainId 和 resources\n\nThe `chainId` field identifies the Solana cluster (e.g., \"mainnet-beta\", \"devnet\", \"testnet\"). This prevents cross-cluster authentication where a signature obtained on devnet is replayed on mainnet. The `resources` field is an optional array of URIs that the sign-in grants access to. These are informational 和 displayed by the 钱包.\n\n安全 rule: if your dApp operates on a specific cluster, verify that the chainId in the signed output matches your expected cluster. Resources should be validated as well-formed URIs but their enforcement is application-specific.\n\n## Checklist\n- Domain must not include protocol, path, or port\n- Nonce must be >= 8 alphanumeric characters, generated server-side\n- issuedAt must not be in the future; reject stale requests\n- expirationTime (if present) must be after issuedAt 和 not yet passed\n- Address must be 32-44 characters of valid base58\n- Statement should default to \"Sign in to <domain>\" if not provided\n\n## Red flags\n- Accepting client-generated nonces\n- Not validating domain format (allowing protocol prefixes)\n- Missing atomic nonce invalidation (check-then-delete race condition)\n- No maximum age check on issuedAt\n- Storing secrets in the statement field\n",
            "duration": "50 min"
          },
          "siws-v2-message-preview": {
            "title": "Message preview: how 钱包 render SIWS requests",
            "content": "# Message preview: how 钱包 render SIWS requests\n\nWhen a dApp sends a SIWS sign-in request to a 钱包, the 钱包 transforms the structured input into a human-readable message that the user sees on the approval screen. Understanding exactly how this rendering works is critical 用于 dApp developers — it determines what users see, what they trust, 和 what they sign.\n\n## The SIWS message format\n\nThe SIWS standard defines a specific text format 用于 the message that gets signed. The 钱包 constructs this message from the structured input fields. The format follows a predictable template that 钱包 can both generate 和 parse. The message begins 使用 the domain 和 address, followed by a statement, then a structured block of metadata fields.\n\nA complete SIWS message looks like this:\n\n```\nexample.com wants you to sign in with your Solana account:\n7Y4f3Taf6YKqz3Y3h2Hj9uV4UT2Df6gKGfE6q8SxT6aY\n\nSign in to example.com\n\nNonce: abc12345def67890\nIssued At: 2024-01-15T10:30:00Z\nExpiration Time: 2024-01-15T11:30:00Z\nChain ID: mainnet-beta\n```\n\nThe first line always follows the pattern: \"`<domain>` wants you to sign in 使用 your Solana 账户:\". This phrasing is standardized so users 学习 to recognize it across all SIWS-compatible dApps. The second line is the full public key address. A blank line separates the header from the statement. Another blank line separates the statement from the metadata fields.\n\n## 钱包 rendering expectations\n\nModern Solana 钱包 (Phantom, Solflare, Backpack) recognize SIWS-formatted messages 和 render them 使用 enhanced UI. Instead of displaying raw text, they parse the structured fields 和 present them in a formatted approval screen 使用 clear sections.\n\nThe domain is typically displayed prominently at the top of the approval screen, often 使用 the dApp's favicon if available. This is the primary trust signal — users should check this domain matches the site they are interacting 使用. Some 钱包 cross-reference the domain against the connecting origin 和 display a warning if they do not match.\n\nThe statement is shown in a distinct section, often 使用 larger or bolder text. This is the human-readable explanation of what the user is approving. 用于 sign-in requests, it typically says something like \"Sign in to example.com\" or a custom message the dApp provides.\n\nThe metadata fields (nonce, issuedAt, expirationTime, chainId, resources) are shown in a structured format, often collapsible or in a \"details\" section. Most users do not read these fields, but their presence signals that the request is well-formed 和 follows the standard. 安全-conscious users can verify the nonce matches their expectation 和 the timestamps are reasonable.\n\n## What users actually see versus what gets signed\n\nIt is important to understand that what the 钱包 displays 和 what actually gets signed can differ. The 钱包 renders a formatted UI from the parsed fields, but the actual bytes that are signed are the serialized message text in the standard format. The 钱包 constructs the canonical message text, displays a parsed version to the user, 和 signs the canonical text.\n\nThis creates a trust model: the user trusts the 钱包 to accurately represent the message content. If a 钱包 has a rendering bug (e.g., it shows the wrong domain), the user might approve a message they would otherwise reject. This is why using well-audited, mainstream 钱包 is important 用于 SIWS 安全.\n\nThe signed bytes include the full message text prefixed 使用 a Solana-specific preamble. The Ed25519 signature covers the entire message, including all fields. Changing any field (even adding a space) produces a completely different signature. This ensures that the server can verify not just that the user signed something, but that they signed the exact message 使用 the exact fields the server expected.\n\n## Building preview UIs in dApps\n\nBefore sending a SIWS request to the 钱包, many dApps show a preview of the message in their own UI. This preview serves two purposes: it prepares the user 用于 what they will see in the 钱包 (reducing confusion 和 approval time), 和 it provides a last checkpoint before triggering the 钱包 interaction.\n\nThe dApp preview should mirror the 钱包's rendering as closely as possible. Show the domain, statement, 和 key metadata fields. Indicate that the user will be asked to approve this message in their 钱包. If the dApp is using a custom statement, display it exactly as it will appear.\n\nDo not include fields in the preview that might confuse users. The nonce, 用于 example, is a random string that has no meaning to the user. Showing it adds visual noise without value. The preview can omit the nonce while the actual signed message includes it. Similarly, the chainId is important 用于 verification but rarely interesting to users unless the dApp operates across multiple clusters.\n\n## Edge cases in rendering\n\nSeveral edge cases affect how SIWS messages are rendered 和 signed. Long domains may be truncated in 钱包 UIs — ensure your domain is concise. Internationalized domain names (IDN) should be tested 使用 your target 钱包, as some 钱包 may not render Unicode characters correctly. The statement field has no maximum length in the standard, but extremely long statements will be truncated or require scrolling in the 钱包, reducing the chance that users read them fully.\n\nEmpty optional fields are omitted from the message text. If no expirationTime is set, the \"Expiration Time:\" line does not appear. If no resources are specified, no resources section appears. The message format adjusts dynamically based on which fields are present.\n\n## Checklist\n- Know the canonical SIWS message format 和 field ordering\n- Understand that 钱包 parse 和 render structured UI from the message\n- Build dApp-side previews that mirror 钱包 rendering\n- Test your SIWS messages 使用 target 钱包 to verify display\n- Keep statements concise 和 domains short\n\n## Red flags\n- Assuming all 钱包 render SIWS messages identically\n- Including sensitive data in the statement (it is visible in the signed message)\n- Using extremely long statements that 钱包 truncate\n- Not 测试 使用 real 钱包 approval screens during development\n",
            "duration": "45 min"
          },
          "siws-v2-sign-in-input": {
            "title": "Challenge: Build a validated SIWS sign-in input",
            "content": "# Challenge: Build a validated SIWS sign-in input\n\nImplement a function that creates a validated Sign-In 使用 Solana input:\n\n- Validate domain (non-empty, must not include protocol prefix)\n- Validate nonce (at least 8 characters, alphanumeric only)\n- Validate address format (32-44 characters 用于 Solana base58)\n- Set issuedAt (required) 和 optional expirationTime 使用 ordering check\n- Default statement to \"Sign in to <domain>\" if not provided\n- Return structured result 使用 valid flag 和 collected errors\n\nYour implementation must be fully deterministic — same input always produces same output.",
            "duration": "50 min"
          }
        }
      },
      "siws-v2-verification": {
        "title": "Verification & 安全",
        "description": "Server-side verification invariants, nonce replay defenses, session management, 和 deterministic auth audit reporting.",
        "lessons": {
          "siws-v2-verify-sign-in": {
            "title": "Challenge: Verify a SIWS sign-in response",
            "content": "# Challenge: Verify a SIWS sign-in response\n\nImplement server-side verification of a SIWS sign-in output:\n\n- Check domain matches expected domain\n- Check nonce matches expected nonce\n- Check issuedAt is not in the future relative to currentTime\n- Check expirationTime (if present) has not passed\n- Check address matches expected signer\n- Return verification result 使用 individual check statuses 和 error list\n\nAll five checks must pass 用于 the sign-in to be considered verified.",
            "duration": "50 min"
          },
          "siws-v2-sessions": {
            "title": "Sessions 和 logout: what to store 和 what not to store",
            "content": "# Sessions 和 logout: what to store 和 what not to store\n\nAfter a successful SIWS sign-in verification, the server must establish a session so the user does not need to re-authenticate on every request. Session management 用于 钱包-based authentication has unique characteristics compared to traditional username-password systems. Understanding what to store, where to store it, 和 how to handle logout is essential 用于 building secure dApps.\n\n## What a SIWS session contains\n\nA SIWS session represents a verified claim: \"Public key X proved ownership by signing a SIWS message 用于 domain Y at time Z.\" The session should store exactly enough information to enforce authorization decisions without requiring re-verification. The minimum session payload includes: the 钱包 address (public key), the domain the sign-in was verified 用于, the session creation time, 和 the session expiration time.\n\nDo NOT store the SIWS signature, the signed message, or the nonce in the session. These are verification artifacts, not session data. The signature has no purpose after verification — it proved the user controlled the key at the time of signing, 和 that proof is now captured by the session itself. Storing signatures in sessions creates unnecessary data that, if leaked, provides no additional attack surface but adds complexity 和 storage cost.\n\nSession identifiers should be opaque, random tokens — not derived from the 钱包 address. Using the 钱包 address as a session ID is a common mistake because 钱包 addresses are public. Anyone who knows a user's address could forge requests. The session ID must be a cryptographically random string (e.g., 256-bit random value, base64-encoded) that maps to the session data on the server side.\n\n## Server-side vs client-side session storage\n\nServer-side sessions store session data in a backend store (Redis, database, in-memory map) 和 issue a session token (cookie or bearer token) to the client. The client presents the token on each request, 和 the server looks up the associated session data. This is the most secure pattern because the server controls all session state.\n\nClient-side sessions (JWTs) encode the session data directly in the token. The server signs the JWT 和 the client includes it in requests. The server verifies the JWT signature 和 reads the session data without a backend lookup. This is simpler to deploy but has significant drawbacks: JWTs cannot be individually revoked (you must wait 用于 expiration or maintain a revocation list), the session data is visible to the client (encrypted JWTs mitigate this), 和 JWT size grows 使用 payload data.\n\n用于 SIWS authentication, server-side sessions are recommended because they support immediate revocation (critical 用于 安全 incidents) 和 keep session data private. If using JWTs, keep the payload minimal (钱包 address 和 expiration only), use short expiration times (15-60 minutes), 和 implement a refresh token flow 用于 session extension.\n\n## Session expiration 和 refresh\n\nSession lifetimes 用于 钱包-authenticated dApps should be shorter than traditional web sessions. Users can sign a new SIWS message quickly (a few seconds), so the cost of re-authentication is low. Recommended session lifetime is 1-4 hours 用于 active sessions, 使用 a sliding window that extends the expiration on each authenticated request.\n\nRefresh tokens can extend session lifetime without requiring re-authentication. The flow is: issue a short-lived access token (15-60 minutes) 和 a longer-lived refresh token (24-72 hours). When the access token expires, the client presents the refresh token to obtain a new access token. The refresh token is single-use (rotated on each refresh) 和 stored securely.\n\nAbsolute session lifetime should be enforced regardless of refresh activity. Even if a user keeps refreshing, the session should eventually require re-authentication. A reasonable absolute lifetime is 7-30 days. This limits the damage from a stolen refresh token.\n\n## Logout implementation\n\nLogout 用于 钱包-based authentication is simpler than login but has important nuances. The server must invalidate the session on the backend (delete the session from the store or add the JWT to a revocation list). The client must clear all local session artifacts (cookies, localStorage tokens, in-memory state).\n\n钱包 disconnection is not the same as logout. When a user disconnects their 钱包 from the dApp (using the 钱包's disconnect feature), the dApp should treat this as a logout signal 和 invalidate the server session. However, some dApps maintain the session even after 钱包 disconnection, which can confuse users who expect disconnection to log them out.\n\nImplementing \"logout everywhere\" (invalidating all sessions 用于 a 钱包 address) requires server-side session storage 使用 an index by 钱包 address. When triggered, query all sessions 用于 the address 和 invalidate them. This is useful 用于 安全 incidents or when the user suspects their session was compromised.\n\n## What NOT to store in sessions\n\nNever store the user's private key (obviously). Never store the SIWS nonce (it has been consumed 和 should be deleted from the nonce store). Never store the raw SIWS signature (it is a verification artifact). Never store personally identifiable information (PII) unless your dApp explicitly collects it — 钱包 addresses are pseudonymous by default.\n\nAvoid storing 钱包 balances, token holdings, or other on-chain data in the session. This data changes constantly 和 becomes stale immediately. Fetch it fresh from the RPC when needed. Sessions should be lightweight: address, permissions, timestamps, 和 nothing more.\n\n## Checklist\n- Store 钱包 address, domain, creation time, 和 expiration in sessions\n- Use cryptographically random session IDs, not 钱包 addresses\n- Prefer server-side sessions 用于 immediate revocation capability\n- Enforce absolute session lifetime even 使用 refresh tokens\n- Invalidate sessions on both logout 和 钱包 disconnect\n- Never store signatures, nonces, or PII in sessions\n\n## Red flags\n- Using 钱包 address as session ID\n- Storing SIWS signature or nonce in the session\n- No session expiration or unlimited lifetime\n- JWT sessions without revocation mechanism\n- Ignoring 钱包 disconnect events\n",
            "duration": "45 min"
          },
          "siws-v2-replay-protection": {
            "title": "Replay protection 和 nonce registry 设计",
            "content": "# Replay protection 和 nonce registry 设计\n\nReplay attacks are the most critical threat to any signature-based authentication system. In a replay attack, an adversary captures a valid signed message 和 submits it again to the server, impersonating the original signer. SIWS addresses this 使用 nonce-based replay protection, but the implementation details of the nonce registry determine whether the protection actually works.\n\n## How replay attacks work against SIWS\n\nConsider a SIWS sign-in flow without proper nonce management. The user signs a message: \"example.com wants you to sign in 使用 your Solana 账户: Wallet111... Nonce: abc123 Issued At: 2024-01-01T00:00:00Z\". The server verifies the signature 和 creates a session. The signed message 和 signature are transmitted over HTTPS 和 should be safe in transit.\n\nHowever, the signed message could be captured through: a compromised server log that records request bodies, a malicious browser extension that intercepts WebSocket traffic, a man-in-the-middle proxy in a development or corporate environment, or a compromised CDN that logs request payloads. If the attacker obtains the signed SIWS output, they can submit it to the server as if they were the original signer.\n\nWithout nonce protection, the server would verify the signature (it is valid — the user really did sign it), check the domain (it matches), check the timestamps (they may still be within bounds), 和 accept the authentication. The attacker now has a valid session 用于 the victim's 钱包 address.\n\n## Nonce lifecycle\n\nThe nonce lifecycle has four phases: generation, issuance, verification, 和 consumption. Each phase has specific requirements.\n\nGeneration: the server creates a cryptographically random nonce using a secure random number generator. The nonce must be unpredictable — an attacker should not be able to guess the next nonce by observing previous ones. Use at least 128 bits of entropy (16 bytes, 22 base64 characters or 32 hex characters). Store the nonce in the registry 使用 a TTL 和 the expected 钱包 address.\n\nIssuance: the server includes the nonce in the SIWS input sent to the client. The nonce travels from server to client to 钱包 和 back. During this transit, the nonce is not secret (it is included in the signed message), but it is unique. The important property is not secrecy but freshness — this specific nonce has never been used before.\n\nVerification: when the server receives the signed SIWS output, it extracts the nonce 和 checks the registry. The nonce must exist in the registry (rejecting fabricated nonces), must not be marked as consumed (rejecting replays), 和 must not have expired (rejecting stale requests). These checks must happen before signature verification to fail fast on obvious replays.\n\nConsumption: after successful verification, the nonce is atomically marked as consumed or deleted from the registry. This is the critical step — if consumption is not atomic, two concurrent requests 使用 the same nonce could both pass the \"not consumed\" check before either marks it as consumed. This race condition completely defeats replay protection.\n\n## Nonce registry 设计 patterns\n\nThe nonce registry is the data structure that stores issued nonces 和 tracks their state. Several patterns are used in production.\n\nIn-memory map 使用 TTL: a simple hash map where keys are nonce strings 和 values are metadata (creation time, expected address, consumed flag). A background timer periodically cleans expired entries. This works 用于 single-server deployments but does not scale to multiple servers (each server has its own map 和 cannot validate nonces issued by other servers).\n\nRedis 使用 atomic operations: Redis provides the ideal primitives 用于 nonce management. Use SET 使用 NX (set-if-not-exists) 用于 atomic consumption: attempt to set a \"consumed\" key; if it already exists, the nonce was already used. Use TTL on nonce keys 用于 automatic expiration. Redis is distributed, so all servers share the same nonce registry.\n\nThe Redis pattern 用于 atomic nonce consumption:\n\n1. On issuance: `SET nonce:<value> \"issued\" EX 600` (10-minute TTL)\n2. On verification: `SET nonce:<value>:consumed \"1\" NX EX 600`\n   - If SET NX succeeds (returns OK): nonce is valid 和 now consumed\n   - If SET NX fails (returns nil): nonce was already consumed (replay attempt)\n\nDatabase 使用 unique constraints: store nonces in a table 使用 a unique constraint on the nonce value 和 a \"consumed_at\" column. Consumption is an UPDATE that sets consumed_at where consumed_at IS NULL. If the update affects 0 rows, the nonce was already consumed. Database 交易 ensure atomicity but add latency compared to Redis.\n\n## Handling edge cases\n\nClock skew between servers affects nonce TTL enforcement. If server A issues a nonce 使用 a 10-minute TTL but server B's clock is 3 minutes ahead, server B may consider the nonce expired after only 7 minutes from the user's perspective. Use NTP synchronization across servers 和 add a grace period (30-60 seconds) to TTL checks.\n\nNonce reuse across different 钱包 addresses should be rejected. Even if 钱包 A's nonce was consumed, do not allow 钱包 B to use the same nonce value. This is automatically handled if the nonce registry indexes by nonce value regardless of address. However, some implementations associate nonces 使用 specific addresses 和 might accidentally allow cross-address reuse.\n\nHigh-traffic systems may generate thousands of nonces per second. The registry must handle this volume without becoming a bottleneck. Redis handles this easily. In-memory maps work if garbage collection of expired nonces is efficient. Database-backed registries need proper indexing 和 periodic cleanup of consumed nonces.\n\n## Monitoring 和 alerting\n\nProduction nonce registries should emit metrics: nonces generated per minute, nonces consumed per minute, replay attempts blocked per minute, nonces expired unused per minute. A sudden spike in replay attempts indicates an active attack. A high ratio of expired-to-consumed nonces may indicate UX issues (users starting but not completing sign-in).\n\nLog every replay attempt 使用 the nonce value, the submitting IP address, 和 the associated 钱包 address. This data feeds into 安全 incident investigation. Alert on replay attempt rates exceeding a threshold (e.g., more than 10 per minute from the same IP).\n\n## Checklist\n- Use cryptographically random nonces 使用 >= 128 bits of entropy\n- Implement atomic nonce consumption (check-和-invalidate in one operation)\n- Set nonce TTL matching the sign-in expiration window (5-15 minutes)\n- Use Redis or equivalent distributed store 用于 multi-server deployments\n- Monitor 和 alert on replay attempt rates\n- Clean up expired nonces periodically\n\n## Red flags\n- Non-atomic nonce consumption (check-then-delete race condition)\n- In-memory nonce storage in a multi-server 部署\n- No nonce TTL (nonces accumulate forever)\n- Allowing nonce reuse across different 钱包 addresses\n- No monitoring of replay attempt rates\n",
            "duration": "50 min"
          },
          "siws-v2-auth-report": {
            "title": "Checkpoint: Generate an auth audit report",
            "content": "# Checkpoint: Generate an auth audit report\n\nBuild the final auth audit report that combines all 课程 concepts:\n\n- Process an array of authentication attempts 使用 address, nonce, 和 verified status\n- Track used nonces to detect 和 block replay attempts (duplicate nonce = replay)\n- Count successful sign-ins, failed sign-ins, 和 replay attempts blocked\n- Count unique 钱包 addresses across all attempts\n- Build a nonce registry 使用 status 用于 each attempt: \"consumed\", \"rejected\", or \"replay-blocked\"\n- Include the report timestamp\n\nThis checkpoint validates your complete understanding of SIWS authentication 和 nonce-based replay protection.",
            "duration": "55 min"
          }
        }
      }
    }
  },
  "priority-fees-compute-budget": {
    "title": "Priority Fees & Compute Budget",
    "description": "Defensive Solana fee engineering 使用 deterministic compute planning, adaptive priority policy, 和 confirmation-aware UX reliability contracts.",
    "duration": "9 hours",
    "tags": [
      "solana",
      "fees",
      "compute-budget",
      "reliability"
    ],
    "modules": {
      "pfcb-v2-foundations": {
        "title": "Fee 和 Compute Foundations",
        "description": "Inclusion mechanics, compute/fee coupling, 和 explorer-driven policy 设计 使用 deterministic reliability framing.",
        "lessons": {
          "pfcb-v2-fee-market-reality": {
            "title": "Fee markets on Solana: what actually moves inclusion",
            "content": "# Fee markets on Solana: what actually moves inclusion\n\nPriority fees on Solana are often explained as a simple slider, but production systems need a more precise model. Inclusion is influenced by contention 用于 compute, 验证者 scheduling pressure, local leader behavior, 和 the 交易's own resource request profile. Engineers who only look at a single median fee value usually misprice during bursty traffic 和 then overpay during recovery. This 课时 gives a 实战, defensive framework 用于 pricing inclusion without relying on superstition.\n\nA 交易 does not compete only on total lamports paid. It competes on requested compute unit price 和 resource footprint under slot-level pressure. If you request very high compute units 和 low micro-lamports per compute unit, you may still lose to smaller requests paying a healthier rate. In practice, 钱包 should treat compute limit 和 compute price as coupled decisions. Choosing either one in isolation leads to unstable behavior. A route that usually lands 使用 250,000 units may occasionally need 350,000 because state branches differ. If your safety margin is too tight, you fail. If your safety margin is too loose 和 your price is high, you overpay.\n\nDefensive engineering starts 使用 synthetic sample sets 和 deterministic policy simulation. Even if your production system eventually consumes live telemetry, your 课程 project 和 baseline tests should prove policy behavior under controlled volatility regimes: calm, elevated, 和 spike. A calm regime might show p50 和 p90 close together, while a spike regime has p90 several multiples above p50. This spread is important because it tells you whether your percentile target alone is enough, or whether you need a volatility guard that adds a controlled premium.\n\nAnother misunderstood point is confirmation UX. Users often interpret \"submitted\" as \"done,\" but processed status is still vulnerable to rollback scenarios 和 reordering. 用于 high-value flows, the UI should explain exactly why it waits 用于 confirmed or finalized. This is not academic: support burden spikes when users see optimistic success then reversal. Defensive products align language 使用 protocol reality by attaching explicit state labels 和 expected next actions.\n\nA robust fee policy also defines failure classes. If a 交易 misses inclusion windows repeatedly, the policy should identify whether to raise compute price, raise compute limit, refresh blockhash, or re-quote. Blindly retrying the same payload can amplify congestion 和 degrade user trust. Good systems cap retries 和 emit deterministic diagnostics that make support 和 analytics useful.\n\nYou should model inclusion strategy as policy outputs, not imperative side effects. A policy function should return chosen percentile, volatility adjustment, final micro-lamports target, confidence label, 和 warnings. By keeping this deterministic 和 serializable, teams can diff policy versions 和 verify behavior changes before deploying. This is the same discipline used in risk engines: reproducible decisions first, runtime integrations second.\n\nFinally, keep user education integrated into the product flow. A short explanation that \"network congestion increased your priority fee to improve inclusion probability\" reduces confusion 和 failed-signature churn. It also helps users compare urgency tiers in a way that feels fair. Defensive UX is not only about blocking risky actions; it is about exposing enough context to prevent panic 和 repeated mistakes.\n\n\nThis material should be operationalized 使用 deterministic fixtures 和 explicit release criteria. Teams should preserve a small set of baseline scenarios that represent normal traffic, moderate stress, 和 severe stress. 用于 each scenario, compare policy outputs before 和 after changes, 和 require review notes when confidence labels, warnings, or recommendations move in a meaningful way. This discipline prevents accidental drift, keeps support playbooks aligned 使用 runtime behavior, 和 makes incident response faster because everyone shares the same deterministic artifact language. In practice, the strongest reliability teams treat these artifacts as release gates, not optional documentation, 和 they keep fixture ownership explicit so updates remain intentional 和 auditable.\n\n## Operator mindset\n\nFee policy is an inclusion-probability model, not a guarantee engine. Good systems expose confidence, assumptions, 和 fallback actions explicitly so operators can respond quickly when regimes shift.\n\n## Checklist\n- Couple compute limit 和 compute price decisions in one policy output.\n- Use percentile targeting plus volatility guard 用于 unstable markets.\n- Treat confirmation states as distinct UX contracts.\n- Cap retries 和 classify misses before adjusting fees.\n- Emit deterministic policy reports 用于 audits 和 regressions.\n",
            "duration": "55 min"
          },
          "pfcb-v2-compute-budget-failure-modes": {
            "title": "Compute budget 基础 和 common failure modes",
            "content": "# Compute budget 基础 和 common failure modes\n\nMost 交易 failures blamed on \"network issues\" are actually planning errors in compute budget 和 payload sizing. A defensive client treats compute planning as a deterministic preflight policy: estimate required compute, apply bounded margin, decide whether heap allocation is warranted, 和 explain the result before signing. This 课时 focuses on failure modes that recur in production 钱包 和 DeFi frontends.\n\nThe first class is explicit compute exhaustion. When 指令 paths consume more than the 交易 limit, execution aborts 和 users still pay base fees 用于 work already attempted. Teams frequently set one global limit 用于 all routes, which is convenient but unreliable. Route complexity differs by pool topology, 账户 cache warmth, 和 账户 creation branches. Planning must operate on per-flow estimates, not app-wide constants.\n\nThe second class is excessive compute requests paired 使用 aggressive bid pricing. This can cause overpayment 和 user distrust, especially in periods where lower limits would still succeed. A safe policy sets lower 和 upper bounds, applies a margin to synthetic or simulated expected compute, 和 clamps to protocol max. If a requested override is present, the system should still clamp 和 document why. Deterministic reasoning strings are useful because support 和 QA can inspect exactly why a plan was chosen.\n\nThe third class is 交易 size pressure. Large 账户 metas 和 指令 data increase serialization footprint, 和 large payloads often correlate 使用 higher compute paths. While compute planning does not directly solve size limit errors, the same planner can emit a hint when 交易 size exceeds a threshold 和 recommend route simplification or decomposition. In this 课程, we keep it deterministic: no RPC checks, only input-driven policy outputs.\n\nA related failure class is memory pressure in workloads that deserialize heavy 账户 sets. Some clients include heap-frame recommendations based on route complexity or size threshold. If you include this in a deterministic planner, keep the conditions explicit 和 stable. Ambiguous heuristics create policy churn that is hard to test.\n\nGood confirmation UX is another defensive layer. Processed means accepted by a node, confirmed adds stronger network observation, finalized is strongest settlement confidence. 用于 low-value actions, processed plus pending indicator can be acceptable. 用于 high-risk value transfer, confirmed or finalized should gate \"success\" copy. Engineers should encode this as policy output rather than ad hoc component logic.\n\nA mature planner also returns warnings. Examples include \"override clamped to max,\" \"size indicates high serialization risk,\" or \"sample set too small 用于 confident bid.\" Warnings should not be noisy; each one should map to an actionable path. Over-warning trains users to ignore alerts, while under-warning hides real risk.\n\nIn deterministic environments, each policy branch should be testable 使用 small synthetic fixtures. You want stable outputs 用于 JSON snapshots, markdown reports, 和 support triage docs. This discipline scales to production because the same decision shape can later consume live inputs without changing contract semantics.\n\n\nThis material should be operationalized 使用 deterministic fixtures 和 explicit release criteria. Teams should preserve a small set of baseline scenarios that represent normal traffic, moderate stress, 和 severe stress. 用于 each scenario, compare policy outputs before 和 after changes, 和 require review notes when confidence labels, warnings, or recommendations move in a meaningful way. This discipline prevents accidental drift, keeps support playbooks aligned 使用 runtime behavior, 和 makes incident response faster because everyone shares the same deterministic artifact language. In practice, the strongest reliability teams treat these artifacts as release gates, not optional documentation, 和 they keep fixture ownership explicit so updates remain intentional 和 auditable.\n\n## Checklist\n- Compute plans should be bounded, deterministic, 和 explainable.\n- Planner should expose warning signals, not only numeric outputs.\n- Confirmation messaging should reflect actual settlement guarantees.\n- Inputs must be validated; invalid estimates should fail fast.\n- Unit tests should cover clamp logic 和 edge thresholds.\n",
            "duration": "55 min"
          },
          "pfcb-v2-planner-explorer": {
            "title": "Explorer: compute budget planner inputs to plan",
            "content": "# Explorer: compute budget planner inputs to plan\n\nExplorers are useful only when they expose policy tradeoffs clearly. 用于 a fee 和 compute planner, that means visualizing how input estimates, percentile targets, 和 confirmation requirements produce a deterministic recommendation. This 课时 frames an explorer as a decision table that can be replayed by engineers, support staff, 和 users.\n\nStart 使用 the three input groups: workload profile, fee samples, 和 UX confirmation target. Workload profile includes synthetic 指令 CU estimates 和 payload size. Fee samples represent recent or scenario-based micro-lamport values. Confirmation target defines settlement strictness 用于 the user action type. A deterministic planner should convert these into a stable tuple: compute plan, priority estimate, 和 warnings.\n\nThe key teaching point is that explorer values should not mutate silently. If a user changes percentile from 50 to 75, the output should change in an obvious 和 traceable way. If volatility spread exceeds policy guard, the explorer should display a clear badge: \"guard applied.\" This approach teaches policy causality 和 prevents magical thinking about fees.\n\nExplorer 设计 should also separate confidence from urgency. Confidence describes how trustworthy the current estimate is, often based on sample depth 和 spread stability. Urgency is a user choice: how quickly inclusion is desired. Confusing these concepts leads to poor defaults 和 frustrated users. A cautious user may still choose medium urgency if confidence is low 和 warnings are high.\n\nA defensive explorer includes side-by-side outputs 用于 JSON 和 markdown summary. JSON provides machine-readable deterministic artifacts 用于 snapshots 和 regression tests. Markdown provides human-readable communication 用于 support 和 incident reviews. Both should derive from the same payload to avoid divergence.\n\nIn production teams, explorer traces can become a lightweight runbook. If a user reports repeated misses, support can replay the same inputs 和 inspect whether the policy selected reasonable values. If not, policy changes can be proposed 使用 test fixtures before rollout. If yes, the issue may be external congestion or stale quote flow, not planner logic.\n\nFrom an engineering quality perspective, deterministic explorers reduce blame cycles. Instead of \"it felt wrong,\" teams can point to exact sample sets, percentile choice, spread guard status, 和 final plan fields. This clarity is a force multiplier 用于 reliability work.\n\nThe last 设计 principle is explicit assumptions. If your explorer assumes synthetic samples, label them clearly. If it assumes no RPC feedback, state that. Honest boundaries improve trust 和 encourage users to interpret outputs correctly.\n\n\nThis material should be operationalized 使用 deterministic fixtures 和 explicit release criteria. Teams should preserve a small set of baseline scenarios that represent normal traffic, moderate stress, 和 severe stress. 用于 each scenario, compare policy outputs before 和 after changes, 和 require review notes when confidence labels, warnings, or recommendations move in a meaningful way. This discipline prevents accidental drift, keeps support playbooks aligned 使用 runtime behavior, 和 makes incident response faster because everyone shares the same deterministic artifact language. In practice, the strongest reliability teams treat these artifacts as release gates, not optional documentation, 和 they keep fixture ownership explicit so updates remain intentional 和 auditable.\n\n## Checklist\n- Show clear mapping from each input control to each output field.\n- Expose volatility guard activation as an explicit state.\n- Keep confidence 和 urgency as separate concepts.\n- Produce identical output 用于 repeated identical inputs.\n- Export JSON 和 markdown from the same canonical payload.\n",
            "duration": "50 min"
          }
        }
      },
      "pfcb-v2-project-journey": {
        "title": "Fee Optimizer Project Journey",
        "description": "Implement deterministic planners, confirmation policy engines, 和 stable fee strategy artifacts 用于 release review.",
        "lessons": {
          "pfcb-v2-plan-compute-budget": {
            "title": "Challenge: implement planComputeBudget()",
            "content": "Implement a deterministic compute budget planner. No RPC calls; operate only on provided input data.",
            "duration": "40 min"
          },
          "pfcb-v2-estimate-priority-fee": {
            "title": "Challenge: implement estimatePriorityFee()",
            "content": "Implement policy-based priority fee estimation using synthetic sample arrays 和 deterministic warnings.",
            "duration": "40 min"
          },
          "pfcb-v2-confirmation-ux-policy": {
            "title": "Challenge: confirmation level decision engine",
            "content": "Encode confirmation UX policy 用于 processed, confirmed, 和 finalized states using deterministic risk bands.",
            "duration": "35 min"
          },
          "pfcb-v2-fee-plan-summary-markdown": {
            "title": "Challenge: build feePlanSummary markdown",
            "content": "Build stable markdown output 用于 a fee strategy summary that users 和 support teams can review quickly.",
            "duration": "35 min"
          },
          "pfcb-v2-fee-optimizer-checkpoint": {
            "title": "Checkpoint: Fee Optimizer report",
            "content": "Produce a deterministic checkpoint report JSON 用于 the Fee Optimizer final project artifact.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "bundles-atomicity": {
    "title": "Bundles & 交易 Atomicity",
    "description": "设计 defensive multi-交易 Solana flows 使用 deterministic atomicity validation, compensation modeling, 和 audit-ready safety reporting.",
    "duration": "9 hours",
    "tags": [
      "atomicity",
      "bundles",
      "defensive-design",
      "solana"
    ],
    "modules": {
      "bundles-v2-atomicity-foundations": {
        "title": "Atomicity Foundations",
        "description": "User-intent expectations, flow decomposition, 和 deterministic risk-graph modeling 用于 multi-step reliability.",
        "lessons": {
          "bundles-v2-atomicity-model": {
            "title": "Atomicity concepts 和 why users assume all-or-nothing",
            "content": "# Atomicity concepts 和 why users assume all-or-nothing\n\nUsers rarely think in 交易 graphs. They think in intents: \"swap my token\" or \"close my position.\" When a workflow spans multiple 交易, user expectation remains all-or-nothing unless your UI teaches otherwise. This mismatch between intent-level atomicity 和 protocol-level execution can produce severe trust failures even when each 交易 is technically valid. Defensive engineering starts by mapping user intent boundaries 和 showing where partial execution can occur.\n\nIn Solana systems, multi-step flows are common. You may need token approval-like setup, associated token 账户 creation, route execution, 和 cleanup. Each step has independent confirmation behavior 和 can fail 用于 different reasons. If a flow halts after a preparatory step, the user can be left in a state they never intended: allowances enabled, rent paid 用于 unused 账户, or funds moved into 中级 holding 账户.\n\nA rigorous model begins 使用 explicit step typing. Every step should be tagged by function 和 risk: setup, value transfer, settlement, compensation, 和 cleanup. Then define dependencies between steps 和 mark whether each step is idempotent. Idempotency matters because retry logic can create duplicates if a step is not safely repeatable. This is not only a backend concern; frontend orchestration 和 钱包 prompts must respect idempotency constraints.\n\nAnother key concept is compensating action coverage. If a value-transfer step fails midway, does a deterministic refund path exist? If not, your flow should be marked high risk 和 your UI should block or require additional confirmation. Teams often postpone compensation 设计 until incident response, but defensive 课程 设计 should treat compensation as a first-class requirement.\n\nBundle thinking helps organize these concerns. Even without live relay APIs, you can compose a deterministic bundle structure representing intended ordering 和 invariants. This structure teaches engineers how to reason about all-or-nothing intent, retries, 和 fallback paths. It also enables stable unit tests that validate graph shape 和 risk reports.\n\nFrom a UX angle, the most important move is honest framing. If strict atomicity is not guaranteed, state it directly. Users tolerate complexity when language is clear: \"Step 2 may fail after Step 1 succeeds; automatic refund logic is applied if needed.\" Hiding this reality may reduce initial friction but increases long-term mistrust.\n\nSupport 和 incident teams benefit from deterministic flow reports. A report should list steps, dependencies, idempotency status, 和 detected issues such as missing refunds or broken dependencies. When users report failed swaps, this report enables quick triage: was the failure expected 和 safely compensated, or did the flow violate defined invariants?\n\nUltimately, atomicity is a contract between engineering 和 user expectations. Protocol constraints do not remove that responsibility. They make explicit modeling, 测试, 和 communication mandatory.\n\n\nThis material should be operationalized 使用 deterministic fixtures 和 explicit release criteria. Teams should preserve a small set of baseline scenarios that represent normal traffic, moderate stress, 和 severe stress. 用于 each scenario, compare policy outputs before 和 after changes, 和 require review notes when confidence labels, warnings, or recommendations move in a meaningful way. This discipline prevents accidental drift, keeps support playbooks aligned 使用 runtime behavior, 和 makes incident response faster because everyone shares the same deterministic artifact language. In practice, the strongest reliability teams treat these artifacts as release gates, not optional documentation, 和 they keep fixture ownership explicit so updates remain intentional 和 auditable.\n\n## Operator mindset\n\nAtomicity is a user-trust contract. If strict all-or-nothing is unavailable, compensation guarantees 和 residual risks must be explicit, testable, 和 observable in reports.\n\n## Checklist\n- Model flows by intent, not only by 交易 count.\n- Annotate each step 使用 dependencies 和 idempotency.\n- Require explicit compensation paths 用于 value-transfer failures.\n- Produce deterministic safety reports 用于 each flow version.\n- Teach users where all-or-nothing is guaranteed 和 where it is not.\n",
            "duration": "55 min"
          },
          "bundles-v2-flow-risk-points": {
            "title": "Multi-交易 flows: approvals, ATA creation, swaps, refunds",
            "content": "# Multi-交易 flows: approvals, ATA creation, swaps, refunds\n\nA reliable flow simulator must encode where partial execution risk lives. In practice, risk points cluster at boundaries: before value transfer, during value transfer, 和 after value transfer when cleanup or refund steps should run. This 课时 maps common Solana flow stages 和 shows defensive controls that keep failure behavior predictable.\n\nThe first stage is prerequisite setup. 账户 initialization 和 ATA creation are often safe 和 idempotent if implemented correctly, but they still consume fees 和 may fail under congestion. If setup fails, users should see precise messaging 和 retry guidance. If setup succeeds 和 later steps fail, your state machine must remember setup completion to avoid duplicate 账户 creation attempts.\n\nThe second stage is authorization-like setup. On Solana this may differ from EVM approvals, but the pattern remains: a step grants capability to later 指令. Non-idempotent or overly broad permissions here amplify downstream risk. Flow 验证者 should detect non-idempotent authorization steps 和 force explicit refund or revocation logic if subsequent steps fail.\n\nThe third stage is value transfer or swap execution. This is where market drift, stale quotes, 和 route failure can break expectations. A deterministic simulator should not fetch live prices; instead it should model success/failure branches 和 expected compensation behavior. This lets teams test policy without network noise.\n\nThe fourth stage is compensation. If swap execution fails after setup or partial settlement, compensation is the difference between recoverable error 和 user-facing loss. Compensation steps must be discoverable, ordered, 和 testable. Simulators should flag flows missing compensation when any non-idempotent or value-affecting step exists.\n\nThe fifth stage is cleanup. Cleanup can include revoking transient permissions, closing temporary 账户, or recording final status artifacts. Cleanup should be safe to retry 和 should not hide failures. Some teams skip cleanup during congestion, but then debt accumulates in user 账户 和 backend state.\n\nDefensive patterns include idempotency keys 用于 orchestration, deterministic status transitions, 和 explicit issue codes 用于 each risk category. 用于 example, the missing-refund issue code should always map to the same report semantics so monitoring dashboards remain stable.\n\nA flow graph explorer can teach these points effectively. By visualizing nodes 和 edges 使用 risk annotations, teams quickly see where assumptions are weak. Edges should represent hard dependencies, not optional sequencing preferences. If a dependency references a missing step, the graph should fail validation immediately.\n\nDuring incident reviews, deterministic graph reports outperform log fragments. They provide compact, reproducible context: what was planned, what safety checks failed, 和 which invariants were violated. This reduces MTTR 和 avoids repeated misclassification.\n\n\nThis material should be operationalized 使用 deterministic fixtures 和 explicit release criteria. Teams should preserve a small set of baseline scenarios that represent normal traffic, moderate stress, 和 severe stress. 用于 each scenario, compare policy outputs before 和 after changes, 和 require review notes when confidence labels, warnings, or recommendations move in a meaningful way. This discipline prevents accidental drift, keeps support playbooks aligned 使用 runtime behavior, 和 makes incident response faster because everyone shares the same deterministic artifact language. In practice, the strongest reliability teams treat these artifacts as release gates, not optional documentation, 和 they keep fixture ownership explicit so updates remain intentional 和 auditable.\n\n## Checklist\n- Label setup, value, compensation, 和 cleanup steps explicitly.\n- Treat non-idempotent setup as high-risk without compensating actions.\n- Validate dependency graph integrity before execution planning.\n- Encode deterministic issue codes 和 severity mapping.\n- Keep simulator behavior offline 和 reproducible.\n",
            "duration": "55 min"
          },
          "bundles-v2-flow-explorer": {
            "title": "Explorer: flow graph steps 和 risk points",
            "content": "# Explorer: flow graph steps 和 risk points\n\nFlow graph explorers are most valuable when they highlight risk semantics, not just sequence order. A defensive explorer should display each step 使用 dependency context, idempotency flag, 和 compensation coverage. Engineers should be able to answer three questions immediately: what can fail, what can be retried safely, 和 what protects users if a value step fails.\n\nStart by treating each node as a contract. A node contract defines preconditions, side effects, 和 postconditions. Preconditions include required upstream steps 和 expected inputs. Side effects include 账户 state changes or transfer intents. Postconditions include observable status updates 和 possible compensation requirements. When node contracts are explicit, validation rules become straightforward 和 deterministic.\n\nEdges in the graph should represent hard causality. If step B depends on step A output, represent that as an edge 和 validate existence at build time. Optional order preferences should not be encoded as dependencies because they can produce false positives 和 brittle reports. Keep graph semantics strict 和 minimal.\n\nRisk annotations should be first-class fields. Instead of deducing risk later from prose, attach tags such as value-transfer, non-idempotent, requires-refund, 和 cleanup-only. Report generation can then aggregate these tags into issue summaries 和 recommended mitigations.\n\nA robust explorer also teaches \"atomic in user model\" versus \"atomic on chain.\" You can annotate the whole flow 使用 intent boundary metadata that states whether strict atomic guarantee exists. If not, the explorer should list compensation guarantees 和 residual risk in plain language.\n\nDeterministic bundle composition is a useful next layer. Even without calling relay services, you can generate a bundle artifact that enumerates 交易 groupings 和 invariants. This allows stable comparisons across policy revisions. If a future change removes a refund invariant, tests should fail immediately.\n\nEngineers should avoid dynamic output fields like timestamps inside core report payloads. Keep those in outer metadata if needed. Stable JSON 和 markdown outputs make review diffs reliable 和 reduce false positives in CI snapshots.\n\nFrom a teaching standpoint, explorer sessions should include both safe 和 unsafe examples. Seeing a missing dependency or missing refund issue in a concrete graph is more memorable than reading abstract warnings. The 课程 challenge sequence then asks learners to codify the same checks.\n\nFinally, remember that atomicity work is reliability work. It is not a special 安全-only track. The same graph discipline helps product, backend, 和 support teams share one truth source 用于 multi-step behavior.\n\n\nThis material should be operationalized 使用 deterministic fixtures 和 explicit release criteria. Teams should preserve a small set of baseline scenarios that represent normal traffic, moderate stress, 和 severe stress. 用于 each scenario, compare policy outputs before 和 after changes, 和 require review notes when confidence labels, warnings, or recommendations move in a meaningful way. This discipline prevents accidental drift, keeps support playbooks aligned 使用 runtime behavior, 和 makes incident response faster because everyone shares the same deterministic artifact language. In practice, the strongest reliability teams treat these artifacts as release gates, not optional documentation, 和 they keep fixture ownership explicit so updates remain intentional 和 auditable.\n\n## Checklist\n- Represent node contracts 和 dependency edges explicitly.\n- Annotate risk tags directly in graph data.\n- Distinguish user-intent atomicity from protocol guarantees.\n- Generate deterministic bundle 和 report artifacts.\n- Include unsafe example graphs in test fixtures.\n",
            "duration": "50 min"
          }
        }
      },
      "bundles-v2-project-journey": {
        "title": "Atomic Swap Flow Simulator",
        "description": "Build, validate, 和 report deterministic flow safety 使用 compensation checks, idempotency handling, 和 bundle artifacts.",
        "lessons": {
          "bundles-v2-build-atomic-flow": {
            "title": "Challenge: implement buildAtomicFlow()",
            "content": "Build a normalized deterministic flow graph from steps 和 dependencies.",
            "duration": "40 min"
          },
          "bundles-v2-validate-atomicity": {
            "title": "Challenge: implement validateAtomicity()",
            "content": "Detect partial execution risk, missing refunds, 和 non-idempotent steps.",
            "duration": "40 min"
          },
          "bundles-v2-failure-handling-patterns": {
            "title": "Challenge: failure handling 使用 idempotency keys",
            "content": "Encode deterministic failure handling metadata, including compensation state.",
            "duration": "35 min"
          },
          "bundles-v2-bundle-composer": {
            "title": "Challenge: deterministic bundle composer",
            "content": "Compose a deterministic bundle structure 用于 an atomic flow. No relay calls.",
            "duration": "35 min"
          },
          "bundles-v2-flow-safety-report": {
            "title": "Checkpoint: flow safety report",
            "content": "Generate a stable markdown flow safety report checkpoint artifact.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "mempool-ux-defense": {
    "title": "Mempool Reality & Anti-Sandwich UX",
    "description": "Defensive swap UX engineering 使用 deterministic risk grading, bounded slippage policies, 和 incident-ready safety communication.",
    "duration": "9 hours",
    "tags": [
      "mempool",
      "ux",
      "slippage",
      "risk-policy"
    ],
    "modules": {
      "mempoolux-v2-foundations": {
        "title": "Mempool Reality 和 UX Defense",
        "description": "Quote-to-execution risk modeling, slippage guardrails, 和 defensive user education 用于 safer swap decisions.",
        "lessons": {
          "mempoolux-v2-quote-execution-gap": {
            "title": "What can go wrong between quote 和 execution",
            "content": "# What can go wrong between quote 和 execution\n\nA swap quote is a prediction, not a guarantee. Between quote generation 和 execution, liquidity changes, competing orders land, 和 network conditions shift. Users often assume that seeing a quote means they will receive that outcome, but production UX must teach 和 enforce the gap between quote time 和 execution time. This 课程 is defensive by 设计: no exploit strategies, only protective policy 和 communication.\n\nThe first risk is quote staleness. Even in calm periods, a quote generated several seconds ago can diverge from current route quality. During high activity, divergence can happen in sub-second windows. A protective UI should track quote age continuously 和 degrade confidence as age increases. At defined thresholds, it should warn or block execution until a refresh occurs.\n\nThe second risk is slippage misconfiguration. Slippage tolerance exists to bound acceptable execution drift. If set too tight, legitimate 交易 fail frequently. If set too wide, users can receive unexpectedly poor execution. Defensive systems define policy bounds 和 recommend values based on route characteristics, not a single static default.\n\nThe third risk is 价格影响 misunderstanding. 价格影响 measures how much your order moves market price due to route depth. Slippage tolerance measures allowed execution variance. They are related but not interchangeable. Teaching this difference prevents users from widening slippage to \"fix\" impact-heavy trades that should instead be resized or rerouted.\n\nThe fourth risk is route complexity. Multi-hop routes can improve nominal quote value but introduce more points of state dependency 和 timing drift. A risk engine should 账户 用于 hop count as a reliability input. This does not mean all multi-hop routes are unsafe; it means risk should be surfaced proportionally.\n\nThe fifth risk is liquidity quality. Low-liquidity routes are more fragile under contention. Deterministic scoring can treat liquidity as one signal among many, producing grade outputs like low, medium, high, 和 critical. Grades should be accompanied by reasons, so warnings are explainable.\n\nProtective UX is not just warning banners. It includes defaults, disabled states, timed refresh prompts, 和 clear language about what each control does. If users do not understand controls, they either ignore them or misconfigure them. The best interfaces explain tradeoffs in one sentence 和 keep 高级 controls available without forcing novices into risky settings.\n\nPolicy engines should produce deterministic artifacts 用于 testability. Given identical input tuples, risk grade 和 warnings should remain identical. This enables stable unit tests 和 predictable support behavior. It also allows teams to review policy changes as code diffs rather than subjective UI adjustments.\n\nThe goal is not zero failed swaps; the goal is informed, bounded risk 使用 transparent behavior. Users accept tradeoffs when systems are honest 和 consistent.\n\n\nThis material should be operationalized 使用 deterministic fixtures 和 explicit release criteria. Teams should preserve a small set of baseline scenarios that represent normal traffic, moderate stress, 和 severe stress. 用于 each scenario, compare policy outputs before 和 after changes, 和 require review notes when confidence labels, warnings, or recommendations move in a meaningful way. This discipline prevents accidental drift, keeps support playbooks aligned 使用 runtime behavior, 和 makes incident response faster because everyone shares the same deterministic artifact language. In practice, the strongest reliability teams treat these artifacts as release gates, not optional documentation, 和 they keep fixture ownership explicit so updates remain intentional 和 auditable.\n\n## Operator mindset\n\nProtected swap UX is policy UX. Defaults, warnings, 和 block states should be deterministic, explainable, 和 versioned so teams can defend decisions during incidents.\n\n## Checklist\n- Track quote age 和 apply graded stale-quote policies.\n- Separate 价格影响 education from slippage controls.\n- Incorporate route hops 和 liquidity into risk scoring.\n- Emit deterministic risk reasons 用于 UX copy.\n- Block execution only when policy thresholds are clearly crossed.\n",
            "duration": "55 min"
          },
          "mempoolux-v2-slippage-guardrails": {
            "title": "Slippage controls 和 guardrails",
            "content": "# Slippage controls 和 guardrails\n\nSlippage settings are a policy surface, not a cosmetic preference. Defensive swap UX defines explicit bounds, context-aware defaults, 和 clear consequences when users attempt risky overrides. This 课时 focuses on guardrail 设计 that reduces avoidable losses while preserving user agency.\n\nA strong policy starts 使用 minimum 和 maximum bounds. The minimum protects against unusable settings that cause endless failures. The maximum protects against overly permissive settings that convert volatility into severe execution loss. Between bounds, choose a default aligned 使用 typical route behavior. 用于 many flows this is moderate, then dynamically adjusted by quote freshness 和 impact context.\n\nGuardrails should respond to stale quotes. If quote age passes a threshold, a safe policy can lower recommended slippage 和 request refresh before signing. If quote age becomes severely stale, execution should be blocked 使用 a deterministic message. Blocking should be rare but unambiguous. Users should know whether a refresh can unblock immediately.\n\nImpact-aware adjustment is another essential control. High projected impact may require either tighter trade sizing or broader tolerance depending on objective. Defensive UX should encourage reviewing trade size first, not instantly widening tolerance. If users choose high tolerance anyway, warnings should explain downside plainly.\n\nOverride behavior must be deterministic. When a user-selected value exceeds policy max, clamp it 和 emit a warning that can be exported in reports. Silent clamping is dangerous because users think they are running one setting while the engine uses another. Explicit feedback builds trust 和 prevents support confusion.\n\nCopy quality matters. Avoid technical jargon in warning body text. A good warning says what is wrong, why it matters, 和 what to do next. 用于 example: \"Quote is stale; refresh before signing to avoid outdated execution terms.\" This is better than \"staleness threshold exceeded.\" Engineers can keep technical details in debug exports.\n\nGuardrails should also integrate 使用 route preview components. Showing risk grade beside slippage recommendation helps users interpret controls in context. If grade is high 和 slippage recommendation is near max, the UI should highlight additional caution 和 maybe suggest smaller size.\n\nFrom an implementation perspective, a pure deterministic function is ideal: input config plus quote context yields warnings, recommended bps, 和 blocked flag. This function can be unit tested across edge scenarios 和 reused in frontend 和 backend validation paths.\n\nFinally, policy reviews should be versioned. If teams change bounds or thresholds, they should compare old 和 new outputs across fixture sets before rollout. This prevents regressions where well-intended tweaks accidentally increase risk exposure.\n\n\nThis material should be operationalized 使用 deterministic fixtures 和 explicit release criteria. Teams should preserve a small set of baseline scenarios that represent normal traffic, moderate stress, 和 severe stress. 用于 each scenario, compare policy outputs before 和 after changes, 和 require review notes when confidence labels, warnings, or recommendations move in a meaningful way. This discipline prevents accidental drift, keeps support playbooks aligned 使用 runtime behavior, 和 makes incident response faster because everyone shares the same deterministic artifact language. In practice, the strongest reliability teams treat these artifacts as release gates, not optional documentation, 和 they keep fixture ownership explicit so updates remain intentional 和 auditable.\n\n## Checklist\n- Define min, default, 和 max slippage as explicit policy values.\n- Apply stale-quote logic before execution 和 adjust recommendations.\n- Clamp unsafe overrides 使用 clear warning messages.\n- Surface blocked state only 用于 clearly defined severe conditions.\n- Keep policy deterministic 和 version-reviewable.\n",
            "duration": "55 min"
          },
          "mempoolux-v2-freshness-explorer": {
            "title": "Explorer: quote freshness timer 和 decision table",
            "content": "# Explorer: quote freshness timer 和 decision table\n\nA quote freshness explorer should make policy behavior obvious under time pressure. Users 和 engineers need to see when a quote transitions from safe to warning to blocked. This 课时 defines a decision table approach that pairs timer state 使用 slippage 和 impact context.\n\nThe timer should not be a decorative countdown. It is a state driver 使用 explicit thresholds. 用于 example, 0-10 seconds may be low concern, 10-20 seconds warning, 和 above 20 seconds blocked 用于 certain route classes. Thresholds can vary by asset class 和 liquidity quality, but the explorer must display the active policy version so users understand why behavior changed.\n\nDecision tables combine timer bands 使用 additional signals: projected impact, hop count, 和 liquidity score. A single stale timer does not always imply severe risk; it depends on route fragility. Deterministic scoring helps aggregate these dimensions into one grade while preserving reason strings.\n\nAn effective explorer view presents both grade 和 recommendation fields. Grade communicates severity. Recommendation communicates next action: refresh quote, tighten slippage, reduce size, or proceed. Without recommendation, users see red flags but lack direction.\n\nEngineers should include edge fixtures where metrics conflict. Example: fresh quote but very high impact 和 low liquidity; or stale quote 使用 low impact 和 high liquidity. These fixtures prevent simplistic heuristics from dominating policy 和 help teams calibrate thresholds intentionally.\n\nThe explorer also supports user education around anti-sandwich posture without teaching offensive behavior. You can explain that wider slippage 和 stale quotes increase adverse execution risk, 和 that refreshing quote plus tighter controls reduces exposure. Keep messaging defensive 和 实战.\n\n用于 reliability teams, deterministic explorer outputs become regression baselines. If a code change alters grade 用于 a fixture unexpectedly, CI catches it before production. This is particularly important when tuning thresholds during volatile periods.\n\nOutput formatting should remain stable. Use canonical JSON order 用于 exported config, 和 stable markdown 用于 support docs. Avoid timestamps in core payloads to preserve snapshot equality. If timestamps are required, store them outside deterministic artifact fields.\n\nFinally, link explorer states to UI banners. If grade is critical, banner severity should be error 使用 explicit action. If grade is medium, warning banner 使用 optional guidance may suffice. This mapping is implemented in later challenges.\n\n\nThis material should be operationalized 使用 deterministic fixtures 和 explicit release criteria. Teams should preserve a small set of baseline scenarios that represent normal traffic, moderate stress, 和 severe stress. 用于 each scenario, compare policy outputs before 和 after changes, 和 require review notes when confidence labels, warnings, or recommendations move in a meaningful way. This discipline prevents accidental drift, keeps support playbooks aligned 使用 runtime behavior, 和 makes incident response faster because everyone shares the same deterministic artifact language. In practice, the strongest reliability teams treat these artifacts as release gates, not optional documentation, 和 they keep fixture ownership explicit so updates remain intentional 和 auditable.\n\n## Checklist\n- Treat freshness timer as policy input, not visual decoration.\n- Combine timer state 使用 impact, hops, 和 liquidity signals.\n- Emit grade plus actionable recommendation.\n- Test conflicting-signal fixtures 用于 policy balance.\n- Keep exported artifacts deterministic 和 stable.\n",
            "duration": "50 min"
          }
        }
      },
      "mempoolux-v2-project-journey": {
        "title": "Protected Swap UI Project Journey",
        "description": "Implement deterministic policy engines, safety messaging, 和 stable protection-config artifacts 用于 release 治理.",
        "lessons": {
          "mempoolux-v2-evaluate-swap-risk": {
            "title": "Challenge: implement evaluateSwapRisk()",
            "content": "Implement deterministic swap risk grading from quote, slippage, impact, hops, 和 liquidity inputs.",
            "duration": "40 min"
          },
          "mempoolux-v2-slippage-guard": {
            "title": "Challenge: implement slippageGuard()",
            "content": "Build bounded slippage recommendations 使用 warnings 和 hard-block states.",
            "duration": "40 min"
          },
          "mempoolux-v2-impact-vs-slippage": {
            "title": "Challenge: model 价格影响 vs slippage",
            "content": "Encode a deterministic interpretation of impact-to-tolerance ratio 用于 user education.",
            "duration": "35 min"
          },
          "mempoolux-v2-swap-safety-banner": {
            "title": "Challenge: build swapSafetyBanner()",
            "content": "Map deterministic risk grades to defensive banner copy 和 severity.",
            "duration": "35 min"
          },
          "mempoolux-v2-protection-config-export": {
            "title": "Checkpoint: swap protection config export",
            "content": "Export a stable deterministic policy config artifact 用于 the Protected Swap UI checkpoint.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "indexing-webhooks-pipelines": {
    "title": "Indexers, Webhooks & Reorg-Safe Pipelines",
    "description": "Build production-grade deterministic indexing pipelines 用于 duplicate-safe ingestion, reorg handling, 和 integrity-first reporting.",
    "duration": "9 hours",
    "tags": [
      "indexing",
      "webhooks",
      "reorgs",
      "reliability"
    ],
    "modules": {
      "indexpipe-v2-foundations": {
        "title": "Indexer Reliability Foundations",
        "description": "Event identity modeling, confirmation semantics, 和 deterministic ingest-to-apply pipeline behavior.",
        "lessons": {
          "indexpipe-v2-indexing-basics": {
            "title": "Indexing 101: logs, 账户, 和 交易 parsing",
            "content": "# Indexing 101: logs, 账户, 和 交易 parsing\n\nReliable indexers are not just fast parsers. They are consistency systems that decide what to trust, when to trust it, 和 how to recover from changing chain history. On Solana, event ingestion often starts from logs or parsed 指令, but production pipelines need deterministic keying, replay controls, 和 state application rules that survive retries 和 reorgs.\n\nA basic pipeline has four stages: ingest, dedupe, confirmation gating, 和 state apply. Ingest captures raw events 使用 enough metadata to reconstruct ordering context: slot, signature, 指令 index, event type, 和 affected 账户. Dedupe ensures duplicate deliveries do not produce duplicate state transitions. Confirmation gating delays state application until depth conditions are met. Apply mutates snapshots in deterministic order.\n\nMany teams fail in the first stage by capturing incomplete event identity fields. If you omit 指令 index or event kind, collisions appear 和 dedupe becomes unsafe. Composite keys should be explicit 和 stable. They should also be derived purely from event payload so keys remain reproducible in tests 和 backfills.\n\nParsing strategy matters too. Logs are convenient but can drift across program versions. Parsed 指令 data can be more structured but may require custom decoders. Defensive indexing stores normalized events in one canonical schema regardless of source. This isolates downstream logic from parser changes.\n\nIdempotency is essential. Your ingestion path may receive duplicates from retries, webhook redelivery, or backfill overlap. If dedupe is weak, balances drift 和 downstream analytics become untrustworthy. Deterministic dedupe 使用 composite keys is the first line of defense.\n\nThe apply stage should avoid hidden nondeterminism. If events are applied in arrival order without stable sort keys, two replays can produce different snapshots. Always sort by deterministic key before apply. If you need tie-breakers, define them explicitly.\n\nSnapshot 设计 should prioritize auditability. Keep applied event keys, pending keys, 和 finalized keys visible. These sets make it easy to reason about what the snapshot currently reflects 和 why. They also simplify integrity checks later.\n\nFinally, keep deterministic outputs central to your developer workflow. Pipeline reports 和 snapshots should be exportable in stable formats 用于 test snapshots 和 incident analysis. Reliability work depends on reproducible evidence.\n\n\nTo keep this durable, teams should document fixture ownership 和 rotate review responsibilities so event taxonomy stays aligned 使用 protocol upgrades. Without this operational ownership, pipelines drift into untested assumptions, 和 recovery playbooks age out. Deterministic explorers stay valuable only when fixtures evolve 使用 production reality 和 every stage still reports clear, machine-verifiable state transitions under replay 和 stress.\n\nThis material should be operationalized 使用 deterministic fixtures 和 explicit release criteria. Teams should preserve a small set of baseline scenarios that represent normal traffic, moderate stress, 和 severe stress. 用于 each scenario, compare policy outputs before 和 after changes, 和 require review notes when confidence labels, warnings, or recommendations move in a meaningful way. This discipline prevents accidental drift, keeps support playbooks aligned 使用 runtime behavior, 和 makes incident response faster because everyone shares the same deterministic artifact language. In practice, the strongest reliability teams treat these artifacts as release gates, not optional documentation, 和 they keep fixture ownership explicit so updates remain intentional 和 auditable.\n\n## Operator mindset\n\nIndexing is a correctness pipeline before it is an analytics pipeline. Fast ingestion 使用 weak dedupe, confirmation, or replay guarantees produces confidently wrong outputs.\n\n## Checklist\n- Capture complete event identity fields at ingest time.\n- Normalize events from logs 和 parsed 指令 into one schema.\n- Use deterministic composite keys 用于 dedupe.\n- Sort events stably before state application.\n- Track applied, pending, 和 finalized sets in snapshots.\n",
            "duration": "55 min"
          },
          "indexpipe-v2-reorg-confirmation-reality": {
            "title": "Reorgs 和 fork choice: why confirmed is not finalized",
            "content": "# Reorgs 和 fork choice: why confirmed is not finalized\n\nConfirmation labels are useful but often misunderstood in indexing pipelines. A confirmed event has stronger confidence than processed, but it is not equivalent to final settlement. Pipelines that apply confirmed events directly to user-visible balances without rollback strategy can show transient truth as permanent truth. Defensive 设计 acknowledges this 和 encodes reversible state transitions.\n\nReorg-aware indexing starts 使用 depth thresholds. 用于 each event, compute depth as head slot minus event slot. If depth is below confirmed threshold, event remains pending. If depth passes confirmed threshold, event can be applied to provisional state. If depth passes finalized threshold, event is considered settled. These rules should be policy inputs, not hidden constants.\n\nWhy maintain provisional state at all? Because users 和 systems often need timely feedback before finalization. The solution is not to ignore confirmed events but to annotate confidence clearly. Dashboards can show provisional balances 使用 settlement badges. Automated systems can choose whether to act on provisional or finalized data.\n\nFork choice changes can invalidate previously observed confirmed events. If your pipeline tracks applied keys 和 supports replay, you can recompute snapshot deterministically from deduped events 和 updated confirmation context. Pipelines that mutate opaque state without replay ability struggle during reorg recovery.\n\nDeterministic apply logic helps here. If the same deduped event set 和 same confirmation policy produce the same snapshot every run, recovery is straightforward. If apply order depends on arrival timing, recovery becomes guesswork.\n\nAnother reliability pattern is explicit pending queues. Instead of dropping low-depth events, keep them keyed 和 observable. This improves debugging: you can explain to users that an event exists but has not crossed confirmation threshold. It also avoids ingestion gaps when head advances.\n\nIntegrity checks should enforce structural assumptions: finalized keys must be a subset of applied keys, balances must be finite 和 non-negative under your business rules, 和 snapshot counts should align 使用 event sets. Failing these checks should mark snapshot as invalid 和 block downstream export.\n\nCommunication matters as much as mechanics. Product teams should avoid copy that implies final settlement when data is only confirmed. Small text differences reduce major support incidents during volatile periods.\n\nThe overarching principle is to make uncertainty explicit 和 reversible. Reorg-safe pipelines are less about predicting forks 和 more about handling them cleanly when they happen.\n\n\nThis material should be operationalized 使用 deterministic fixtures 和 explicit release criteria. Teams should preserve a small set of baseline scenarios that represent normal traffic, moderate stress, 和 severe stress. 用于 each scenario, compare policy outputs before 和 after changes, 和 require review notes when confidence labels, warnings, or recommendations move in a meaningful way. This discipline prevents accidental drift, keeps support playbooks aligned 使用 runtime behavior, 和 makes incident response faster because everyone shares the same deterministic artifact language. In practice, the strongest reliability teams treat these artifacts as release gates, not optional documentation, 和 they keep fixture ownership explicit so updates remain intentional 和 auditable.\n\n## Checklist\n- Define confirmed 和 finalized depth thresholds explicitly.\n- Separate pending, applied, 和 finalized event sets.\n- Keep replayable deterministic apply logic.\n- Run integrity checks on every snapshot export.\n- Surface settlement confidence clearly in UI 和 APIs.\n",
            "duration": "55 min"
          },
          "indexpipe-v2-pipeline-explorer": {
            "title": "Explorer: ingest to dedupe to confirm to apply",
            "content": "# Explorer: ingest to dedupe to confirm to apply\n\nA pipeline explorer should explain transformation stages clearly so engineers can inspect where correctness can break. 用于 indexing reliability, the core stages are ingest, dedupe, confirmation gating, 和 apply. Each stage must expose deterministic inputs 和 outputs.\n\nIngest stage receives raw events from simulated webhooks, log streams, or backfills. At this point, duplicates 和 out-of-order delivery are expected. The explorer should show raw count 和 normalized schema count so users can verify parser coverage.\n\nDedupe stage converts event arrays into a set based on composite keys. Good explorers display before/after counts 和 list dropped duplicates. This transparency helps debug webhook retries 和 backfill overlap behavior.\n\nConfirmation stage partitions deduped events into pending, applied, 和 finalized sets based on depth policy. The explorer should make head slot 和 policy thresholds visible. Hidden thresholds are a frequent source of confusion when teams compare environments.\n\nApply stage computes 账户 balances or state snapshots deterministically from applied events only. Explorer outputs should include sorted balances 和 event key lists. Sorted output is crucial 用于 snapshot equality 测试.\n\nIntegrity stage validates structural assumptions: no negative balances, no non-finite numbers, finalized subset relation, 和 stable event references. The explorer should display PASS/FAIL 和 issue list. This teaches engineers to treat integrity checks as mandatory gates, not optional diagnostics.\n\n用于 backfills, explorer scenarios should include missing-slot windows 和 idempotency keys. This demonstrates how replay-safe job planning interacts 使用 the same dedupe 和 apply rules. A reliable backfill system does not bypass core pipeline logic.\n\nDeterministic report generation closes the loop. Export markdown 用于 human review 和 JSON 用于 machine consumption. Both should be reproducible from the same snapshot object. Avoid embedding volatile metadata in core payload fields.\n\nA well-designed explorer becomes a teaching tool 和 an operational tool. During incidents, teams can replay problematic event sets 和 compare outputs across policy versions. During onboarding, new engineers 学习 stage responsibilities quickly without production access.\n\nOperational ownership keeps this useful over time. Teams should rotate fixture maintenance responsibilities 和 document why each scenario exists so updates remain intentional. As protocols evolve, parser assumptions 和 event fields can drift. A maintained explorer corpus catches drift early, forces policy review before releases, 和 preserves confidence that ingest, dedupe, confirmation gating, 和 apply stages still produce reproducible results under stress.\n\n\nThis material should be operationalized 使用 deterministic fixtures 和 explicit release criteria. Teams should preserve a small set of baseline scenarios that represent normal traffic, moderate stress, 和 severe stress. 用于 each scenario, compare policy outputs before 和 after changes, 和 require review notes when confidence labels, warnings, or recommendations move in a meaningful way. This discipline prevents accidental drift, keeps support playbooks aligned 使用 runtime behavior, 和 makes incident response faster because everyone shares the same deterministic artifact language. In practice, the strongest reliability teams treat these artifacts as release gates, not optional documentation, 和 they keep fixture ownership explicit so updates remain intentional 和 auditable.\n\n## Checklist\n- Show per-stage counts 和 transformations.\n- Make confirmation policy parameters explicit.\n- Render sorted deterministic snapshots.\n- Gate exports on integrity checks.\n- Keep report payloads stable 用于 regression tests.\n",
            "duration": "50 min"
          }
        }
      },
      "indexpipe-v2-project-journey": {
        "title": "Reorg-Safe Indexer Project Journey",
        "description": "Build dedupe, confirmation-aware apply logic, integrity gates, 和 stable reporting artifacts 用于 operational triage.",
        "lessons": {
          "indexpipe-v2-dedupe-events": {
            "title": "Challenge: implement dedupeEvents()",
            "content": "Implement stable event deduplication 使用 deterministic composite keys.",
            "duration": "40 min"
          },
          "indexpipe-v2-apply-confirmations": {
            "title": "Challenge: implement applyWithConfirmations()",
            "content": "Apply events deterministically 使用 confirmation depth policy 和 pending/finalized sets.",
            "duration": "40 min"
          },
          "indexpipe-v2-backfill-idempotency": {
            "title": "Challenge: backfill 和 idempotency planning",
            "content": "Create deterministic backfill planning output 使用 replay-safe idempotency keys.",
            "duration": "35 min"
          },
          "indexpipe-v2-snapshot-integrity": {
            "title": "Challenge: snapshot integrity checks",
            "content": "Implement deterministic snapshotIntegrityCheck() outputs 用于 negative 和 structural failures.",
            "duration": "35 min"
          },
          "indexpipe-v2-pipeline-report-checkpoint": {
            "title": "Checkpoint: pipeline report export",
            "content": "Generate a stable markdown report artifact 用于 the Reorg-Safe Indexer checkpoint.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "rpc-reliability-latency": {
    "title": "RPC Reliability & Latency Engineering",
    "description": "Engineer production multi-provider Solana RPC clients 使用 deterministic retry, routing, caching, 和 observability policies.",
    "duration": "9 hours",
    "tags": [
      "rpc",
      "latency",
      "reliability",
      "observability"
    ],
    "modules": {
      "rpc-v2-foundations": {
        "title": "RPC Reliability Foundations",
        "description": "Real-world RPC failure behavior, endpoint selection strategy, 和 deterministic retry policy modeling.",
        "lessons": {
          "rpc-v2-failure-landscape": {
            "title": "RPC failures in real life: timeouts, 429s, stale nodes",
            "content": "# RPC failures in real life: timeouts, 429s, stale nodes\n\nReliable client infrastructure begins 使用 realistic failure assumptions. RPC calls fail 用于 many reasons: transient network timeouts, provider rate limits, stale nodes trailing cluster head, 和 occasional inconsistent responses under load. A defensive client does not treat these as edge cases; it treats them as normal operating conditions.\n\nTimeouts are the most common class. If timeout values are too short, healthy providers appear unreliable. If too long, user-facing latency becomes unacceptable 和 retries trigger too late. Good policy defines request timeout by operation type 和 sets bounded retry schedules.\n\nHTTP 429 rate limiting is another predictable behavior, not a surprise. Providers enforce quotas 和 burst controls. A resilient client observes 429 ratio per endpoint 和 adapts by reducing pressure on overloaded nodes while shifting traffic to healthier ones. Blind retry against the same endpoint amplifies throttling.\n\nStale node lag is particularly dangerous 用于 state-sensitive applications. A node can respond quickly but serve outdated slot state, causing confusing balances or stale quote decisions. Endpoint health scoring should include slot lag, not only latency 和 success rate.\n\nMulti-provider strategy is the baseline 用于 serious applications. Even when one provider is excellent, outages 和 regional issues happen. A client should maintain endpoint metadata, collect health samples, 和 choose endpoints by deterministic policy rather than random rotation.\n\nObservability is what makes reliability engineering actionable. Track total requests, success/error counts, latency quantiles, 和 histogram buckets. Without this telemetry, teams tune retry policies by anecdote. 使用 telemetry, teams can identify whether changes improve p95 latency or simply shift failures around.\n\nDeterministic policy modeling is valuable before production integration. You can simulate endpoint samples 和 verify that selection behavior is stable 和 explainable. If the chosen endpoint changes unexpectedly 用于 identical input samples, your scoring function needs refinement.\n\nCaching adds complexity. Cache misses 和 stale reads are not just 性能 details; they affect correctness. Invalidation policy should react to 账户 changes 和 node lag. Aggressive invalidation may increase load; weak invalidation may serve stale state. Explicit policy 和 metrics help navigate this tradeoff.\n\nThe core message is pragmatic: assume RPC instability, 设计 用于 graceful degradation, 和 measure everything 使用 deterministic reducers that can be unit tested.\n\n\nOperational readiness also requires owning fixture updates as providers change rate-limit behavior 和 latency profiles. If fixture sets stay static, policy tuning optimizes 用于 old incidents 和 misses new failure signatures. Keep a cadence 用于 reviewing percentile distributions, endpoint score drift, 和 retry outcomes so deterministic policies remain grounded in current provider behavior while preserving reproducibility.\n\nThis material should be operationalized 使用 deterministic fixtures 和 explicit release criteria. Teams should preserve a small set of baseline scenarios that represent normal traffic, moderate stress, 和 severe stress. 用于 each scenario, compare policy outputs before 和 after changes, 和 require review notes when confidence labels, warnings, or recommendations move in a meaningful way. This discipline prevents accidental drift, keeps support playbooks aligned 使用 runtime behavior, 和 makes incident response faster because everyone shares the same deterministic artifact language. In practice, the strongest reliability teams treat these artifacts as release gates, not optional documentation, 和 they keep fixture ownership explicit so updates remain intentional 和 auditable.\n\n## Operator mindset\n\nRPC policy is risk routing, not just request routing. Endpoint choice, retry cadence, 和 cache invalidation directly determine whether users see timely truth or stale confusion.\n\n## Checklist\n- Treat timeouts, 429s, 和 stale lag as default conditions.\n- Use multi-provider endpoint selection 使用 health scoring.\n- Include slot lag in endpoint quality calculations.\n- Define retry schedules 使用 bounded backoff.\n- Instrument latency 和 success metrics continuously.\n",
            "duration": "55 min"
          },
          "rpc-v2-multi-endpoint-strategies": {
            "title": "Multi-endpoint strategies: hedged requests 和 fallbacks",
            "content": "# Multi-endpoint strategies: hedged requests 和 fallbacks\n\nMulti-endpoint 设计 is more than adding a backup URL. It is a scheduling problem where each request should be sent to the most suitable endpoint given recent health signals 和 operation urgency. This 课时 focuses on deterministic strategy patterns you can validate offline.\n\nFallback strategy is the simplest pattern: try one endpoint, then another on failure. It reduces outage risk but may still produce high tail latency if initial endpoints are degraded. Hedged strategy improves tail latency by issuing a second request after a short delay if the first has not returned. Hedging increases load, so it must be controlled by policy 和 only used 用于 high-value paths.\n\nEndpoint selection should rely on a composite score that includes success rate, p95 latency, rate-limit ratio, slot lag, 和 optional static weight 用于 trusted providers. Scores should be computed deterministically from sampled inputs so decisions are reproducible. Tie-breaking should also be deterministic to avoid flapping.\n\nRate-limit-aware routing is critical. If one provider shows increasing 429 ratio, a resilient client should back off traffic there 和 prefer alternatives. This avoids retry storms 和 helps maintain aggregate throughput.\n\nRegional diversity adds resilience. If all endpoints are in one region, regional network incidents can affect all providers simultaneously. Tagging endpoints by region allows policy constraints such as preferring local region first but failing over cross-region when health degrades.\n\nCircuit-breaking patterns can protect users during severe incidents. If an endpoint crosses error thresholds, mark it temporarily degraded 和 avoid selecting it 用于 a cooling period. Deterministic simulations can model this behavior without real network calls.\n\nObservability ties it together. Endpoint decisions should emit reasoning strings or structured fields so operators can inspect why a node was chosen. This is especially useful when users report intermittent failures.\n\nIn many systems, endpoint policy 和 retry policy are separate 模块. Keep interfaces clean: selection chooses target endpoint, retry schedule defines attempts 和 delays, metrics reducer evaluates outcomes. This separation improves testability 和 change safety.\n\nFinally, avoid hidden randomness in core selection logic. Randomized tie-breakers may seem harmless but they complicate reproducibility 和 debugging. Deterministic order supports reliable incident analysis.\n\n\nThis material should be operationalized 使用 deterministic fixtures 和 explicit release criteria. Teams should preserve a small set of baseline scenarios that represent normal traffic, moderate stress, 和 severe stress. 用于 each scenario, compare policy outputs before 和 after changes, 和 require review notes when confidence labels, warnings, or recommendations move in a meaningful way. This discipline prevents accidental drift, keeps support playbooks aligned 使用 runtime behavior, 和 makes incident response faster because everyone shares the same deterministic artifact language. In practice, the strongest reliability teams treat these artifacts as release gates, not optional documentation, 和 they keep fixture ownership explicit so updates remain intentional 和 auditable.\n\n## Checklist\n- Score endpoints using multiple reliability signals.\n- Use deterministic tie-breaking to avoid flapping.\n- Apply rate-limit-aware traffic shifting.\n- Keep fallback 和 retry policy responsibilities separate.\n- Emit endpoint reasoning 用于 operational debugging.\n",
            "duration": "55 min"
          },
          "rpc-v2-retry-explorer": {
            "title": "Explorer: retry/backoff simulator",
            "content": "# Explorer: retry/backoff simulator\n\nRetry 和 backoff policies determine whether clients recover gracefully or amplify outages. A simulator should make schedule behavior explicit so teams can reason about user latency 和 provider pressure. This 课时 builds a deterministic view of retry policy outputs 和 their tradeoffs.\n\nA retry schedule has three core dimensions: number of attempts, per-attempt timeout, 和 delay before each retry. Exponential backoff grows delay rapidly 和 reduces pressure in prolonged incidents. Linear backoff grows slower 和 can be useful 用于 short-lived blips. Both need max-delay caps to avoid runaway wait times.\n\nThe first attempt should always be represented in the schedule 使用 zero delay. This improves traceability 和 ensures telemetry can map attempt index to behavior consistently. Many teams model only retries 和 lose visibility into full request lifecycle.\n\nPolicy inputs should be validated. Negative retries or non-positive timeouts are configuration errors 和 should fail fast. Deterministic validation in a pure function prevents silent misconfiguration in production.\n\nThe simulator should also show expected user-facing latency envelope. 用于 example, timeout 900ms 使用 two retries 和 exponential delays of 100ms 和 200ms implies worst-case response around 2.9 seconds before failover completion. This helps product teams set realistic loading copy.\n\nRetry policy must integrate 使用 endpoint selection. Retrying against the same degraded endpoint repeatedly is usually inferior to endpoint-aware retries. Even if your simulator keeps 模块 separate, it should explain this interaction.\n\nJitter is often used in distributed systems to prevent synchronization spikes. In this deterministic 课程 we omit jitter from challenge outputs 用于 snapshot stability, but teams should understand where jitter fits in production.\n\nMetrics reducers provide feedback loop 用于 tuning. If p95 improves but error count rises, policy may be too aggressive. If errors drop but latency explodes, policy may be too conservative. Deterministic histogram 和 quantile outputs make this tradeoff visible.\n\nA final best practice is policy versioning. When retry settings change, compare outputs 用于 fixture scenarios before 部署. This catches accidental behavior changes 和 enables confident rollbacks.\n\nOperational readiness also requires a habit of refreshing fixture sets as provider behavior evolves. Rate-limit patterns, slot lag profiles, 和 latency distributions change over time, 和 static fixtures can hide policy regressions. Reliability teams should schedule periodic fixture audits, compare score deltas across providers, 和 document threshold changes so retry 和 selection policies remain explainable 和 reproducible under current network conditions.\n\n\nThis material should be operationalized 使用 deterministic fixtures 和 explicit release criteria. Teams should preserve a small set of baseline scenarios that represent normal traffic, moderate stress, 和 severe stress. 用于 each scenario, compare policy outputs before 和 after changes, 和 require review notes when confidence labels, warnings, or recommendations move in a meaningful way. This discipline prevents accidental drift, keeps support playbooks aligned 使用 runtime behavior, 和 makes incident response faster because everyone shares the same deterministic artifact language. In practice, the strongest reliability teams treat these artifacts as release gates, not optional documentation, 和 they keep fixture ownership explicit so updates remain intentional 和 auditable.\n\n## Checklist\n- Represent full schedule including initial attempt.\n- Validate retry configuration inputs strictly.\n- Bound delays 使用 max caps.\n- Estimate user-facing worst-case latency from schedule.\n- Review policy changes against deterministic fixtures.\n",
            "duration": "50 min"
          }
        }
      },
      "rpc-v2-project-journey": {
        "title": "RPC Multi-Provider Client Project Journey",
        "description": "Build deterministic policy engines 用于 routing, retries, metrics reduction, 和 health report exports.",
        "lessons": {
          "rpc-v2-rpc-policy": {
            "title": "Challenge: implement rpcPolicy()",
            "content": "Build deterministic timeout 和 retry schedule outputs from policy input.",
            "duration": "40 min"
          },
          "rpc-v2-select-endpoint": {
            "title": "Challenge: implement selectRpcEndpoint()",
            "content": "Choose the best endpoint deterministically from health samples 和 endpoint metadata.",
            "duration": "40 min"
          },
          "rpc-v2-cache-invalidation-policy": {
            "title": "Challenge: caching 和 invalidation policy",
            "content": "Emit deterministic cache invalidation actions when 账户 updates 和 lag signals arrive.",
            "duration": "35 min"
          },
          "rpc-v2-metrics-reducer": {
            "title": "Challenge: metrics reducer 和 histogram buckets",
            "content": "Reduce simulated RPC events into deterministic histogram 和 p50/p95 metrics.",
            "duration": "35 min"
          },
          "rpc-v2-health-report-checkpoint": {
            "title": "Checkpoint: RPC health report export",
            "content": "Export deterministic JSON 和 markdown health report artifacts 用于 multi-provider reliability review.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "rust-data-layout-borsh": {
    "title": "Rust Data Layout & Borsh Mastery",
    "description": "Rust-first Solana data layout engineering 使用 deterministic byte-level tooling 和 compatibility-safe schema practices.",
    "duration": "10 hours",
    "tags": [
      "rust",
      "borsh",
      "data-layout",
      "solana"
    ],
    "modules": {
      "rdb-v2-foundations": {
        "title": "Data Layout Foundations",
        "description": "Alignment behavior, Borsh encoding rules, 和 实战 parsing safety 用于 stable byte-level contracts.",
        "lessons": {
          "rdb-v2-layout-alignment-padding": {
            "title": "Memory layout: alignment, padding, 和 why Solana 账户 care",
            "content": "# Memory layout: alignment, padding, 和 why Solana 账户 care\n\nRust layout behavior is deterministic inside one compiled binary but can vary when assumptions are implicit. 用于 Solana 账户, this matters because raw bytes are persisted on-chain 和 parsed by multiple clients across versions. If you 设计 账户 structures without explicit layout strategy, subtle padding 和 alignment changes can break compatibility or produce incorrect parsing in downstream tools.\n\nRust default layout optimizes 用于 compiler freedom. Field order in memory 用于 plain structs is not a stable ABI contract unless you opt into representations such as repr(C). In low-level 账户 work, repr(C) gives more predictable ordering 和 alignment behavior, but it does not remove all complexity. Padding still appears between fields when alignment requires it. 用于 example, a u8 followed by u64 introduces 7 bytes of padding before the u64 offset. If your parser ignores this, every field after that point is shifted 和 corrupted.\n\nOn Solana, 账户 rent is proportional to byte size, so padding is not only a correctness issue; it is a cost issue. Poor field ordering can inflate 账户 sizes across millions of 账户. A common optimization is grouping larger aligned fields first, then smaller fields. But this must be balanced against readability 和 migration safety. If you reorder fields in a live protocol, old data may no longer parse under new assumptions. Migration tooling should be explicit 和 versioned.\n\nBorsh serialization avoids some ABI ambiguity by defining field order in schema rather than raw struct memory. However, zero-copy patterns 和 manual slicing still depend on precise offsets. Teams should understand both worlds: in-memory layout rules 用于 zero-copy 和 schema-based encoding rules 用于 Borsh.\n\nIn production engineering, layout decisions should be documented 使用 deterministic outputs: field offsets, per-field padding, struct alignment, 和 total size. These outputs can be compared in CI to catch accidental drift from refactors. The goal is not theoretical elegance; the goal is stable data contracts over time.\n\n## Operator mindset\n\nSchema bytes are production API surface. Treat offset changes, enum ordering, 和 parser semantics as compatibility events requiring explicit review.\n\nProduction teams should treat layout 和 serialization contracts as long-lived APIs. Any change to field order, enum variant index, or alignment assumptions can break deployed clients, indexers, or migration scripts. A safe process is to version schemas, ship fixture updates, 和 require deterministic regression outputs before release. Reviewers should compare expected byte offsets, expected encoded bytes, 和 parser error behavior 用于 malformed inputs. If one field widens from u32 to u64, the review should explicitly call out downstream effects on 账户 size, rent budget, 和 compatibility. Deterministic helpers make this 实战: you can produce a stable JSON report in CI 和 diff it like source code. In Solana 和 Anchor contexts, this discipline prevents subtle data corruption bugs that are expensive to diagnose after 部署.\n\nAnother operational rule is to keep parser failures structured. A generic \"decode failed\" message is not enough 用于 incident response. Good error payloads include field name, offset, 和 failure category such as out-of-bounds, invalid bool byte, or unsupported dynamic shape. This is especially important 用于 indexers 和 analytics pipelines that need to decide whether to quarantine an event or retry 使用 a newer schema version. Teams that encode rich deterministic error reports reduce triage time 和 avoid accidental data loss. Over time, this becomes part of reliability culture: parse strict, report clearly, 和 test every boundary condition before shipping.\n\nTeams should also document explicit schema 治理 rules. If a field type changes, reviewers should verify migration strategy, historical replay impact, 和 compatibility 使用 archived reports. A healthy 治理 checklist asks who owns schema evolution, how compatibility windows are communicated, 和 which fixtures are mandatory before release. This level of process may feel heavy 用于 small projects, but it is exactly what prevents costly corruption incidents at scale. Deterministic byte-level artifacts are the 实战 mechanism that keeps this 治理 lightweight enough to use: they are simple to diff, easy to discuss, 和 difficult to misinterpret.\n",
            "duration": "55 min"
          },
          "rdb-v2-borsh-enums-vectors-strings": {
            "title": "Struct 和 enum layout pitfalls plus Borsh rules",
            "content": "# Struct 和 enum layout pitfalls plus Borsh rules\n\nBorsh is widely used because it gives deterministic serialization across languages, but teams still get tripped up by how enums, vectors, 和 strings map to bytes. Understanding these rules is essential 用于 robust 账户 parsing 和 client interoperability.\n\n用于 structs, Borsh encodes fields in declaration order. There is no implicit alignment padding in the serialized stream. That is different from in-memory layout 和 one reason Borsh is popular 用于 stable wire formats. 用于 enums, Borsh writes a one-byte variant index first, then the variant payload. Changing variant order in code changes the index mapping 和 is therefore a breaking format change. This is a common source of accidental incompatibility.\n\nVectors 和 strings are length-prefixed 使用 little-endian u32 before data bytes. If parsing code trusts the length blindly without bounds checks, malformed or truncated data can cause out-of-bounds reads or allocation abuse. Safe parsers validate available bytes before allocating or slicing.\n\nAnother pitfall is conflating pubkey strings 使用 pubkey bytes. Borsh encodes bytes, not base58 text. If a client serializes public keys as strings while another expects 32-byte arrays, decoding fails despite both sides using \"Borsh\" terminology. Teams should define schema types precisely.\n\nError 设计 is part of serialization safety. Distinguish malformed length prefix, unknown enum variant, unsupported dynamic type, 和 primitive decode out-of-bounds. Structured errors let callers decide whether to retry, drop, or quarantine payloads.\n\nFinally, encoding 和 decoding tests should run symmetrically 使用 fixed fixtures. A deterministic fixture suite catches regressions early 和 gives confidence that Rust, TypeScript, 和 analytics parsers agree on the same bytes.\nProduction teams should treat layout 和 serialization contracts as long-lived APIs. Any change to field order, enum variant index, or alignment assumptions can break deployed clients, indexers, or migration scripts. A safe process is to version schemas, ship fixture updates, 和 require deterministic regression outputs before release. Reviewers should compare expected byte offsets, expected encoded bytes, 和 parser error behavior 用于 malformed inputs. If one field widens from u32 to u64, the review should explicitly call out downstream effects on 账户 size, rent budget, 和 compatibility. Deterministic helpers make this 实战: you can produce a stable JSON report in CI 和 diff it like source code. In Solana 和 Anchor contexts, this discipline prevents subtle data corruption bugs that are expensive to diagnose after 部署.\n\nAnother operational rule is to keep parser failures structured. A generic \"decode failed\" message is not enough 用于 incident response. Good error payloads include field name, offset, 和 failure category such as out-of-bounds, invalid bool byte, or unsupported dynamic shape. This is especially important 用于 indexers 和 analytics pipelines that need to decide whether to quarantine an event or retry 使用 a newer schema version. Teams that encode rich deterministic error reports reduce triage time 和 avoid accidental data loss. Over time, this becomes part of reliability culture: parse strict, report clearly, 和 test every boundary condition before shipping.\n\nTeams should also document explicit schema 治理 rules. If a field type changes, reviewers should verify migration strategy, historical replay impact, 和 compatibility 使用 archived reports. A healthy 治理 checklist asks who owns schema evolution, how compatibility windows are communicated, 和 which fixtures are mandatory before release. This level of process may feel heavy 用于 small projects, but it is exactly what prevents costly corruption incidents at scale. Deterministic byte-level artifacts are the 实战 mechanism that keeps this 治理 lightweight enough to use: they are simple to diff, easy to discuss, 和 difficult to misinterpret.\n",
            "duration": "55 min"
          },
          "rdb-v2-layout-visualizer": {
            "title": "Explorer: layout visualizer 用于 field offsets",
            "content": "# Explorer: layout visualizer 用于 field offsets\n\nA layout visualizer turns abstract alignment rules into concrete numbers engineers can review. Instead of debating whether a struct is \"probably fine,\" teams can inspect exact offsets, padding, 和 total size.\n\nThe visualizer workflow is straightforward: provide ordered fields 和 types, compute alignments, insert required padding, 和 emit final layout metadata. This output should be deterministic 和 serializable so CI can compare snapshots.\n\nWhen using this in Solana development, combine visualizer output 使用 账户 rent planning 和 migration docs. If a proposed field addition increases total size, quantify the impact 和 decide whether to append, split 账户 state, or introduce versioned 账户. Do not rely on intuition 用于 byte-level decisions.\n\nVisualizers are also useful 用于 onboarding. New contributors can quickly see why u8/u64 ordering changes offsets 和 why safe parsers need explicit bounds checks. This reduces recurring parsing bugs 和 review churn.\n\nA high-quality visualizer report includes field name, offset, size, alignment, padding-before, trailing padding, 和 struct alignment. Keep key ordering stable so report diffs remain readable.\n\nEngineers should pair visualizer output 使用 parse tests. If layout says a bool lives at offset 0 和 u8 at offset 1, parser tests should assert exactly that. Deterministic systems connect 设计 artifacts 和 runtime checks.\nProduction teams should treat layout 和 serialization contracts as long-lived APIs. Any change to field order, enum variant index, or alignment assumptions can break deployed clients, indexers, or migration scripts. A safe process is to version schemas, ship fixture updates, 和 require deterministic regression outputs before release. Reviewers should compare expected byte offsets, expected encoded bytes, 和 parser error behavior 用于 malformed inputs. If one field widens from u32 to u64, the review should explicitly call out downstream effects on 账户 size, rent budget, 和 compatibility. Deterministic helpers make this 实战: you can produce a stable JSON report in CI 和 diff it like source code. In Solana 和 Anchor contexts, this discipline prevents subtle data corruption bugs that are expensive to diagnose after 部署.\n\nAnother operational rule is to keep parser failures structured. A generic \"decode failed\" message is not enough 用于 incident response. Good error payloads include field name, offset, 和 failure category such as out-of-bounds, invalid bool byte, or unsupported dynamic shape. This is especially important 用于 indexers 和 analytics pipelines that need to decide whether to quarantine an event or retry 使用 a newer schema version. Teams that encode rich deterministic error reports reduce triage time 和 avoid accidental data loss. Over time, this becomes part of reliability culture: parse strict, report clearly, 和 test every boundary condition before shipping.\n\nTeams should also document explicit schema 治理 rules. If a field type changes, reviewers should verify migration strategy, historical replay impact, 和 compatibility 使用 archived reports. A healthy 治理 checklist asks who owns schema evolution, how compatibility windows are communicated, 和 which fixtures are mandatory before release. This level of process may feel heavy 用于 small projects, but it is exactly what prevents costly corruption incidents at scale. Deterministic byte-level artifacts are the 实战 mechanism that keeps this 治理 lightweight enough to use: they are simple to diff, easy to discuss, 和 difficult to misinterpret.\n",
            "duration": "50 min"
          }
        }
      },
      "rdb-v2-project-journey": {
        "title": "账户 Layout Inspector Project Journey",
        "description": "Implement deterministic layout analysis, encoding/decoding, safe parsing, 和 compatibility-focused reporting helpers.",
        "lessons": {
          "rdb-v2-compute-layout": {
            "title": "Challenge: implement computeLayout()",
            "content": "Compute deterministic field offsets, alignment padding, 和 total struct size.",
            "duration": "40 min"
          },
          "rdb-v2-borsh-encode-decode": {
            "title": "Challenge: implement borshEncode/borshDecode helpers",
            "content": "Implement deterministic Borsh encode/decode 使用 structured error handling.",
            "duration": "40 min"
          },
          "rdb-v2-zero-copy-tradeoffs": {
            "title": "Challenge: zero-copy vs Borsh tradeoff model",
            "content": "Model deterministic tradeoff scoring between zero-copy 和 Borsh approaches.",
            "duration": "35 min"
          },
          "rdb-v2-safe-parse-account-data": {
            "title": "Challenge: implement safeParseAccountData()",
            "content": "Parse 账户 bytes 使用 deterministic bounds checks 和 structured errors.",
            "duration": "35 min"
          },
          "rdb-v2-layout-report-checkpoint": {
            "title": "Checkpoint: stable layout report",
            "content": "Produce stable JSON 和 markdown layout artifacts 用于 the final project.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "rust-errors-invariants": {
    "title": "Rust Error 设计 & Invariants",
    "description": "Build typed invariant guard libraries 使用 deterministic evidence artifacts, compatibility-safe error contracts, 和 audit-ready reporting.",
    "duration": "10 hours",
    "tags": [
      "rust",
      "errors",
      "invariants",
      "reliability"
    ],
    "modules": {
      "rei-v2-foundations": {
        "title": "Rust Error 和 Invariant Foundations",
        "description": "Typed error taxonomy, Result/context propagation patterns, 和 deterministic invariant 设计 fundamentals.",
        "lessons": {
          "rei-v2-error-taxonomy": {
            "title": "Error taxonomy: recoverable vs fatal",
            "content": "# Error taxonomy: recoverable vs fatal\n\nRust encourages explicit error modeling, but teams still produce weak error contracts when they rely on ad hoc strings or inconsistent wrappers. In Solana 和 Anchor-adjacent systems, this becomes painful quickly because on-chain failures, off-chain pipelines, 和 frontend UX all need coherent semantics.\n\nA 实战 taxonomy starts 使用 recoverable versus fatal classes. Recoverable errors represent expected contract violations: stale data, missing signer, value out of range, or transient dependency mismatch. Fatal errors represent corrupted assumptions: impossible state, incompatible schema version, or invariant breach that requires operator intervention.\n\nTyped enums are the center of this 设计. A code such as NEGATIVE_VALUE or MISSING_AUTHORITY is unambiguous 和 searchable. Attaching structured context fields gives downstream systems enough detail 用于 logging 和 user-facing copy without string parsing.\n\nAvoid stringly error contracts where every caller invents custom messages. Those systems accumulate inconsistent wording 和 ambiguous categories. Instead, keep messages deterministic 和 derive user copy from code + context in one mapping layer.\n\nInvariants should be designed 用于 testability. If an invariant cannot be expressed as a deterministic function over known inputs, it is hard to validate 和 easy to regress. Start 使用 small ensure helpers that return typed results, then compose them into higher-level guards.\n\nIn production, error taxonomies should be reviewed like API changes. Renaming codes or changing severity mapping can break alert rules 和 client handling. Version these changes 和 validate 使用 fixture suites.\n\n## Operator mindset\n\nInvariant errors are operational contracts. If code, severity, 和 context are not stable, monitoring 和 user recovery flows degrade even when logic is correct.\n\nProduction reliability work depends on deterministic error behavior. Teams should agree on typed error codes, stable context fields, 和 explicit severity mapping so runtime incidents are diagnosable without guessing. 用于 invariants, each failed check should identify what contract was violated, where in the flow it happened, 和 whether the failure is recoverable. If one subsystem emits free-form strings while another emits numeric codes, dashboards become inconsistent 和 alert tuning becomes fragile. A typed error library 使用 deterministic reports solves this by making failure semantics machine-readable 和 human-readable at the same time.\n\nEvidence chains are equally important. A report that says \"failed\" without chronological context has limited value. A deterministic chain 使用 injected timestamps 和 step IDs gives auditors 和 engineers a replayable explanation of what passed, what failed, 和 in which order. This is especially useful when protocol upgrades adjust invariant rules: reviewers can diff old 和 new evidence outputs 和 verify expected changes before 部署. Over time, these deterministic artifacts become part of release discipline 和 reduce regressions caused by informal error handling.\n\nWhen error contracts evolve, teams should run compatibility drills. These drills intentionally replay older fixture sets against newer error libraries 和 confirm that alerts, dashboards, 和 user-facing copy still map correctly. If mappings drift, update guides 和 fallback behavior should ship together 使用 code changes. This avoids the common failure mode where backend semantics change but frontend messaging lags behind, confusing users 和 support teams. Deterministic reports are a force multiplier here because they make drift visible immediately instead of after production incidents.\n\nSustained quality also requires explicit ownership of invariant catalogs. Every invariant should have a named owner, a rationale, 和 a linked test fixture. When teams cannot answer why an invariant exists, they often remove it during refactors 和 reintroduce old classes of failures. A lightweight ownership table prevents this. Pair it 使用 quarterly reviews where engineers evaluate false-positive rates, update context fields, 和 verify UX mappings remain actionable. During incidents, this preparation pays off: responders can identify which invariant tripped, understand expected remediation, 和 communicate clearly to users. Deterministic evidence artifacts make postmortems faster because the same chain can be replayed exactly across environments.\n",
            "duration": "55 min"
          },
          "rei-v2-result-context-patterns": {
            "title": "Result<T, E> patterns, ? operator, 和 context",
            "content": "# Result<T, E> patterns, ? operator, 和 context\n\nResult-based control flow is one of Rust's strongest tools 用于 building robust services 和 on-chain-adjacent clients. The key is not merely using Result, but designing error types 和 propagation boundaries that preserve enough context 用于 debugging 和 UX decisions.\n\nThe ? operator keeps code concise, but it can hide context unless error conversion layers are explicit. Invariant-centric systems should wrap lower-level failures 使用 domain meaning before returning to upper layers. 用于 example, a parse failure in 账户 metadata should map to a deterministic invariant code 和 include the field path.\n\nContext should be structured rather than baked into message text. A map of key/value fields like {label, value, limit} is easier to aggregate 和 filter than sentence fragments. It also supports localization 和 role-specific message rendering.\n\nAnother pattern is separating validation from side effects. If ensure helpers only evaluate conditions 和 construct typed errors, they are deterministic 和 unit-testable. Side effects such as logging or telemetry emission can happen at call boundaries.\n\nWhen building libraries, avoid exposing too many internal codes. Public codes should represent stable contracts, while internal details can remain nested context. This helps keep compatibility manageable.\n\nTest strategy should include positive cases, negative cases, 和 report formatting checks. Deterministic report output is valuable 用于 code review because changes are visible as stable diffs, not only behavioral assertions.\nProduction reliability work depends on deterministic error behavior. Teams should agree on typed error codes, stable context fields, 和 explicit severity mapping so runtime incidents are diagnosable without guessing. 用于 invariants, each failed check should identify what contract was violated, where in the flow it happened, 和 whether the failure is recoverable. If one subsystem emits free-form strings while another emits numeric codes, dashboards become inconsistent 和 alert tuning becomes fragile. A typed error library 使用 deterministic reports solves this by making failure semantics machine-readable 和 human-readable at the same time.\n\nEvidence chains are equally important. A report that says \"failed\" without chronological context has limited value. A deterministic chain 使用 injected timestamps 和 step IDs gives auditors 和 engineers a replayable explanation of what passed, what failed, 和 in which order. This is especially useful when protocol upgrades adjust invariant rules: reviewers can diff old 和 new evidence outputs 和 verify expected changes before 部署. Over time, these deterministic artifacts become part of release discipline 和 reduce regressions caused by informal error handling.\n\nWhen error contracts evolve, teams should run compatibility drills. These drills intentionally replay older fixture sets against newer error libraries 和 confirm that alerts, dashboards, 和 user-facing copy still map correctly. If mappings drift, update guides 和 fallback behavior should ship together 使用 code changes. This avoids the common failure mode where backend semantics change but frontend messaging lags behind, confusing users 和 support teams. Deterministic reports are a force multiplier here because they make drift visible immediately instead of after production incidents.\n\nSustained quality also requires explicit ownership of invariant catalogs. Every invariant should have a named owner, a rationale, 和 a linked test fixture. When teams cannot answer why an invariant exists, they often remove it during refactors 和 reintroduce old classes of failures. A lightweight ownership table prevents this. Pair it 使用 quarterly reviews where engineers evaluate false-positive rates, update context fields, 和 verify UX mappings remain actionable. During incidents, this preparation pays off: responders can identify which invariant tripped, understand expected remediation, 和 communicate clearly to users. Deterministic evidence artifacts make postmortems faster because the same chain can be replayed exactly across environments.\n",
            "duration": "55 min"
          },
          "rei-v2-invariant-decision-tree": {
            "title": "Explorer: invariant decision tree",
            "content": "# Explorer: invariant decision tree\n\nAn invariant decision tree helps teams reason about guard ordering 和 failure priority. Not every invariant should be checked in arbitrary order. Early checks should prevent expensive work 和 produce the clearest failure semantics.\n\nA common flow: structural preconditions first, authority checks second, value bounds third, relational checks fourth. This ordering minimizes noisy failures 和 improves auditability. If authority is missing, there is little value in evaluating downstream value checks.\n\nDecision trees also help map errors to UX behavior. A recoverable user input violation may show inline correction hints, while a fatal integrity breach should hard-stop 使用 escalation messaging.\n\nIn deterministic systems, tree traversal should be explicit 和 testable. Given the same input, the same failing node should be reported every time. This allows stable evidence chains 和 reliable automation.\n\nExplorer tooling can visualize this by showing the path taken, checks skipped, 和 final outcome. Teams can then tune guard order intentionally 和 document rationale.\nProduction reliability work depends on deterministic error behavior. Teams should agree on typed error codes, stable context fields, 和 explicit severity mapping so runtime incidents are diagnosable without guessing. 用于 invariants, each failed check should identify what contract was violated, where in the flow it happened, 和 whether the failure is recoverable. If one subsystem emits free-form strings while another emits numeric codes, dashboards become inconsistent 和 alert tuning becomes fragile. A typed error library 使用 deterministic reports solves this by making failure semantics machine-readable 和 human-readable at the same time.\n\nEvidence chains are equally important. A report that says \"failed\" without chronological context has limited value. A deterministic chain 使用 injected timestamps 和 step IDs gives auditors 和 engineers a replayable explanation of what passed, what failed, 和 in which order. This is especially useful when protocol upgrades adjust invariant rules: reviewers can diff old 和 new evidence outputs 和 verify expected changes before 部署. Over time, these deterministic artifacts become part of release discipline 和 reduce regressions caused by informal error handling.\n\nWhen error contracts evolve, teams should run compatibility drills. These drills intentionally replay older fixture sets against newer error libraries 和 confirm that alerts, dashboards, 和 user-facing copy still map correctly. If mappings drift, update guides 和 fallback behavior should ship together 使用 code changes. This avoids the common failure mode where backend semantics change but frontend messaging lags behind, confusing users 和 support teams. Deterministic reports are a force multiplier here because they make drift visible immediately instead of after production incidents.\n\nSustained quality also requires explicit ownership of invariant catalogs. Every invariant should have a named owner, a rationale, 和 a linked test fixture. When teams cannot answer why an invariant exists, they often remove it during refactors 和 reintroduce old classes of failures. A lightweight ownership table prevents this. Pair it 使用 quarterly reviews where engineers evaluate false-positive rates, update context fields, 和 verify UX mappings remain actionable. During incidents, this preparation pays off: responders can identify which invariant tripped, understand expected remediation, 和 communicate clearly to users. Deterministic evidence artifacts make postmortems faster because the same chain can be replayed exactly across environments.\n",
            "duration": "50 min"
          }
        }
      },
      "rei-v2-project-journey": {
        "title": "Invariant Guard Library Project Journey",
        "description": "Implement guard helpers, evidence-chain generation, 和 stable audit reporting 用于 reliability 和 incident response.",
        "lessons": {
          "rei-v2-invariant-error-helpers": {
            "title": "Challenge: implement InvariantError + ensure helpers",
            "content": "Implement typed invariant errors 和 deterministic ensure helpers.",
            "duration": "40 min"
          },
          "rei-v2-evidence-chain-builder": {
            "title": "Challenge: implement deterministic EvidenceChain",
            "content": "Build a deterministic evidence chain 使用 injected timestamps.",
            "duration": "40 min"
          },
          "rei-v2-property-ish-invariant-tests": {
            "title": "Challenge: deterministic invariant case runner",
            "content": "Run deterministic invariant case sets 和 return failed IDs.",
            "duration": "35 min"
          },
          "rei-v2-format-report": {
            "title": "Challenge: implement formatReport() stable markdown",
            "content": "Format a deterministic markdown evidence report.",
            "duration": "35 min"
          },
          "rei-v2-invariant-audit-checkpoint": {
            "title": "Checkpoint: invariant audit report",
            "content": "Export deterministic invariant audit checkpoint artifacts.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "rust-perf-onchain-thinking": {
    "title": "Rust 性能 用于 On-chain Thinking",
    "description": "Simulate 和 optimize compute-cost behavior 使用 deterministic Rust-first tooling 和 budget-driven 性能 治理.",
    "duration": "10 hours",
    "tags": [
      "rust",
      "performance",
      "compute",
      "solana"
    ],
    "modules": {
      "rpot-v2-foundations": {
        "title": "性能 Foundations",
        "description": "Rust 性能 思维模型s, data-structure tradeoffs, 和 deterministic cost reasoning 用于 reliable optimization decisions.",
        "lessons": {
          "rpot-v2-perf-mental-model": {
            "title": "性能 思维模型: allocations, clones, hashing",
            "content": "# 性能 思维模型: allocations, clones, hashing\n\nRust 性能 work in Solana ecosystems is mostly about data movement discipline. Teams often chase micro-optimizations while ignoring dominant costs such as repeated allocations, unnecessary cloning, 和 redundant hashing in loops.\n\nA useful 思维模型 starts 使用 cost buckets. Allocation cost includes heap growth, allocator metadata, 和 cache disruption. Clone cost depends on object size 和 ownership patterns. Hash cost depends on bytes hashed 和 hash invocation frequency. Loop cost depends on iteration count 和 per-iteration work. Map lookup cost depends on data structure choice 和 access pattern.\n\nThe point of this model is not exact runtime cycles. The point is relative pressure. If one path performs ten allocations 和 another performs one allocation, the former should trigger scrutiny even before microbenchmarking.\n\nOn-chain thinking reinforces this: compute budgets are finite, 和 predictable resource usage matters. Even off-chain indexers 和 simulators benefit from the same discipline because latency tails 和 CPU burn impact reliability.\n\nDeterministic models are ideal 用于 CI. Given identical operation counts, output should be identical. Reviewers can reason about deltas directly 和 reject regressions early.\n\n## Operator mindset\n\n性能 guidance should be versioned 和 budgeted. Without explicit budgets 和 stable cost categories, optimization work drifts toward anecdote instead of measurable outcomes.\n\n性能 engineering 用于 on-chain-adjacent Rust systems should be deterministic by default. Timing benchmarks are useful but noisy across machines 和 CI runners. A stable cost model that converts operation counts into weighted costs gives teams a consistent baseline 用于 regression detection. The model does not replace real profiling; it complements it by making early 设计 tradeoffs explicit 和 reviewable.\n\nWhen you model costs, keep weights documented 和 intentionally conservative. If allocations are expensive in your environment, give them a higher coefficient 和 track reductions across releases. If map lookups dominate hot loops, surface that as a recommendation category. Stable reports 使用 before/after breakdowns let reviewers validate that claimed optimizations actually reduce modeled cost instead of merely shifting work.\n\nSerialization churn is another hidden cost center. Repeated encode/decode cycles inside loops often produce avoidable overhead in indexers 和 client-side simulation tools. Deterministic byte-count models are an effective teaching tool because they make waste visible without requiring instrumentation overhead. Combined 使用 suggestion outputs 和 checkpoint reports, these models become 实战 guardrails 用于 engineering quality.\n\nMature teams combine these deterministic models 使用 periodic empirical profiling to recalibrate weights. If production traces show map lookups dominating more than expected, adjust coefficients 和 rerun fixture suites so optimization priorities stay realistic. This prevents model stagnation 和 keeps recommendations aligned 使用 actual system behavior. The key is to treat model updates as versioned changes 使用 explicit reasoning, not ad hoc tweaks. Deterministic reports then provide historical continuity, letting teams explain why 性能 guidance changed 和 how improvements were verified.\n\nTeams should also define 性能 budgets per workflow rather than relying only on aggregate totals. A route-planning path may tolerate moderate hashing cost but strict allocation limits, while a reporting path may prioritize serialization efficiency. Budgeted categories make optimization goals concrete 和 avoid endless debates about which metric matters most. In release reviews, compare modeled costs against these budgets 和 require explicit waivers when thresholds are exceeded. Keep waiver text deterministic 和 tracked in artifacts so exceptions do not become silent defaults. Over time, this process builds a reliable 性能 culture where improvements are intentional, measurable, 和 easy to audit.\n",
            "duration": "55 min"
          },
          "rpot-v2-data-structure-tradeoffs": {
            "title": "Data structures: Vec, HashMap, BTreeMap tradeoffs",
            "content": "# Data structures: Vec, HashMap, BTreeMap tradeoffs\n\nData structure choice is one of the highest leverage 性能 decisions in Rust systems. Vec offers compact contiguous storage 和 predictable iteration speed. HashMap offers average-case fast lookup but can have higher allocation 和 hashing overhead. BTreeMap provides ordered keys 和 stable traversal costs 使用 different memory locality characteristics.\n\nIn on-chain-adjacent simulations 和 indexers, workloads vary. If you mostly append 和 iterate, Vec plus binary search or index maps can outperform heavier maps. If random key lookup dominates, HashMap may win despite hash overhead. If deterministic ordering is required 用于 report output or canonical snapshots, BTreeMap can simplify stable behavior.\n\nThe wrong pattern is premature abstraction that hides access patterns. Engineers should instrument operation counts 和 use cost models to evaluate actual use cases. Deterministic benchmark fixtures make this reproducible.\n\nAnother 实战 tradeoff is allocation strategy. Reusing buffers 和 reserving capacity can reduce churn substantially. This is often more impactful than iterator-vs-loop debates.\n\nKeep 设计 reviews concrete: expected reads, writes, key cardinality, ordering requirements, 和 mutation frequency. Then choose structures intentionally 和 document rationale.\n性能 engineering 用于 on-chain-adjacent Rust systems should be deterministic by default. Timing benchmarks are useful but noisy across machines 和 CI runners. A stable cost model that converts operation counts into weighted costs gives teams a consistent baseline 用于 regression detection. The model does not replace real profiling; it complements it by making early 设计 tradeoffs explicit 和 reviewable.\n\nWhen you model costs, keep weights documented 和 intentionally conservative. If allocations are expensive in your environment, give them a higher coefficient 和 track reductions across releases. If map lookups dominate hot loops, surface that as a recommendation category. Stable reports 使用 before/after breakdowns let reviewers validate that claimed optimizations actually reduce modeled cost instead of merely shifting work.\n\nSerialization churn is another hidden cost center. Repeated encode/decode cycles inside loops often produce avoidable overhead in indexers 和 client-side simulation tools. Deterministic byte-count models are an effective teaching tool because they make waste visible without requiring instrumentation overhead. Combined 使用 suggestion outputs 和 checkpoint reports, these models become 实战 guardrails 用于 engineering quality.\n\nMature teams combine these deterministic models 使用 periodic empirical profiling to recalibrate weights. If production traces show map lookups dominating more than expected, adjust coefficients 和 rerun fixture suites so optimization priorities stay realistic. This prevents model stagnation 和 keeps recommendations aligned 使用 actual system behavior. The key is to treat model updates as versioned changes 使用 explicit reasoning, not ad hoc tweaks. Deterministic reports then provide historical continuity, letting teams explain why 性能 guidance changed 和 how improvements were verified.\n\nTeams should also define 性能 budgets per workflow rather than relying only on aggregate totals. A route-planning path may tolerate moderate hashing cost but strict allocation limits, while a reporting path may prioritize serialization efficiency. Budgeted categories make optimization goals concrete 和 avoid endless debates about which metric matters most. In release reviews, compare modeled costs against these budgets 和 require explicit waivers when thresholds are exceeded. Keep waiver text deterministic 和 tracked in artifacts so exceptions do not become silent defaults. Over time, this process builds a reliable 性能 culture where improvements are intentional, measurable, 和 easy to audit.\n",
            "duration": "55 min"
          },
          "rpot-v2-cost-sandbox": {
            "title": "Explorer: cost model sandbox",
            "content": "# Explorer: cost model sandbox\n\nA cost sandbox lets teams test optimization hypotheses without waiting 用于 full benchmark infrastructure. Provide operation counts, compute weighted costs, 和 inspect which buckets dominate total pressure.\n\nThe sandbox should separate baseline 和 optimized inputs so diffs are explicit. If a change claims fewer allocations but increases map lookups sharply, the model should show the net effect. This prevents one-dimensional optimization that regresses other paths.\n\nSuggestion generation should be threshold-based 和 deterministic. 用于 example, if allocation cost exceeds a threshold, recommend pre-allocation 和 buffer reuse. If serialization cost dominates, recommend batching or avoiding repeated decode/encode loops.\n\nStable report outputs are critical 用于 engineering workflows. JSON payloads feed CI checks, markdown summaries support code review 和 team communication. Keep key ordering stable so string equality tests remain meaningful.\n\nSandboxes are not production profilers, but they are excellent decision support tools when kept deterministic 和 aligned 使用 known workload patterns.\n性能 engineering 用于 on-chain-adjacent Rust systems should be deterministic by default. Timing benchmarks are useful but noisy across machines 和 CI runners. A stable cost model that converts operation counts into weighted costs gives teams a consistent baseline 用于 regression detection. The model does not replace real profiling; it complements it by making early 设计 tradeoffs explicit 和 reviewable.\n\nWhen you model costs, keep weights documented 和 intentionally conservative. If allocations are expensive in your environment, give them a higher coefficient 和 track reductions across releases. If map lookups dominate hot loops, surface that as a recommendation category. Stable reports 使用 before/after breakdowns let reviewers validate that claimed optimizations actually reduce modeled cost instead of merely shifting work.\n\nSerialization churn is another hidden cost center. Repeated encode/decode cycles inside loops often produce avoidable overhead in indexers 和 client-side simulation tools. Deterministic byte-count models are an effective teaching tool because they make waste visible without requiring instrumentation overhead. Combined 使用 suggestion outputs 和 checkpoint reports, these models become 实战 guardrails 用于 engineering quality.\n\nMature teams combine these deterministic models 使用 periodic empirical profiling to recalibrate weights. If production traces show map lookups dominating more than expected, adjust coefficients 和 rerun fixture suites so optimization priorities stay realistic. This prevents model stagnation 和 keeps recommendations aligned 使用 actual system behavior. The key is to treat model updates as versioned changes 使用 explicit reasoning, not ad hoc tweaks. Deterministic reports then provide historical continuity, letting teams explain why 性能 guidance changed 和 how improvements were verified.\n\nTeams should also define 性能 budgets per workflow rather than relying only on aggregate totals. A route-planning path may tolerate moderate hashing cost but strict allocation limits, while a reporting path may prioritize serialization efficiency. Budgeted categories make optimization goals concrete 和 avoid endless debates about which metric matters most. In release reviews, compare modeled costs against these budgets 和 require explicit waivers when thresholds are exceeded. Keep waiver text deterministic 和 tracked in artifacts so exceptions do not become silent defaults. Over time, this process builds a reliable 性能 culture where improvements are intentional, measurable, 和 easy to audit.\n",
            "duration": "50 min"
          }
        }
      },
      "rpot-v2-project-journey": {
        "title": "Compute Budget Profiler (Sim)",
        "description": "Build deterministic profilers, recommendation engines, 和 report outputs aligned to explicit 性能 budgets.",
        "lessons": {
          "rpot-v2-cost-model-estimate": {
            "title": "Challenge: implement CostModel::estimate()",
            "content": "Estimate deterministic operation costs from fixed weighting rules.",
            "duration": "40 min"
          },
          "rpot-v2-optimize-function-metrics": {
            "title": "Challenge: optimize function metrics",
            "content": "Apply deterministic before/after metric reductions 和 diff outputs.",
            "duration": "40 min"
          },
          "rpot-v2-serialization-costs": {
            "title": "Challenge: model serialization overhead",
            "content": "Compute deterministic serialization overhead 和 byte savings.",
            "duration": "35 min"
          },
          "rpot-v2-suggest-optimizations": {
            "title": "Challenge: implement suggestOptimizations()",
            "content": "Generate stable optimization suggestions from deterministic metrics.",
            "duration": "35 min"
          },
          "rpot-v2-perf-report-checkpoint": {
            "title": "Checkpoint: stable perf report",
            "content": "Export deterministic JSON 和 markdown profiler reports.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "rust-async-indexer-pipeline": {
    "title": "Concurrency & Async 用于 Indexers (Rust)",
    "description": "Rust-first async pipeline engineering 使用 bounded concurrency, replay-safe reducers, 和 deterministic operational reporting.",
    "duration": "10 hours",
    "tags": [
      "rust",
      "async",
      "indexer",
      "pipeline"
    ],
    "modules": {
      "raip-v2-foundations": {
        "title": "Async Pipeline Foundations",
        "description": "Async/concurrency fundamentals, backpressure behavior, 和 deterministic execution modeling 用于 indexer reliability.",
        "lessons": {
          "raip-v2-async-fundamentals": {
            "title": "Async fundamentals: futures, tasks, channels",
            "content": "# Async fundamentals: futures, tasks, channels\n\nRust async systems are built on explicit scheduling rather than implicit thread-per-task models. Futures represent pending work, executors poll futures, 和 channels coordinate data flow. 用于 indexers, this architecture supports high throughput but requires careful control of concurrency 和 backpressure.\n\nA common failure mode is unbounded task spawning. It may look fine in local tests, then collapse in production under burst traffic due to memory pressure 和 queue growth. Defensive 设计 uses bounded concurrency 使用 explicit task budgets.\n\nChannels are powerful but can hide overload when used without capacity limits. Bounded channels make pressure visible: producers block or shed work when consumers lag. In deterministic simulations, this behavior can be modeled by explicit queues 和 tick-based progression.\n\nThe key mindset is reproducibility. If pipeline behavior cannot be replayed deterministically, debugging 和 regression 测试 become guesswork. Simulated executors solve this by removing wall-clock dependence.\n\n## Operator mindset\n\nAsync pipelines are reliability systems, not just throughput systems. Concurrency limits, retry behavior, 和 reducer determinism must stay auditable under stress.\n\nAsync reliability work is strongest when concurrency behavior is testable without wall-clock timing. Real timers 和 threads can introduce nondeterminism that obscures logic bugs. A simulated scheduler 使用 deterministic tick advancement provides a clean environment 用于 validating bounded concurrency, retry sequencing, 和 backpressure behavior. In this model, tasks consume fixed ticks, queues are explicit, 和 completion order is reproducible.\n\nBackpressure 设计 should also be visible in reports. If incoming work exceeds concurrency budget, queues should grow predictably 和 metrics should expose this. Deterministic tests can assert queue length, total ticks, 和 completion order 用于 stress scenarios. This creates confidence that production systems degrade gracefully under load rather than failing unpredictably.\n\nReorg-safe indexing pipelines require idempotency 和 stable reducers. Duplicate deliveries should collapse by key, 和 snapshot reducers should produce canonical state outputs. If reducer output order drifts across runs, diff-based monitoring becomes noisy 和 incident triage slows down. Stable JSON 和 markdown reports prevent that by keeping artifacts comparable between runs 和 between code versions.\n\nOperational teams should maintain scenario catalogs 用于 burst traffic, retry storms, 和 partial-stage failures. Each scenario should specify expected queue depth, retry schedule, 和 final snapshot state. Running these catalogs on every release gives confidence that changes to scheduler logic, retry tuning, or reducer semantics do not introduce hidden regressions. This practice also improves onboarding: new engineers can study concrete scenarios 和 学习 system behavior quickly without touching production infrastructure. Deterministic simulation is the foundation that makes this sustainable.\n\nAnother important discipline is stage-level observability contracts. Each stage should emit deterministic counters 用于 accepted work, deferred work, retries, 和 dropped events. Without these counters, backpressure incidents become anecdotal 和 tuning decisions become reactive. 使用 deterministic metrics, teams can set concrete objectives such as maximum queue depth under specified load fixtures. These objectives should be tested in CI 使用 mocked scheduler runs, 和 regressions should block release until reviewed. This mirrors how robust distributed systems are managed in production: clear contracts, repeatable experiments, 和 explicit failure budgets. 用于 educational environments, it also reinforces that async correctness is not only about compiling futures but about predictable system behavior under stress.\n\nTeams should capture one-page runbooks 用于 each failure mode 和 link them directly from report outputs so responders can act immediately. These runbooks should include ownership, rollback criteria, 和 communication templates 用于 fast coordination.\n",
            "duration": "55 min"
          },
          "raip-v2-backpressure-concurrency": {
            "title": "Concurrency limits 和 backpressure",
            "content": "# Concurrency limits 和 backpressure\n\nBackpressure is not optional in high-volume pipelines. Without it, producer speed can overwhelm reducers, retries, or storage sinks. A resilient 设计 sets explicit concurrency caps 和 queue semantics that are easy to reason about.\n\nSemaphore-style limits are a common pattern: only N tasks can run at once. Additional tasks wait in queue. Deterministic simulation can model this 使用 a running list 和 remaining tick counters.\n\nRetry behavior interacts 使用 backpressure. If retries ignore queue pressure, they amplify congestion. Deterministic retry schedules should be bounded 和 inspectable.\n\n设计 reviews should ask: what is max concurrent work, what is queue policy, what happens on overload, 和 how is fairness maintained. Stable run reports provide concrete answers.\nAsync reliability work is strongest when concurrency behavior is testable without wall-clock timing. Real timers 和 threads can introduce nondeterminism that obscures logic bugs. A simulated scheduler 使用 deterministic tick advancement provides a clean environment 用于 validating bounded concurrency, retry sequencing, 和 backpressure behavior. In this model, tasks consume fixed ticks, queues are explicit, 和 completion order is reproducible.\n\nBackpressure 设计 should also be visible in reports. If incoming work exceeds concurrency budget, queues should grow predictably 和 metrics should expose this. Deterministic tests can assert queue length, total ticks, 和 completion order 用于 stress scenarios. This creates confidence that production systems degrade gracefully under load rather than failing unpredictably.\n\nReorg-safe indexing pipelines require idempotency 和 stable reducers. Duplicate deliveries should collapse by key, 和 snapshot reducers should produce canonical state outputs. If reducer output order drifts across runs, diff-based monitoring becomes noisy 和 incident triage slows down. Stable JSON 和 markdown reports prevent that by keeping artifacts comparable between runs 和 between code versions.\n\nOperational teams should maintain scenario catalogs 用于 burst traffic, retry storms, 和 partial-stage failures. Each scenario should specify expected queue depth, retry schedule, 和 final snapshot state. Running these catalogs on every release gives confidence that changes to scheduler logic, retry tuning, or reducer semantics do not introduce hidden regressions. This practice also improves onboarding: new engineers can study concrete scenarios 和 学习 system behavior quickly without touching production infrastructure. Deterministic simulation is the foundation that makes this sustainable.\n\nAnother important discipline is stage-level observability contracts. Each stage should emit deterministic counters 用于 accepted work, deferred work, retries, 和 dropped events. Without these counters, backpressure incidents become anecdotal 和 tuning decisions become reactive. 使用 deterministic metrics, teams can set concrete objectives such as maximum queue depth under specified load fixtures. These objectives should be tested in CI 使用 mocked scheduler runs, 和 regressions should block release until reviewed. This mirrors how robust distributed systems are managed in production: clear contracts, repeatable experiments, 和 explicit failure budgets. 用于 educational environments, it also reinforces that async correctness is not only about compiling futures but about predictable system behavior under stress.\n\nTeams should capture one-page runbooks 用于 each failure mode 和 link them directly from report outputs so responders can act immediately. These runbooks should include ownership, rollback criteria, 和 communication templates 用于 fast coordination.\n",
            "duration": "55 min"
          },
          "raip-v2-pipeline-graph-explorer": {
            "title": "Explorer: pipeline graph 和 concurrency",
            "content": "# Explorer: pipeline graph 和 concurrency\n\nPipeline graphs help teams communicate stage boundaries, concurrency budgets, 和 retry behaviors. A graph that shows ingest, dedupe, retry, 和 snapshot stages 使用 explicit capacities is far more actionable than prose descriptions.\n\nIn deterministic simulation, each stage can be represented as queue + worker budget. Events progress in ticks, 和 transitions are logged in timeline snapshots. This makes race conditions 和 starvation visible.\n\nA good explorer shows total ticks, completion order, 和 per-tick running/completed sets. These artifacts become checkpoints 用于 regression tests.\n\nPair graph exploration 使用 idempotency key tests. Duplicate events should not mutate state repeatedly. Stable reducers 和 sorted outputs make this easy to verify.\n\nThe final objective is operational confidence: when congestion or reorg scenarios occur, teams can replay deterministic fixtures 和 compare expected versus actual behavior quickly.\nAsync reliability work is strongest when concurrency behavior is testable without wall-clock timing. Real timers 和 threads can introduce nondeterminism that obscures logic bugs. A simulated scheduler 使用 deterministic tick advancement provides a clean environment 用于 validating bounded concurrency, retry sequencing, 和 backpressure behavior. In this model, tasks consume fixed ticks, queues are explicit, 和 completion order is reproducible.\n\nBackpressure 设计 should also be visible in reports. If incoming work exceeds concurrency budget, queues should grow predictably 和 metrics should expose this. Deterministic tests can assert queue length, total ticks, 和 completion order 用于 stress scenarios. This creates confidence that production systems degrade gracefully under load rather than failing unpredictably.\n\nReorg-safe indexing pipelines require idempotency 和 stable reducers. Duplicate deliveries should collapse by key, 和 snapshot reducers should produce canonical state outputs. If reducer output order drifts across runs, diff-based monitoring becomes noisy 和 incident triage slows down. Stable JSON 和 markdown reports prevent that by keeping artifacts comparable between runs 和 between code versions.\n\nOperational teams should maintain scenario catalogs 用于 burst traffic, retry storms, 和 partial-stage failures. Each scenario should specify expected queue depth, retry schedule, 和 final snapshot state. Running these catalogs on every release gives confidence that changes to scheduler logic, retry tuning, or reducer semantics do not introduce hidden regressions. This practice also improves onboarding: new engineers can study concrete scenarios 和 学习 system behavior quickly without touching production infrastructure. Deterministic simulation is the foundation that makes this sustainable.\n\nAnother important discipline is stage-level observability contracts. Each stage should emit deterministic counters 用于 accepted work, deferred work, retries, 和 dropped events. Without these counters, backpressure incidents become anecdotal 和 tuning decisions become reactive. 使用 deterministic metrics, teams can set concrete objectives such as maximum queue depth under specified load fixtures. These objectives should be tested in CI 使用 mocked scheduler runs, 和 regressions should block release until reviewed. This mirrors how robust distributed systems are managed in production: clear contracts, repeatable experiments, 和 explicit failure budgets. 用于 educational environments, it also reinforces that async correctness is not only about compiling futures but about predictable system behavior under stress.\n\nTeams should capture one-page runbooks 用于 each failure mode 和 link them directly from report outputs so responders can act immediately. These runbooks should include ownership, rollback criteria, 和 communication templates 用于 fast coordination.\n",
            "duration": "50 min"
          }
        }
      },
      "raip-v2-project-journey": {
        "title": "Reorg-safe Async Pipeline Project Journey",
        "description": "Implement deterministic scheduling, retries, dedupe/reducer stages, 和 report exports 用于 reorg-safe pipeline operations.",
        "lessons": {
          "raip-v2-pipeline-run": {
            "title": "Challenge: implement Pipeline::run()",
            "content": "Simulate bounded-concurrency execution 使用 deterministic task ordering.",
            "duration": "40 min"
          },
          "raip-v2-retry-policy": {
            "title": "Challenge: implement RetryPolicy schedule",
            "content": "Generate deterministic retry delay schedules 用于 linear 和 exponential policies.",
            "duration": "40 min"
          },
          "raip-v2-idempotency-dedupe": {
            "title": "Challenge: idempotency key dedupe",
            "content": "Deduplicate replay events by deterministic idempotency keys.",
            "duration": "35 min"
          },
          "raip-v2-snapshot-reducer": {
            "title": "Challenge: implement SnapshotReducer",
            "content": "Build deterministic snapshot state from simulated event streams.",
            "duration": "35 min"
          },
          "raip-v2-pipeline-report-checkpoint": {
            "title": "Checkpoint: pipeline run report",
            "content": "Export deterministic run report artifacts 用于 the async pipeline simulation.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "rust-proc-macros-codegen-safety": {
    "title": "Procedural Macros & Codegen 用于 Safety",
    "description": "Rust macro/codegen safety taught through deterministic parser 和 check-generation tooling 使用 audit-friendly outputs.",
    "duration": "10 hours",
    "tags": [
      "rust",
      "macros",
      "codegen",
      "safety"
    ],
    "modules": {
      "rpmcs-v2-foundations": {
        "title": "Macro 和 Codegen Foundations",
        "description": "Macro 思维模型s, constraint DSL 设计, 和 safety-driven code generation fundamentals.",
        "lessons": {
          "rpmcs-v2-macro-mental-model": {
            "title": "Macro 思维模型: declarative vs procedural",
            "content": "# Macro 思维模型: declarative vs procedural\n\nRust macros come in two broad forms: declarative macros 用于 pattern-based expansion 和 procedural macros 用于 syntax-aware transformation. Anchor relies heavily on macro-driven ergonomics to generate 账户 validation 和 指令 plumbing.\n\n用于 safety engineering, the value is consistency. Instead of hand-writing signer 和 owner checks in every handler, macro-style codegen can enforce these rules from concise attributes. This reduces copy-paste drift 和 makes review focus on policy intent.\n\nIn this 课程, we simulate proc-macro behavior 使用 deterministic TypeScript parser/generator helpers. The goal is conceptual transfer: attribute input -> AST -> generated checks -> runtime evaluation report.\n\nA macro 思维模型 helps avoid two mistakes: trusting generated behavior blindly 和 over-generalizing DSL syntax. Good macro 设计 keeps syntax explicit, expansion predictable, 和 errors readable.\n\nTreat generated checks as code artifacts, not opaque internals. Store them in tests, compare them in diffs, 和 validate behavior on controlled fixtures.\n\n## Operator mindset\n\nCodegen safety depends on reviewable output. If generated checks are not deterministic 和 diff-friendly, teams lose trust 和 incidents take longer to diagnose.\n\nMacro-inspired codegen is powerful because it can enforce safety contracts consistently across many handlers. In Anchor 和 Rust ecosystems, this is one reason attribute-based constraints reduce boilerplate 和 catch classes of validation bugs early. 用于 teaching in a browser environment, a deterministic parser 和 generator provides the same conceptual value without requiring compiler plugins.\n\nThe important principle is that generated checks must be reviewable. If developers cannot inspect generated output, trust erodes 和 debugging becomes harder. Stable generated strings, golden file tests, 和 deterministic run reports solve this. Teams can diff generated code as plain text 和 confirm that constraint changes are intentional.\n\nAnother key rule is clear DSL 设计. Attribute syntax should be strict enough to reject ambiguous input 和 explicit enough to encode signer, owner, relation, 和 mutability constraints. Parsing errors should include line-level hints where possible. Structured run results should identify failing constraints by kind 和 target, enabling direct remediation. This keeps codegen a safety tool rather than a hidden source of complexity.\n\nAs DSLs grow, teams should version grammar rules 和 keep migration guides 用于 older attribute forms. Unversioned grammar drift can silently break generated checks 和 create false confidence in safety coverage. Deterministic parsing fixtures catch these regressions early, especially when paired 使用 golden output snapshots 和 runtime validation cases. The result is a codegen workflow where changes are explicit, reviewable, 和 testable, which is exactly the behavior needed 用于 safety-critical constraint systems.\n\nHigh-quality codegen systems also include policy review gates. Before accepting a new attribute form, reviewers should verify that generated checks remain readable, failure messages remain actionable, 和 runtime evaluation remains deterministic. If a feature adds complexity without measurable safety benefit, it should be postponed. This keeps DSL scope disciplined 和 avoids turning safety tooling into a maintenance burden. Teams can further strengthen this 使用 compatibility suites that replay historical DSL inputs against new parsers 和 compare outputs byte-用于-byte. When differences appear, release notes should explain why behavior changed 和 how downstream users should adapt. This level of rigor is what allows macro-style tooling to scale safely in long-lived Rust ecosystems.\n\nA short policy checklist attached to pull requests keeps these reviews consistent 和 lowers the chance of accidental safety regressions. Include parser compatibility checks, generated diff review, 和 runtime validation signoff in every checklist.\n",
            "duration": "55 min"
          },
          "rpmcs-v2-codegen-safety-patterns": {
            "title": "Safety through codegen: constraint checks",
            "content": "# Safety through codegen: constraint checks\n\nConstraint codegen converts compact declarations into explicit runtime guards. Typical constraints include signer presence, 账户 ownership, has-one relations, 和 mutability requirements.\n\nA strong codegen pipeline validates input syntax strictly, produces deterministic output ordering, 和 emits meaningful errors 用于 unsupported forms. Weak codegen pipelines accept ambiguous syntax 和 produce inconsistent expansion, which undermines trust.\n\nOwnership checks are high-value constraints because 账户 substitution bugs are common in Solana systems. Generated owner guards reduce omission risk. Signer checks ensure privileged paths are gated by explicit authority.\n\nHas-one relation checks encode structural links between 账户 和 authorities. Generated relation checks reduce manual mistakes 和 keep behavior aligned across handlers.\n\nFinally, 测试 generated output via golden strings catches accidental expansion drift. This is especially useful during parser refactors.\nMacro-inspired codegen is powerful because it can enforce safety contracts consistently across many handlers. In Anchor 和 Rust ecosystems, this is one reason attribute-based constraints reduce boilerplate 和 catch classes of validation bugs early. 用于 teaching in a browser environment, a deterministic parser 和 generator provides the same conceptual value without requiring compiler plugins.\n\nThe important principle is that generated checks must be reviewable. If developers cannot inspect generated output, trust erodes 和 debugging becomes harder. Stable generated strings, golden file tests, 和 deterministic run reports solve this. Teams can diff generated code as plain text 和 confirm that constraint changes are intentional.\n\nAnother key rule is clear DSL 设计. Attribute syntax should be strict enough to reject ambiguous input 和 explicit enough to encode signer, owner, relation, 和 mutability constraints. Parsing errors should include line-level hints where possible. Structured run results should identify failing constraints by kind 和 target, enabling direct remediation. This keeps codegen a safety tool rather than a hidden source of complexity.\n\nAs DSLs grow, teams should version grammar rules 和 keep migration guides 用于 older attribute forms. Unversioned grammar drift can silently break generated checks 和 create false confidence in safety coverage. Deterministic parsing fixtures catch these regressions early, especially when paired 使用 golden output snapshots 和 runtime validation cases. The result is a codegen workflow where changes are explicit, reviewable, 和 testable, which is exactly the behavior needed 用于 safety-critical constraint systems.\n\nHigh-quality codegen systems also include policy review gates. Before accepting a new attribute form, reviewers should verify that generated checks remain readable, failure messages remain actionable, 和 runtime evaluation remains deterministic. If a feature adds complexity without measurable safety benefit, it should be postponed. This keeps DSL scope disciplined 和 avoids turning safety tooling into a maintenance burden. Teams can further strengthen this 使用 compatibility suites that replay historical DSL inputs against new parsers 和 compare outputs byte-用于-byte. When differences appear, release notes should explain why behavior changed 和 how downstream users should adapt. This level of rigor is what allows macro-style tooling to scale safely in long-lived Rust ecosystems.\n\nA short policy checklist attached to pull requests keeps these reviews consistent 和 lowers the chance of accidental safety regressions. Include parser compatibility checks, generated diff review, 和 runtime validation signoff in every checklist.\n",
            "duration": "55 min"
          },
          "rpmcs-v2-constraint-builder-explorer": {
            "title": "Explorer: constraint builder to generated checks",
            "content": "# Explorer: constraint builder to generated checks\n\nA constraint builder explorer helps engineers see how DSL choices affect generated code 和 runtime safety outcomes. Input one attribute line, observe parsed AST, generated pseudo-code, 和 pass/fail execution against sample inputs.\n\nThis tight loop is useful 用于 both education 和 production review. Teams can prototype new constraint forms, verify deterministic output, 和 add golden tests before adoption.\n\nThe explorer should surface parse failures clearly. If syntax is invalid, report line 和 expected format. If constraint kind is unsupported, fail 使用 deterministic error text.\n\nGenerated checks should preserve input order unless policy requires canonical sorting. Either way, behavior must be deterministic 和 documented.\n\nRuntime evaluation output should include failure list 使用 kind, target, 和 reason. This allows developers to fix configuration quickly 和 keeps safety reporting actionable.\nMacro-inspired codegen is powerful because it can enforce safety contracts consistently across many handlers. In Anchor 和 Rust ecosystems, this is one reason attribute-based constraints reduce boilerplate 和 catch classes of validation bugs early. 用于 teaching in a browser environment, a deterministic parser 和 generator provides the same conceptual value without requiring compiler plugins.\n\nThe important principle is that generated checks must be reviewable. If developers cannot inspect generated output, trust erodes 和 debugging becomes harder. Stable generated strings, golden file tests, 和 deterministic run reports solve this. Teams can diff generated code as plain text 和 confirm that constraint changes are intentional.\n\nAnother key rule is clear DSL 设计. Attribute syntax should be strict enough to reject ambiguous input 和 explicit enough to encode signer, owner, relation, 和 mutability constraints. Parsing errors should include line-level hints where possible. Structured run results should identify failing constraints by kind 和 target, enabling direct remediation. This keeps codegen a safety tool rather than a hidden source of complexity.\n\nAs DSLs grow, teams should version grammar rules 和 keep migration guides 用于 older attribute forms. Unversioned grammar drift can silently break generated checks 和 create false confidence in safety coverage. Deterministic parsing fixtures catch these regressions early, especially when paired 使用 golden output snapshots 和 runtime validation cases. The result is a codegen workflow where changes are explicit, reviewable, 和 testable, which is exactly the behavior needed 用于 safety-critical constraint systems.\n\nHigh-quality codegen systems also include policy review gates. Before accepting a new attribute form, reviewers should verify that generated checks remain readable, failure messages remain actionable, 和 runtime evaluation remains deterministic. If a feature adds complexity without measurable safety benefit, it should be postponed. This keeps DSL scope disciplined 和 avoids turning safety tooling into a maintenance burden. Teams can further strengthen this 使用 compatibility suites that replay historical DSL inputs against new parsers 和 compare outputs byte-用于-byte. When differences appear, release notes should explain why behavior changed 和 how downstream users should adapt. This level of rigor is what allows macro-style tooling to scale safely in long-lived Rust ecosystems.\n\nA short policy checklist attached to pull requests keeps these reviews consistent 和 lowers the chance of accidental safety regressions. Include parser compatibility checks, generated diff review, 和 runtime validation signoff in every checklist.\n",
            "duration": "50 min"
          }
        }
      },
      "rpmcs-v2-project-journey": {
        "title": "账户 Constraint Codegen (Sim)",
        "description": "Parse DSL constraints, generate checks, run deterministic evaluations, 和 publish stable safety reports.",
        "lessons": {
          "rpmcs-v2-parse-attributes": {
            "title": "Challenge: implement parseAttributes()",
            "content": "Parse mini-DSL constraints into deterministic AST nodes.",
            "duration": "40 min"
          },
          "rpmcs-v2-generate-checks": {
            "title": "Challenge: implement generateChecks()",
            "content": "Generate stable pseudo-code from parsed constraint AST.",
            "duration": "40 min"
          },
          "rpmcs-v2-golden-tests": {
            "title": "Challenge: deterministic golden-file checks",
            "content": "Compare generated check output against deterministic golden strings.",
            "duration": "35 min"
          },
          "rpmcs-v2-run-generated-checks": {
            "title": "Challenge: runGeneratedChecks()",
            "content": "Execute generated constraints on deterministic sample input.",
            "duration": "35 min"
          },
          "rpmcs-v2-generated-safety-report": {
            "title": "Checkpoint: generated safety report",
            "content": "Export deterministic markdown safety report from generated checks.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "anchor-upgrades-migrations": {
    "title": "Anchor Upgrades & 账户 Migrations",
    "description": "设计 production-safe Anchor release workflows 使用 deterministic migration planning, upgrade gates, rollback playbooks, 和 readiness evidence.",
    "duration": "8 hours",
    "tags": [
      "anchor",
      "solana",
      "upgrades",
      "migrations",
      "program-management"
    ],
    "modules": {
      "aum-v2-module-1": {
        "title": "Upgrade Foundations",
        "description": "Authority lifecycle, 账户 versioning strategy, 和 deterministic upgrade risk modeling 用于 Anchor releases.",
        "lessons": {
          "aum-v2-upgrade-authority-lifecycle": {
            "title": "Upgrade authority lifecycle in Anchor programs",
            "content": "# Upgrade authority lifecycle in Anchor programs\n\nAnchor makes 指令 development easier, but upgrade safety still depends on disciplined control of program authority. In production Solana systems, most upgrade incidents are not caused by syntax bugs. They come from process mistakes: wrong key management, unclear release ownership, 和 missing checks between build artifacts 和 deployed programdata. This 课时 teaches a 实战 lifecycle model that maps directly to how Anchor programs are deployed 和 governed.\n\nStart 使用 a strict authority model. Define who can sign upgrades 和 under which conditions. A single hot 钱包 is not acceptable 用于 mature systems. Typical setups use a multisig or 治理 path to approve artifacts, then a controlled signer to perform 部署. The important point is determinism: the same release decision should produce the same auditable evidence each time. That includes artifact hash, release tag, authority signers, 和 rollback policy. If your team cannot reconstruct those facts after a deploy, your process is too weak.\n\nNext, treat build reproducibility as a first-class requirement. You should compare the expected binary hash against programdata hash before 和 after 部署 in your pipeline simulation. Even when this 课程 stays deterministic 和 does not hit RPC, the policy should model hash matching as a gate. If the hash mismatch flag is true, the release is blocked. This simple rule prevents one of the most expensive failure classes: thinking you shipped one artifact while another artifact is actually live.\n\nAuthority transition rules matter too. Some protocols intentionally revoke upgrade authority after a stabilization window. Others keep authority but require 治理 timelocks 和 emergency pause conditions. Neither is universally correct. The key is consistency 使用 explicit trigger conditions. If you revoke authority too early, you lose the ability to patch critical bugs. If you never constrain authority, users cannot trust immutability promises. Anchor does not solve this 治理 tradeoff 用于 you; it only provides the program framework.\n\nRelease communication is part of 安全. Users 和 integrators need predictable language about what changed 和 whether state migration is required. 用于 example, if you add new 账户 fields but keep backward decoding compatibility, your report should say migration is optional 用于 old 账户 和 mandatory 用于 new writes after a certain slot range. If compatibility breaks, the report must include exact batch strategy 和 downtime expectations. Ambiguous language creates support load 和 increases operational risk.\n\nFinally, 设计 your release pipeline 用于 deterministic dry runs. Simulate migration steps, validation checks, 和 report generation locally. The goal is to eliminate unforced errors before any 交易 is signed. A deterministic runbook is not bureaucracy; it is what keeps urgent releases calm 和 reviewable.\n\n## Operator mindset\n\nAnchor upgrades are operations work 使用 cryptographic consequences. Authority controls, migration sequencing, 和 rollback criteria should be treated as release contracts, not informal habits.\n\n\nThis 课时 should become part of a release gate, not informal knowledge. Teams should keep deterministic fixtures 用于 each upgrade class: schema-only changes, 指令 behavior changes, 和 authority changes. 用于 every class, capture expected artifacts 和 compare those exact artifacts on pull requests. Include who approved migration logic, which constraints changed, 和 what rollback trigger would stop rollout. Mature Solana teams also keep a release timeline document 使用 explicit slot windows, RPC provider failover plan, 和 support messaging templates. If a release is paused, the plan should already define whether to retry 使用 the same artifact, revert authority settings, or perform a compensating migration. By preserving this in deterministic markdown 和 stable JSON, teams avoid panic changes during incidents 和 can audit exactly what happened after the fact. The same approach improves onboarding: new engineers 学习 from concrete evidence trails instead of tribal memory.\n\n## Checklist\n- Define clear authority ownership 和 approval flow.\n- Require artifact hash match before rollout.\n- Document authority transition 和 rollback policy.\n- Publish migration impact in deterministic report fields.\n- Block releases when dry-run evidence is missing.\n",
            "duration": "55 min"
          },
          "aum-v2-account-versioning-and-migrations": {
            "title": "账户 versioning 和 migration strategy",
            "content": "# 账户 versioning 和 migration strategy\n\nSolana 账户 are long-lived state containers, so program upgrades must respect historical data. In Anchor, adding or changing 账户 fields can be safe, risky, or catastrophic depending on how version markers, discriminators, 和 decode logic are handled. This 课时 focuses on migration planning that is deterministic, testable, 和 production-oriented.\n\nThe first rule is explicit version markers. Do not infer schema version from 账户 size alone because reallocations 和 optional fields can make that ambiguous. Include a version field 和 define what each version guarantees. Your migration planner can then segment 账户 ranges by version 和 apply deterministic transforms. Without explicit markers, teams often guess state shape 和 ship brittle one-off scripts.\n\nSecond, separate compatibility mode from migration mode. Compatibility mode means new code can read old 和 new versions while writes may still target old shape 用于 a transition period. Migration mode means writes are frozen or routed through upgrade-safe paths while 账户 batches are rewritten. Both modes are valid, but mixing them without clear boundaries creates partial state 和 broken assumptions.\n\nBatching is a 实战 necessity. Large protocols cannot migrate every 账户 in one 交易 or one slot. Your plan should define batch size, ordering, 和 integrity checks. 用于 example, process 账户 indexes in deterministic ascending order 和 verify expected post-migration invariants after each batch. If a batch fails, rerun exactly that batch 使用 idempotent logic. Deterministic batch identifiers make this auditable 和 easier to recover.\n\nPlan 用于 dry-run 和 rollback before execution. A migration plan should include prepare, migrate, verify, 和 finalize steps 使用 explicit criteria. Prepare can freeze new writes 和 snapshot baseline metrics. Verify can compare counts by version 和 check critical invariants. Finalize can re-enable writes 和 publish a signed report. Rollback should be defined as a separate branch, not improvised during incident pressure.\n\nAnchor adds value here through typed 账户 contexts 和 constraints, but migrations still require careful data engineering. 用于 every changed 账户 type, maintain deterministic test fixtures: old bytes, expected new bytes, 和 expected structured decode output. This catches layout regressions early 和 builds confidence when migrating real state.\n\nTreat migration metrics as product metrics too. Users care about downtime, failed actions, 和 consistency across clients. If migration affects read paths, expose status in UX so users understand what is happening. Reliable migrations are as much about communication 和 orchestration as they are about code.\n\n\nThis 课时 should become part of a release gate, not informal knowledge. Teams should keep deterministic fixtures 用于 each upgrade class: schema-only changes, 指令 behavior changes, 和 authority changes. 用于 every class, capture expected artifacts 和 compare those exact artifacts on pull requests. Include who approved migration logic, which constraints changed, 和 what rollback trigger would stop rollout. Mature Solana teams also keep a release timeline document 使用 explicit slot windows, RPC provider failover plan, 和 support messaging templates. If a release is paused, the plan should already define whether to retry 使用 the same artifact, revert authority settings, or perform a compensating migration. By preserving this in deterministic markdown 和 stable JSON, teams avoid panic changes during incidents 和 can audit exactly what happened after the fact. The same approach improves onboarding: new engineers 学习 from concrete evidence trails instead of tribal memory.\n\n## Checklist\n- Use explicit version markers in 账户 data.\n- Define compatibility 和 migration modes separately.\n- Migrate in deterministic batches 使用 idempotent retries.\n- Keep dry-run fixtures 用于 byte-level 和 structured outputs.\n- Publish migration status 和 completion evidence.\n",
            "duration": "55 min"
          },
          "aum-v2-upgrade-risk-explorer": {
            "title": "Explorer: upgrade risk matrix",
            "content": "# Explorer: upgrade risk matrix\n\nA useful upgrade explorer should show cause-和-effect between release inputs 和 safety outcomes. If a flag changes, engineers should immediately see how severity 和 readiness changes. This 课时 teaches how to build 和 read a deterministic risk matrix 用于 Anchor upgrades.\n\nThe matrix starts 使用 five high-signal inputs: upgrade authority present, program hash match, IDL breaking changes count, migration backfill completion, 和 dry-run pass status. These cover 治理, artifact integrity, compatibility risk, data readiness, 和 execution readiness. They are not exhaustive, but they are enough to prevent most avoidable mistakes.\n\nEach matrix row represents a release candidate state. 用于 every row, compute issue codes 和 severity levels in stable order. Stable ordering is not cosmetic; it allows exact output comparisons in CI 和 easy diff review in pull requests. If issue ordering changes between commits without policy changes, you know something in implementation drifted.\n\nSeverity calibration should be conservative. Missing upgrade authority, hash mismatch, 和 failed dry run are high severity because they directly block safe rollout. Incomplete backfill 和 IDL breaking changes are usually medium severity: sometimes resolvable 使用 migration notes 和 staged release windows, but still risky if ignored.\n\nThe explorer should also teach false confidence patterns. 用于 example, a release 使用 zero IDL changes can still be unsafe if program hash does not match approved artifact. Conversely, a release 使用 breaking changes can still be safe if migration plan is complete, compatibility notes are clear, 和 rollout is staged 使用 monitoring. Risk is contextual; deterministic policy helps avoid emotional decisions.\n\nFrom a workflow perspective, the matrix output should feed both engineering 和 support. Engineering uses JSON 用于 machine checks 和 gating. Support uses markdown summary to communicate whether release is ready, delayed, or blocked 和 why. If these outputs disagree, your generation path is wrong. Use one canonical payload 和 derive both formats.\n\nFinally, integrate the explorer into code review. Require reviewers to reference matrix output 用于 each release PR. This keeps decisions anchored in explicit evidence rather than implicit trust in 部署 scripts.\n\n\nThis 课时 should become part of a release gate, not informal knowledge. Teams should keep deterministic fixtures 用于 each upgrade class: schema-only changes, 指令 behavior changes, 和 authority changes. 用于 every class, capture expected artifacts 和 compare those exact artifacts on pull requests. Include who approved migration logic, which constraints changed, 和 what rollback trigger would stop rollout. Mature Solana teams also keep a release timeline document 使用 explicit slot windows, RPC provider failover plan, 和 support messaging templates. If a release is paused, the plan should already define whether to retry 使用 the same artifact, revert authority settings, or perform a compensating migration. By preserving this in deterministic markdown 和 stable JSON, teams avoid panic changes during incidents 和 can audit exactly what happened after the fact. The same approach improves onboarding: new engineers 学习 from concrete evidence trails instead of tribal memory.\n\n## Checklist\n- Use a canonical risk payload 使用 stable ordering.\n- Mark authority/hash/dry-run failures as blocking.\n- Keep JSON 和 markdown generated from one source.\n- Validate matrix behavior 使用 deterministic fixtures.\n- Treat explorer output as part of PR review evidence.\n",
            "duration": "50 min"
          },
          "aum-v2-plan-migration-steps": {
            "title": "Challenge: implement migration step planner",
            "content": "Implement deterministic migration planning output: fromVersion, toVersion, totalBatches, 和 requiresMigration.",
            "duration": "40 min"
          }
        }
      },
      "aum-v2-module-2": {
        "title": "Migration Execution",
        "description": "Safety validation gates, rollback planning, 和 deterministic readiness artifacts 用于 controlled migration execution.",
        "lessons": {
          "aum-v2-validate-upgrade-safety": {
            "title": "Challenge: implement upgrade safety gate checks",
            "content": "Implement deterministic blocking issue checks 用于 authority, artifact hash, 和 dry-run status.",
            "duration": "40 min"
          },
          "aum-v2-rollback-and-incident-playbooks": {
            "title": "Rollback strategy 和 incident playbooks",
            "content": "# Rollback strategy 和 incident playbooks\n\nEven strong upgrade plans can encounter surprises: incompatible downstream clients, unexpected 账户 edge cases, or release pipeline mistakes. Teams that recover quickly are the ones that predefine rollback 和 incident playbooks before any 部署 begins. This 课时 covers pragmatic rollback 设计 用于 Anchor-based systems.\n\nRollback starts 使用 explicit trigger conditions. Do not wait 用于 subjective debate during an incident. Define measurable triggers such as failure rate thresholds, migration error counts, or critical invariant violations. Once trigger conditions are met, the system should move into a known response mode: pause writes, stop new migration batches, 和 publish incident status.\n\nA common mistake is assuming rollback always means restoring old binary immediately. Sometimes that is correct; other times it can worsen state divergence if partial migrations already wrote new version markers. Your playbook should classify failure phase: pre-migration, mid-migration, or post-finalization. Each phase has different safest actions. Mid-migration incidents often require completing compensating transforms before binary rollback.\n\nAnchor 账户 constraints help protect invariant boundaries, but they do not orchestrate recovery sequencing. You still need deterministic tooling 用于 affected 账户 identification, reprocessing queues, 和 reconciliation summaries. Keep these tools pure 和 replayable where possible. If logic cannot be replayed, incident analysis becomes guesswork.\n\nCommunication is part of rollback. Engineering, support, 和 partner teams should consume the same deterministic report fields: release tag, rollback trigger, impacted batch ranges, current mitigation status, 和 next checkpoint time. Avoid free-form updates that diverge across channels.\n\nPost-incident learning must be concrete. 用于 each incident, add one or more deterministic fixtures reproducing the decision path that failed. Update policy functions 和 confirm that the new fixtures prevent recurrence. This is how reliability improves release after release.\n\nFinally, distinguish between emergency stop controls 和 full rollback procedures. Emergency stop is 用于 immediate blast radius reduction. Full rollback or forward-fix decisions can come after state assessment. Blending these concepts causes rushed mistakes.\n\n\nThis 课时 should become part of a release gate, not informal knowledge. Teams should keep deterministic fixtures 用于 each upgrade class: schema-only changes, 指令 behavior changes, 和 authority changes. 用于 every class, capture expected artifacts 和 compare those exact artifacts on pull requests. Include who approved migration logic, which constraints changed, 和 what rollback trigger would stop rollout. Mature Solana teams also keep a release timeline document 使用 explicit slot windows, RPC provider failover plan, 和 support messaging templates. If a release is paused, the plan should already define whether to retry 使用 the same artifact, revert authority settings, or perform a compensating migration. By preserving this in deterministic markdown 和 stable JSON, teams avoid panic changes during incidents 和 can audit exactly what happened after the fact. The same approach improves onboarding: new engineers 学习 from concrete evidence trails instead of tribal memory.\n\n## Checklist\n- Define measurable rollback triggers in advance.\n- Classify incident phase before selecting response path.\n- Keep recovery tooling replayable 和 deterministic.\n- Share one canonical incident report format.\n- Add regression fixtures after every rollback event.\n",
            "duration": "55 min"
          },
          "aum-v2-upgrade-report-markdown": {
            "title": "Challenge: build stable upgrade markdown summary",
            "content": "Generate deterministic markdown from releaseTag, totalBatches, 和 issueCount.",
            "duration": "35 min"
          },
          "aum-v2-upgrade-readiness-checkpoint": {
            "title": "Checkpoint: upgrade readiness artifact",
            "content": "Produce the final deterministic checkpoint artifact 使用 release tag, readiness flag, 和 migration batch count.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-reliability": {
    "title": "Reliability Engineering 用于 Solana",
    "description": "Production-focused reliability engineering 用于 Solana systems: fault tolerance, retries, deadlines, circuit breakers, 和 graceful degradation 使用 measurable operational outcomes.",
    "duration": "6 weeks",
    "tags": [
      "reliability",
      "fault-tolerance",
      "resilience",
      "production"
    ],
    "modules": {
      "mod-10-1": {
        "title": "Fault Tolerance Patterns",
        "description": "Implement fault-tolerance building blocks 使用 clear failure classification, retry boundaries, 和 deterministic recovery behavior.",
        "lessons": {
          "lesson-10-1-1": {
            "title": "Understanding Fault Tolerance",
            "content": "Fault tolerance in Solana systems is not just about catching errors. It is about deciding which failures are safe to retry, which should fail fast, 和 how to preserve user trust while doing both.\n\nA 实战 reliability model starts 使用 failure classes:\n1) transient failures (timeouts, temporary RPC unavailability),\n2) persistent external failures (rate limits, prolonged endpoint degradation),\n3) deterministic business failures (invalid input, invariant violations).\n\nTransient failures may justify bounded retries 使用 backoff. Deterministic business failures should not be retried because retries only add latency 和 load. Persistent external failures often require fallback endpoints, degraded features, or temporary protection modes.\n\nIn Solana workflows, reliability is tightly coupled to freshness constraints. A request can be logically valid but still fail if supporting state has shifted (用于 example stale quote windows or expired blockhash contexts in client workflows). Reliable systems therefore combine retry logic 使用 freshness checks 和 clear abort conditions.\n\nDefensive engineering means defining policies explicitly:\n- maximum retry count,\n- per-attempt timeout,\n- total deadline budget,\n- fallback behavior after budget exhaustion,\n- user-facing messaging 用于 each failure class.\n\nWithout explicit budgets, systems drift into infinite retry loops or fail too early. 使用 explicit budgets, behavior is predictable 和 testable.\n\n用于 production teams, observability is mandatory. Every failed operation should include a deterministic reason code 和 context fields (attempt number, endpoint, elapsed time, policy branch). This turns reliability from guesswork into measurable behavior.\n\nReliable systems do not promise zero failures. They promise controlled failure behavior: bounded latency, clear outcomes, 和 safe degradation under stress.",
            "duration": "45 min"
          },
          "lesson-10-1-2": {
            "title": "Retry Mechanism Challenge",
            "content": "Implement an exponential backoff retry mechanism 用于 handling transient failures.",
            "duration": "45 min"
          },
          "lesson-10-1-3": {
            "title": "Deadline Manager Challenge",
            "content": "Implement a deadline management system to enforce time limits on operations.",
            "duration": "45 min"
          },
          "lesson-10-1-4": {
            "title": "Fallback Handler Challenge",
            "content": "Implement a fallback mechanism that provides alternative execution paths when primary operations fail.",
            "duration": "45 min"
          }
        }
      },
      "mod-10-2": {
        "title": "Resilience Mechanisms",
        "description": "Build resilience mechanisms (circuit breakers, bulkheads, 和 rate controls) that protect core user flows during provider instability.",
        "lessons": {
          "lesson-10-2-1": {
            "title": "Resilience Patterns",
            "content": "Resilience patterns are controls that prevent localized failures from becoming system-wide incidents. On Solana integrations, they are especially important because provider health can change quickly under bursty network conditions.\n\nCircuit breaker pattern:\n- closed: normal operation,\n- open: block requests after repeated failures,\n- half-open: probe recovery 使用 controlled trial requests.\n\nA good breaker uses deterministic thresholds 和 cooldowns, not ad hoc toggles. It should expose state transitions 用于 monitoring 和 post-incident review.\n\nBulkhead pattern isolates resource pools so one failing workflow (用于 example expensive quote refresh loops) cannot starve unrelated workflows (like portfolio reads).\n\nRate limiting controls outbound pressure to providers. Proper limits reduce 429 storms 和 improve overall success rate. Token-bucket strategies are useful because they allow short bursts while preserving long-term bounds.\n\nThese patterns should be coordinated, not layered blindly. 用于 example, aggressive retries plus weak rate limiting can bypass the intent of a circuit breaker. Policy composition must be reviewed end-to-end.\n\nA mature resilience stack includes:\n- deterministic policy config,\n- simulation fixtures 用于 calm vs stressed traffic,\n- dashboard visibility 用于 breaker states 和 reject reasons,\n- explicit user copy 用于 degraded mode.\n\nResilience is successful when users experience predictable service quality under failure, not when systems appear perfect in ideal conditions.",
            "duration": "45 min"
          },
          "lesson-10-2-2": {
            "title": "Circuit Breaker Challenge",
            "content": "Implement a circuit breaker pattern that opens after consecutive failures 和 closes after a recovery period.",
            "duration": "45 min"
          },
          "lesson-10-2-3": {
            "title": "Rate Limiter Challenge",
            "content": "Implement a token bucket rate limiter 用于 controlling request rates.",
            "duration": "45 min"
          },
          "lesson-10-2-4": {
            "title": "Error Classifier Challenge",
            "content": "Implement an error classification system to determine if errors are retryable.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-testing-strategies": {
    "title": "测试 Strategies 用于 Solana",
    "description": "Comprehensive, production-oriented 测试 strategy 用于 Solana: deterministic unit tests, realistic integration tests, fuzz/property 测试, 和 release-confidence reporting.",
    "duration": "5 weeks",
    "tags": [
      "testing",
      "quality-assurance",
      "fuzzing",
      "property-testing"
    ],
    "modules": {
      "mod-11-1": {
        "title": "Unit 和 Integration 测试",
        "description": "Build deterministic unit 和 integration 测试 layers 使用 clear ownership of invariants, fixtures, 和 failure diagnostics.",
        "lessons": {
          "lesson-11-1-1": {
            "title": "测试 Fundamentals",
            "content": "测试 Solana systems effectively requires layered confidence, not one giant test suite.\n\nUnit tests validate pure logic: math, state transitions, 和 invariant checks. They should be fast, deterministic, 和 run on every change.\n\nIntegration tests validate component wiring: 账户模型ing, 指令 construction, 和 cross-模块 behavior under realistic inputs. They should catch schema drift 和 boundary errors that unit tests miss.\n\nA 实战 test pyramid 用于 Solana work:\n1) deterministic unit tests (broadest coverage),\n2) deterministic integration tests (targeted workflow coverage),\n3) environment-dependent checks (smaller set, higher cost).\n\nCommon failure in teams is over-reliance on environment-dependent tests while neglecting deterministic core checks. This creates flaky CI 和 weak debugging signals.\n\nGood test 设计 principles:\n- explicit fixture ownership,\n- stable expected outputs,\n- structured error assertions (not only success assertions),\n- regression fixtures 用于 previously discovered bugs.\n\n用于 production readiness, test outputs should be easy to audit. Summaries should include pass/fail counts by category, failing invariant IDs, 和 deterministic reproduction inputs.\n\n测试 is not just correctness verification; it is an operational communication tool. Strong test artifacts make release decisions clearer 和 incident response faster.",
            "duration": "45 min"
          },
          "lesson-11-1-2": {
            "title": "Test Assertion Framework Challenge",
            "content": "Implement a test assertion framework 用于 verifying program state.",
            "duration": "45 min"
          },
          "lesson-11-1-3": {
            "title": "Mock 账户 Generator Challenge",
            "content": "Create a mock 账户 generator 用于 测试 使用 configurable parameters.",
            "duration": "45 min"
          },
          "lesson-11-1-4": {
            "title": "Test Scenario Builder Challenge",
            "content": "Build a test scenario builder 用于 setting up complex test environments.",
            "duration": "45 min"
          }
        }
      },
      "mod-11-2": {
        "title": "高级 测试 Techniques",
        "description": "Use fuzzing, property-based tests, 和 mutation-style checks to expose edge-case failures before release.",
        "lessons": {
          "lesson-11-2-1": {
            "title": "Fuzzing 和 Property 测试",
            "content": "高级 测试 techniques uncover failures that example-based tests rarely find.\n\nFuzzing explores broad random input space to trigger parser edge cases, boundary overflows, 和 unexpected state combinations. It is especially useful 用于 serialization, decoding, 和 input validation layers.\n\nProperty-based 测试 defines invariants that must hold across many generated inputs. Instead of asserting one output, you assert a rule (用于 example: balances never become negative, or decoded-then-encoded payload remains stable).\n\nMutation-style thinking strengthens this further: intentionally alter assumptions 和 verify tests fail as expected. If tests still pass after a harmful change, coverage is weaker than it appears.\n\nTo keep 高级 测试 实战:\n- use deterministic seeds in CI 用于 reproducibility,\n- store failing cases as permanent regression fixtures,\n- separate heavy campaigns from per-commit fast checks.\n\n高级 tests are most valuable when tied to explicit risk categories. Map each category (serialization safety, invariant consistency, edge-case arithmetic) to at least one dedicated property or fuzz campaign.\n\nTeams that treat fuzz/property failures as first-class release blockers catch subtle defects earlier 和 reduce high-severity production incidents.",
            "duration": "45 min"
          },
          "lesson-11-2-2": {
            "title": "Fuzz Input Generator Challenge",
            "content": "Implement a fuzz input generator 用于 测试 使用 random data.",
            "duration": "45 min"
          },
          "lesson-11-2-3": {
            "title": "Property Verifier Challenge",
            "content": "Implement a property verifier that checks invariants hold across operations.",
            "duration": "45 min"
          },
          "lesson-11-2-4": {
            "title": "Boundary Value Analyzer Challenge",
            "content": "Implement a boundary value analyzer 用于 identifying edge cases.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-program-optimization": {
    "title": "Solana Program Optimization",
    "description": "Engineer production-grade Solana 性能: compute budgeting, 账户 layout efficiency, memory/rent tradeoffs, 和 deterministic optimization workflows.",
    "duration": "5 weeks",
    "tags": [
      "optimization",
      "performance",
      "compute-units",
      "profiling"
    ],
    "modules": {
      "mod-12-1": {
        "title": "Compute Unit Optimization",
        "description": "Optimize compute-heavy paths 使用 explicit CU budgets, operation-level profiling, 和 predictable 性能 tradeoffs.",
        "lessons": {
          "lesson-12-1-1": {
            "title": "Understanding Compute Units",
            "content": "Compute units are the hard resource budget that shapes what your Solana program can do in a single 交易. 性能 optimization starts by treating CU usage as a contract, not an afterthought.\n\nA reliable optimization loop is:\n1) measure baseline CU by operation type,\n2) identify dominant cost buckets (deserialization, hashing, loops, CPI fan-out),\n3) optimize one hotspot at a time,\n4) re-measure 和 keep only changes 使用 clear gains.\n\nCommon anti-patterns include optimizing cold paths, adding complexity without measurement, 和 ignoring 账户-size side effects. In Solana systems, compute 和 账户 设计 are coupled: a larger 账户 can increase deserialization cost, which raises CU pressure.\n\n用于 production teams, deterministic cost fixtures are crucial. They let you compare before/after behavior in CI 和 stop regressions early. 性能 work is most useful when every claim is backed by reproducible evidence, not intuition.",
            "duration": "45 min"
          },
          "lesson-12-1-2": {
            "title": "CU Counter Challenge",
            "content": "Implement a compute unit counter to estimate operation costs.",
            "duration": "45 min"
          },
          "lesson-12-1-3": {
            "title": "Data Structure Optimizer Challenge",
            "content": "Optimize data structures 用于 minimal serialization overhead.",
            "duration": "45 min"
          },
          "lesson-12-1-4": {
            "title": "Batch Operation Optimizer Challenge",
            "content": "Optimize batch operations to minimize compute units.",
            "duration": "45 min"
          }
        }
      },
      "mod-12-2": {
        "title": "Memory 和 Storage Optimization",
        "description": "设计 memory/storage-efficient 账户 layouts 使用 rent-aware sizing, serialization discipline, 和 safe migration planning.",
        "lessons": {
          "lesson-12-2-1": {
            "title": "账户 Data Optimization",
            "content": "账户 data optimization is both a cost 和 correctness discipline. Poor layouts increase rent, slow parsing, 和 make migrations fragile.\n\n设计 principles:\n- Keep hot fields compact 和 easy to parse.\n- Use fixed-size representations where possible.\n- Reserve growth strategy explicitly instead of ad hoc field expansion.\n- Separate frequently-mutated data from rarely-changed metadata when 实战.\n\nLayout decisions should be documented 使用 deterministic artifacts: field offsets, total bytes, 和 expected rent class. If a schema change increases 账户 size, reviewers should see the exact delta 和 migration implications.\n\nProduction optimization is not “smallest possible struct at any cost.” It is stable, readable, 和 migration-safe storage that keeps compute 和 rent budgets predictable over time.",
            "duration": "45 min"
          },
          "lesson-12-2-2": {
            "title": "Data Packer Challenge",
            "content": "Implement a data packer that efficiently packs fields into 账户 data.",
            "duration": "45 min"
          },
          "lesson-12-2-3": {
            "title": "Rent Calculator Challenge",
            "content": "Implement a rent calculator 用于 estimating 账户 storage costs.",
            "duration": "45 min"
          },
          "lesson-12-2-4": {
            "title": "Zero-Copy Deserializer Challenge",
            "content": "Implement a zero-copy deserializer 用于 reading data without allocation.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-tokenomics-design": {
    "title": "Tokenomics 设计 用于 Solana",
    "description": "设计 robust Solana token economies 使用 distribution discipline, vesting safety, staking incentives, 和 治理 mechanics that remain operationally defensible.",
    "duration": "5 weeks",
    "tags": [
      "tokenomics",
      "vesting",
      "staking",
      "governance",
      "incentives"
    ],
    "modules": {
      "mod-13-1": {
        "title": "Token Distribution 和 Vesting",
        "description": "Model token allocation 和 vesting systems 使用 explicit fairness, unlock predictability, 和 deterministic accounting rules.",
        "lessons": {
          "lesson-13-1-1": {
            "title": "Token Distribution Fundamentals",
            "content": "Token distribution is a 安全 和 credibility decision, not just a spreadsheet exercise. Allocation 和 vesting rules shape long-term trust in the protocol.\n\nA strong distribution model answers:\n- who receives tokens 和 why,\n- when they unlock,\n- how unlock schedules affect circulating supply,\n- what controls prevent accidental over-distribution.\n\nVesting 设计 should be deterministic 和 auditable. Cliff 和 linear phases must produce reproducible release amounts 用于 any timestamp. Ambiguous rounding rules create disputes 和 operational risk.\n\nProduction teams should maintain allocation invariants in tests (用于 example: total distributed <= total supply, per-bucket caps respected, no negative vesting balances). Tokenomics quality improves when economics 和 implementation are validated together.",
            "duration": "45 min"
          },
          "lesson-13-1-2": {
            "title": "Vesting Schedule Calculator Challenge",
            "content": "Implement a vesting schedule calculator 使用 cliff 和 linear release.",
            "duration": "45 min"
          },
          "lesson-13-1-3": {
            "title": "Token Allocation Distributor Challenge",
            "content": "Implement a token allocation distributor 用于 managing different stakeholder groups.",
            "duration": "45 min"
          },
          "lesson-13-1-4": {
            "title": "Release Schedule Generator Challenge",
            "content": "Generate a complete release schedule 使用 dates 和 amounts.",
            "duration": "45 min"
          }
        }
      },
      "mod-13-2": {
        "title": "Staking 和 治理",
        "description": "设计 staking 和 治理 mechanics 使用 clear incentive alignment, anti-manipulation constraints, 和 measurable participation health.",
        "lessons": {
          "lesson-13-2-1": {
            "title": "Staking 和 治理 设计",
            "content": "Staking 和 治理 systems must balance participation incentives 使用 manipulation resistance. Rewarding lock behavior is useful, but poorly tuned models can over-concentrate influence.\n\nCore 设计 questions:\n1) How is staking reward rate set 和 adjusted?\n2) How is voting power calculated (raw balance, delegated balance, time-weighted)?\n3) What prevents short-term 治理 capture?\n\n治理 math should be transparent 和 deterministic so users can verify voting outcomes independently. If power calculations are opaque or inconsistent, 治理 trust collapses quickly.\n\nOperationally, track 治理 health metrics: voter participation, delegation concentration, proposal pass patterns, 和 inactive stake ratios. Tokenomics is successful only when on-chain incentive behavior matches intended 治理 outcomes.",
            "duration": "45 min"
          },
          "lesson-13-2-2": {
            "title": "Staking Calculator Challenge",
            "content": "Implement a staking rewards calculator 使用 compounding.",
            "duration": "45 min"
          },
          "lesson-13-2-3": {
            "title": "Voting Power Calculator Challenge",
            "content": "Implement a voting power calculator 使用 delegation support.",
            "duration": "45 min"
          },
          "lesson-13-2-4": {
            "title": "Proposal Threshold Calculator Challenge",
            "content": "Implement a proposal threshold calculator 用于 治理.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-defi-primitives": {
    "title": "DeFi Primitives on Solana",
    "description": "Build 实战 Solana DeFi foundations: AMM mechanics, liquidity accounting, lending primitives, 和 flash-loan-safe composition patterns.",
    "duration": "6 weeks",
    "tags": [
      "defi",
      "amm",
      "lending",
      "yield-farming",
      "flash-loans"
    ],
    "modules": {
      "mod-14-1": {
        "title": "AMM 和 Liquidity",
        "description": "Implement AMM 和 liquidity primitives 使用 deterministic math, slippage-aware outputs, 和 LP accounting correctness.",
        "lessons": {
          "lesson-14-1-1": {
            "title": "AMM Fundamentals",
            "content": "AMM fundamentals are simple in formula but subtle in implementation quality. The invariant math must be deterministic, fee handling explicit, 和 rounding behavior consistent across paths.\n\n用于 constant-product pools, route quality is determined by output-at-size, not headline spot price. Larger trades move further on the curve 和 experience stronger impact, so quote logic must be size-aware.\n\nLiquidity accounting must also be exact: LP shares, fee accrual, 和 pool reserve updates should remain internally consistent under repeated swaps 和 edge-case sizes.\n\nProduction DeFi teams treat AMM math as critical infrastructure. They use fixed fixtures 用于 swap-in/swap-out cases, boundary amounts, 和 fee tiers so regressions are caught before 部署.",
            "duration": "45 min"
          },
          "lesson-14-1-2": {
            "title": "Constant Product AMM Challenge",
            "content": "Implement a constant product AMM 用于 token swaps.",
            "duration": "45 min"
          },
          "lesson-14-1-3": {
            "title": "Liquidity Provider Calculator Challenge",
            "content": "Calculate LP token minting 和 rewards 用于 liquidity providers.",
            "duration": "45 min"
          },
          "lesson-14-1-4": {
            "title": "Price Oracle Challenge",
            "content": "Implement a time-weighted average price oracle.",
            "duration": "45 min"
          }
        }
      },
      "mod-14-2": {
        "title": "Lending 和 Flash Loans",
        "description": "Model lending 和 flash-loan flows 使用 collateral safety, utilization-aware pricing, 和 strict repayment invariants.",
        "lessons": {
          "lesson-14-2-1": {
            "title": "Lending Protocol Mechanics",
            "content": "Lending primitives 和 flash-loan logic are powerful but unforgiving. Safety depends on strict collateral valuation, clear LTV/threshold rules, 和 deterministic repayment checks.\n\nA 实战 lending model should define:\n- collateral valuation source 和 freshness policy,\n- borrow limits 和 liquidation thresholds,\n- utilization-based rate behavior,\n- liquidation 和 bad-debt handling paths.\n\nFlash loans add atomic constraints: borrowed amount plus fee must be repaid in the same 交易 context. Any relaxation of this invariant introduces severe risk.\n\nComposable DeFi 设计 works when every primitive preserves local safety contracts while exposing clear interfaces 用于 higher-level orchestration.",
            "duration": "45 min"
          },
          "lesson-14-2-2": {
            "title": "Collateral Calculator Challenge",
            "content": "Implement collateral 和 borrowing power calculations.",
            "duration": "45 min"
          },
          "lesson-14-2-3": {
            "title": "Interest Rate Model Challenge",
            "content": "Implement a utilization-based interest rate model.",
            "duration": "45 min"
          },
          "lesson-14-2-4": {
            "title": "Flash Loan 验证者 Challenge",
            "content": "Implement flash loan validation logic.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-nft-standards": {
    "title": "NFT Standards on Solana",
    "description": "Implement Solana NFTs 使用 production-ready standards: metadata integrity, collection discipline, 和 高级 programmable/non-transferable behaviors.",
    "duration": "5 weeks",
    "tags": [
      "nft",
      "metaplex",
      "token-metadata",
      "candy-machine",
      "soulbound"
    ],
    "modules": {
      "mod-15-1": {
        "title": "NFT Fundamentals",
        "description": "Build core NFT functionality 使用 standards-compliant metadata, collection verification, 和 deterministic asset-state handling.",
        "lessons": {
          "lesson-15-1-1": {
            "title": "NFT Architecture on Solana",
            "content": "NFT architecture on Solana combines token mechanics 使用 metadata 和 collection semantics. A correct implementation requires more than minting a token 使用 supply one.\n\nCore components include:\n- mint/state ownership correctness,\n- metadata integrity 和 schema consistency,\n- collection linkage 和 verification status,\n- transfer 和 authority policy clarity.\n\nProduction NFT systems should treat metadata as a contract. If fields drift or verification flags are inconsistent, marketplaces 和 钱包 may interpret assets differently.\n\nReliable implementations include deterministic validation 用于 metadata structure, creator share totals, collection references, 和 authority expectations. Standards compliance is what preserves interoperability.",
            "duration": "45 min"
          },
          "lesson-15-1-2": {
            "title": "NFT Metadata Parser Challenge",
            "content": "Parse 和 validate NFT metadata according to Metaplex standards.",
            "duration": "45 min"
          },
          "lesson-15-1-3": {
            "title": "Collection Manager Challenge",
            "content": "Implement NFT collection management 使用 size tracking.",
            "duration": "45 min"
          },
          "lesson-15-1-4": {
            "title": "Attribute Rarity Calculator Challenge",
            "content": "Calculate NFT attribute rarity scores.",
            "duration": "45 min"
          }
        }
      },
      "mod-15-2": {
        "title": "高级 NFT Features",
        "description": "Implement 高级 NFT behaviors (soulbound 和 programmable flows) 使用 explicit policy controls 和 safe update semantics.",
        "lessons": {
          "lesson-15-2-1": {
            "title": "Soulbound 和 Programmable NFTs",
            "content": "高级 NFT features introduce policy complexity that must be explicit. Soulbound behavior, programmable restrictions, 和 dynamic metadata updates all expand failure surface.\n\n用于 soulbound models, non-transferability must be enforced by clear rule paths, not UI assumptions. 用于 programmable NFTs, update permissions 和 transition rules should be deterministic 和 auditable.\n\nDynamic NFT updates should include strong validation 和 event clarity so indexers 和 clients can track state changes correctly.\n\n高级 NFT engineering succeeds when flexibility is paired 使用 strict policy boundaries 和 transparent update behavior.",
            "duration": "45 min"
          },
          "lesson-15-2-2": {
            "title": "Soulbound Token 验证者 Challenge",
            "content": "Implement validation 用于 soulbound (non-transferable) tokens.",
            "duration": "45 min"
          },
          "lesson-15-2-3": {
            "title": "Dynamic NFT Updater Challenge",
            "content": "Implement dynamic NFT attributes that can evolve over time.",
            "duration": "45 min"
          },
          "lesson-15-2-4": {
            "title": "NFT Composability Challenge",
            "content": "Implement NFT composability 用于 equipping items to base NFTs.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-cpi-patterns": {
    "title": "跨程序调用 Patterns",
    "description": "Master CPI composition on Solana 使用 safe 账户 validation, PDA signer discipline, 和 deterministic multi-program orchestration patterns.",
    "duration": "6 weeks",
    "tags": [
      "cpi",
      "cross-program-invocation",
      "composition",
      "pda-signing"
    ],
    "modules": {
      "mod-16-1": {
        "title": "CPI Fundamentals",
        "description": "Build CPI fundamentals 使用 strict 账户/signer checks, ownership validation, 和 safe PDA signing boundaries.",
        "lessons": {
          "lesson-16-1-1": {
            "title": "跨程序调用 Architecture",
            "content": "跨程序调用 (CPI) is where Solana composability becomes 实战 和 where many 安全 failures appear. The caller controls 账户 lists, so every CPI boundary must be treated as untrusted input.\n\nSafe CPI 设计 requires:\n- explicit 账户 identity 和 owner validation,\n- signer 和 writable scope minimization,\n- deterministic PDA derivation 和 signer-seed handling,\n- bounded assumptions about downstream program behavior.\n\ninvoke 和 invoke_signed are not interchangeable conveniences. invoke_signed should only be used when signer proof is truly required 和 seed recipes are canonical.\n\nProduction CPI reliability comes from repeatable guard patterns. If constraints vary handler to handler, reviewers cannot reason about 安全 consistently.",
            "duration": "45 min"
          },
          "lesson-16-1-2": {
            "title": "CPI 账户 验证者 Challenge",
            "content": "Validate 账户 用于 跨程序调用s.",
            "duration": "45 min"
          },
          "lesson-16-1-3": {
            "title": "PDA Signer Challenge",
            "content": "Implement PDA signing 用于 CPI operations.",
            "duration": "45 min"
          },
          "lesson-16-1-4": {
            "title": "指令 Router Challenge",
            "content": "Implement an 指令 router 用于 directing CPI calls.",
            "duration": "45 min"
          }
        }
      },
      "mod-16-2": {
        "title": "高级 CPI Patterns",
        "description": "Compose 高级 multi-program flows 使用 atomicity awareness, consistency checks, 和 deterministic failure handling.",
        "lessons": {
          "lesson-16-2-1": {
            "title": "Multi-Program Composition",
            "content": "Multi-program composition introduces sequencing 和 consistency risk. Even when each CPI call is correct in isolation, combined flows can violate business invariants if ordering or rollback assumptions are weak.\n\nRobust composition patterns include:\n1) explicit stage boundaries,\n2) invariant checks between CPI steps,\n3) deterministic error classes 用于 partial-failure diagnosis,\n4) idempotent recovery paths where possible.\n\n用于 complex operations (atomic swaps, flash-loan sequences), model expected preconditions 和 postconditions per stage. This keeps orchestration testable 和 prevents hidden state drift.\n\nCPI mastery is less about calling many programs 和 more about preserving correctness across program boundaries under adverse inputs.",
            "duration": "45 min"
          },
          "lesson-16-2-2": {
            "title": "Atomic Swap Orchestrator Challenge",
            "content": "Implement an atomic swap across multiple programs.",
            "duration": "45 min"
          },
          "lesson-16-2-3": {
            "title": "State Consistency 验证者 Challenge",
            "content": "Validate state consistency across multiple CPI calls.",
            "duration": "45 min"
          },
          "lesson-16-2-4": {
            "title": "Permissioned Invocation Challenge",
            "content": "Implement permission checks 用于 program invocations.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-mev-strategies": {
    "title": "MEV 和 交易 Ordering",
    "description": "Production-focused 交易-ordering engineering on Solana: MEV-aware routing, bundle strategy, liquidation/arbitrage modeling, 和 user-protective execution controls.",
    "duration": "6 weeks",
    "tags": [
      "mev",
      "arbitrage",
      "liquidation",
      "jito",
      "priority-fees",
      "sandwich"
    ],
    "modules": {
      "mod-17-1": {
        "title": "MEV Fundamentals",
        "description": "Understand MEV mechanics 和 交易 ordering realities, then model opportunities 和 risks 使用 deterministic safety-aware policies.",
        "lessons": {
          "lesson-17-1-1": {
            "title": "MEV on Solana",
            "content": "Maximal Extractable Value (MEV) on Solana is fundamentally about 交易 ordering under limited blockspace. Whether you are building trading tools, liquidation infrastructure, or user-facing apps, you need a realistic model of how ordering pressure changes outcomes.\n\nKey Solana-specific context:\n- ordering can be influenced by priority fees 和 relay/bundle paths,\n- opportunities are short-lived 和 highly competitive,\n- failed or delayed execution can convert expected profit into loss.\n\nA mature MEV approach begins 使用 classification:\n1) opportunity class (arbitrage, liquidation, backrun-style sequencing),\n2) dependency class (single-hop vs multi-hop, oracle-sensitive vs pool-state-sensitive),\n3) risk class (staleness, fill failure, adverse movement, execution contention).\n\n用于 production systems, raw opportunity detection is not enough. You need deterministic filters that reject fragile setups: stale quotes, weak depth, or excessive path complexity relative to expected edge.\n\nMost operational failures come from execution assumptions, not math. Teams should model inclusion probability, fallback paths, 和 cancellation thresholds explicitly.\n\nUser-protective 设计 matters even 用于 高级 orderflow systems. Clear policy around slippage limits, quote freshness, 和 failure reporting prevents silent value leakage 和 reduces support incidents.",
            "duration": "45 min"
          },
          "lesson-17-1-2": {
            "title": "Arbitrage Opportunity Detector Challenge",
            "content": "Detect arbitrage opportunities across DEXes.",
            "duration": "45 min"
          },
          "lesson-17-1-3": {
            "title": "Liquidation Opportunity Finder Challenge",
            "content": "Find undercollateralized positions 用于 liquidation.",
            "duration": "45 min"
          },
          "lesson-17-1-4": {
            "title": "Priority Fee Calculator Challenge",
            "content": "Calculate optimal priority fees 用于 交易 ordering.",
            "duration": "45 min"
          }
        }
      },
      "mod-17-2": {
        "title": "高级 MEV Strategies",
        "description": "设计 高级 ordering/bundle strategies 使用 explicit risk controls, failure handling, 和 user-impact guardrails.",
        "lessons": {
          "lesson-17-2-1": {
            "title": "高级 MEV Techniques",
            "content": "高级 交易-ordering strategies require disciplined orchestration, not just faster opportunity scans.\n\nBundle-oriented execution is valuable because it can express dependency sets 和 all-or-nothing intent, but bundle 设计 must include:\n- strict preconditions,\n- deterministic abort rules,\n- replay-safe identifiers,\n- post-execution reconciliation.\n\nWhen strategy complexity increases (multi-hop paths, conditional liquidations), failure modes multiply: partial fills, stale assumptions, 和 contention spikes. A robust system ranks candidates by expected net value after execution risk, not gross theoretical edge.\n\nOperational controls should include:\n1) bounded retries 使用 fresh-state checks,\n2) confidence scoring 用于 each candidate,\n3) kill-switch thresholds 用于 abnormal failure streaks,\n4) deterministic run reports 用于 incident replay.\n\n高级 MEV tooling is successful when it is both profitable 和 controllable. Deterministic artifacts (decision inputs, chosen path, reason codes) are what make that control real in production.",
            "duration": "45 min"
          },
          "lesson-17-2-2": {
            "title": "Bundle Composer Challenge",
            "content": "Compose 交易 bundles 用于 atomic MEV extraction.",
            "duration": "45 min"
          },
          "lesson-17-2-3": {
            "title": "Multi-Hop Arbitrage Finder Challenge",
            "content": "Find multi-hop arbitrage paths across token pairs.",
            "duration": "45 min"
          },
          "lesson-17-2-4": {
            "title": "MEV Simulation Engine Challenge",
            "content": "Simulate MEV extraction to estimate profitability.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-deployment-cicd": {
    "title": "Program 部署 和 CI/CD",
    "description": "Production 部署 engineering 用于 Solana programs: environment strategy, release gating, CI/CD quality controls, 和 upgrade-safe operational workflows.",
    "duration": "4 weeks",
    "tags": [
      "deployment",
      "cicd",
      "devnet",
      "mainnet",
      "upgrades",
      "testing"
    ],
    "modules": {
      "mod-18-1": {
        "title": "部署 Fundamentals",
        "description": "Model environment-specific 部署 behavior 使用 deterministic configs, artifact checks, 和 release preflight validation.",
        "lessons": {
          "lesson-18-1-1": {
            "title": "Solana 部署 Environments",
            "content": "Solana 部署 is not one command; it is a release system 使用 environment-specific risk. Localnet, devnet, 和 mainnet each serve different validation goals, 和 production quality depends on using them intentionally.\n\nA reliable 部署 workflow defines:\n1) environment purpose 和 promotion criteria,\n2) deterministic config sources,\n3) artifact identity checks,\n4) rollback triggers.\n\nCommon failure patterns include mismatched program IDs, inconsistent config between environments, 和 unverified artifact drift between build 和 deploy. These are process issues that tooling should prevent.\n\nPreflight checks should be mandatory:\n- expected network 和 signer identity,\n- build artifact hash,\n- program size 和 upgrade constraints,\n- required 账户/address assumptions.\n\nEnvironment promotion should be evidence-driven. Passing local tests alone is not enough 用于 mainnet readiness; devnet/staging behavior must confirm operational assumptions under realistic RPC 和 timing conditions.\n\n部署 maturity is measured by reproducibility. If another engineer cannot replay the release inputs 和 get the same artifact 和 checklist outcome, the pipeline is too fragile.",
            "duration": "45 min"
          },
          "lesson-18-1-2": {
            "title": "部署 Config Manager Challenge",
            "content": "Manage 部署 configurations 用于 different environments.",
            "duration": "45 min"
          },
          "lesson-18-1-3": {
            "title": "Program Size 验证者 Challenge",
            "content": "Validate program binary size 和 compute budget.",
            "duration": "45 min"
          },
          "lesson-18-1-4": {
            "title": "Upgrade Authority Manager Challenge",
            "content": "Manage program upgrade authorities 和 permissions.",
            "duration": "45 min"
          }
        }
      },
      "mod-18-2": {
        "title": "CI/CD Pipelines",
        "description": "Build CI/CD pipelines that enforce build/test/安全 gates, compatibility checks, 和 controlled rollout/rollback evidence.",
        "lessons": {
          "lesson-18-2-1": {
            "title": "CI/CD 用于 Solana Programs",
            "content": "CI/CD 用于 Solana should enforce release quality, not just automate command execution.\n\nA 实战 pipeline includes staged gates:\n1) static quality gate (lint/type/安全 checks),\n2) deterministic unit/integration tests,\n3) build reproducibility 和 artifact hashing,\n4) 部署 preflight validation,\n5) controlled rollout 使用 observability checks.\n\nEach gate should produce machine-readable evidence. Release decisions become faster 和 safer when teams can inspect deterministic artifacts instead of scanning raw logs.\n\nVersion compatibility checks are critical in Solana ecosystems where CLI/toolchain mismatches can break builds or runtime expectations. Pipelines should fail fast on incompatible matrices.\n\nRollout strategy should also be explicit: canary/degraded mode, monitoring window, 和 rollback conditions. “Deploy 和 hope” is not a strategy.\n\nThe best CI/CD systems reduce human toil while increasing decision clarity. Automation should encode operational policy, not bypass it.",
            "duration": "45 min"
          },
          "lesson-18-2-2": {
            "title": "Build Pipeline 验证者 Challenge",
            "content": "Validate CI/CD pipeline stages 和 dependencies.",
            "duration": "45 min"
          },
          "lesson-18-2-3": {
            "title": "Version Compatibility Checker Challenge",
            "content": "Check version compatibility between tools 和 dependencies.",
            "duration": "45 min"
          },
          "lesson-18-2-4": {
            "title": "部署 Rollback Manager Challenge",
            "content": "Manage 部署 rollbacks 和 recovery.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-cross-chain-bridges": {
    "title": "Cross-Chain Bridges 和 Wormhole",
    "description": "Build safer cross-chain integrations 用于 Solana using Wormhole-style messaging, attestation verification, 和 deterministic bridge-state controls.",
    "duration": "6 weeks",
    "tags": [
      "bridges",
      "wormhole",
      "cross-chain",
      "interoperability",
      "messaging"
    ],
    "modules": {
      "mod-43-1": {
        "title": "Wormhole Messaging Fundamentals",
        "description": "Understand cross-chain messaging trust boundaries, guardian attestations, 和 deterministic verification pipelines.",
        "lessons": {
          "lesson-43-1-1": {
            "title": "Cross-Chain Messaging Architecture",
            "content": "Cross-chain messaging is a trust-boundary problem before it is a transport problem. In Wormhole-style systems, messages are observed, attested, 和 consumed across different chain environments, each 使用 independent failure modes.\n\nA robust architecture model includes:\n1) emitter semantics (what exactly is being attested),\n2) attestation verification (who signed 和 under what threshold),\n3) replay prevention (message uniqueness 和 consumption state),\n4) execution safety (what happens if target-chain state has changed).\n\nVerification must be deterministic 和 strict. Accepting malformed or weakly validated attestations is a direct safety risk.\n\nCross-chain systems should also expose explicit reason codes 用于 rejects: invalid signatures, stale message, already-consumed message, unsupported payload schema. This improves operator response 和 audit quality.\n\nMessaging reliability depends on observability. Teams need deterministic logs linking source event IDs to target execution outcomes so they can reconcile partial or delayed flows.\n\nCross-chain engineering succeeds when attestation trust assumptions are transparent 和 enforced consistently at every consume path.",
            "duration": "45 min"
          },
          "lesson-43-1-2": {
            "title": "VAA Verifier Challenge",
            "content": "Implement VAA (Verified Action Approval) signature verification.",
            "duration": "45 min"
          },
          "lesson-43-1-3": {
            "title": "Message Emitter Challenge",
            "content": "Implement cross-chain message emission 和 tracking.",
            "duration": "45 min"
          },
          "lesson-43-1-4": {
            "title": "Replay Protection Challenge",
            "content": "Implement replay protection 用于 cross-chain messages.",
            "duration": "45 min"
          }
        }
      },
      "mod-43-2": {
        "title": "Asset Bridging Patterns",
        "description": "Implement asset-bridging patterns 使用 strict supply/accounting invariants, replay protection, 和 reconciliation workflows.",
        "lessons": {
          "lesson-43-2-1": {
            "title": "Token Bridging Mechanics",
            "content": "Token bridging requires strict supply 和 state invariants. Lock-和-mint 和 burn-和-mint models both rely on one central rule: represented supply across chains must remain coherent.\n\nCritical controls include:\n- single-consume message semantics,\n- deterministic mint/unlock accounting,\n- paused-mode handling 用于 incident containment,\n- reconciliation reports between source 和 target totals.\n\nA bridge flow should define state transitions explicitly: initiated, attested, executed, reconciled. Missing state transitions create operational blind spots.\n\nReplay 和 duplication are recurring bridge risks. Systems must key transfer intents deterministically 和 reject repeated execution attempts even under retries or delayed relays.\n\nProduction bridge operations also need runbooks: what to do on attestation delays, threshold signer issues, or target-chain execution failures.\n\nBridging quality is not just throughput; it is verifiable accounting integrity under adverse network conditions.",
            "duration": "45 min"
          },
          "lesson-43-2-2": {
            "title": "Token Locker Challenge",
            "content": "Implement token locking 用于 bridge deposits.",
            "duration": "45 min"
          },
          "lesson-43-2-3": {
            "title": "Wrapped Token Mint Challenge",
            "content": "Manage wrapped token minting 和 supply tracking.",
            "duration": "45 min"
          },
          "lesson-43-2-4": {
            "title": "Bridge Rate Limiter Challenge",
            "content": "Implement rate limiting 用于 bridge withdrawals.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-oracle-pyth": {
    "title": "Oracle Integration 和 Pyth Network",
    "description": "Integrate Solana oracle feeds safely: price validation, confidence/staleness policy, 和 multi-source aggregation 用于 resilient protocol decisions.",
    "duration": "5 weeks",
    "tags": [
      "oracle",
      "pyth",
      "price-feeds",
      "data-validation",
      "aggregation"
    ],
    "modules": {
      "mod-44-1": {
        "title": "Price Feed Fundamentals",
        "description": "Understand oracle data semantics (price, confidence, staleness) 和 enforce deterministic validation before business logic.",
        "lessons": {
          "lesson-44-1-1": {
            "title": "Oracle Price Feeds",
            "content": "Oracle integration is a risk-control problem, not a data-fetch problem. Price feeds must be evaluated 用于 freshness, confidence, 和 contextual fitness before they drive protocol decisions.\n\nA safe oracle validation pipeline checks:\n1) feed status 和 availability,\n2) staleness window compliance,\n3) confidence-band reasonableness,\n4) value bounds against protocol policy.\n\nUsing raw price without confidence or staleness checks can trigger invalid liquidations, bad quotes, or incorrect risk assessments.\n\nValidation outputs should be deterministic 和 structured (accept/reject 使用 reason code). This helps downstream systems choose safe fallback behavior.\n\nProtocols should separate “data exists” from “data is usable.” A feed can be present but still unfit due to stale timestamp or extreme uncertainty.\n\nProduction reliability improves when oracle checks are versioned 和 fixture-tested across calm 和 stressed market scenarios.",
            "duration": "45 min"
          },
          "lesson-44-1-2": {
            "title": "Price 验证者 Challenge",
            "content": "Validate oracle prices 用于 correctness 和 freshness.",
            "duration": "45 min"
          },
          "lesson-44-1-3": {
            "title": "Price Normalizer Challenge",
            "content": "Normalize prices 使用 different exponents to common scale.",
            "duration": "45 min"
          },
          "lesson-44-1-4": {
            "title": "EMA Calculator Challenge",
            "content": "Calculate Exponential Moving Average 用于 price smoothing.",
            "duration": "45 min"
          }
        }
      },
      "mod-44-2": {
        "title": "Multi-Oracle Aggregation",
        "description": "设计 multi-oracle aggregation 和 consensus policies that reduce single-source failure risk while remaining explainable 和 testable.",
        "lessons": {
          "lesson-44-2-1": {
            "title": "Oracle Aggregation Strategies",
            "content": "Multi-oracle aggregation reduces single-point dependency but adds policy complexity. The goal is not to average blindly; it is to produce a robust decision value 使用 clear confidence in adverse conditions.\n\nCommon strategies include median, trimmed mean, 和 weighted consensus. Strategy choice should reflect threat model: outlier resistance, latency tolerance, 和 source diversity.\n\nAggregation policies should define:\n- minimum participating sources,\n- max divergence threshold,\n- fallback action when consensus fails.\n\nIf sources diverge beyond policy bounds, the safe action may be to halt sensitive operations rather than force a number.\n\nDeterministic aggregation reports should include contributing sources, excluded outliers, 和 final consensus rationale. This is essential 用于 audits 和 incident response.\n\nA good oracle stack is transparent: every accepted value can be explained, replayed, 和 defended.",
            "duration": "45 min"
          },
          "lesson-44-2-2": {
            "title": "Median Price Calculator Challenge",
            "content": "Calculate median price from multiple oracle sources.",
            "duration": "45 min"
          },
          "lesson-44-2-3": {
            "title": "Oracle Consensus Challenge",
            "content": "Implement consensus checking across multiple oracle sources.",
            "duration": "45 min"
          },
          "lesson-44-2-4": {
            "title": "Fallback Oracle Manager Challenge",
            "content": "Manage primary 和 fallback oracle sources.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-dao-tooling": {
    "title": "DAO Tooling 和 Autonomous Organizations",
    "description": "Build production-ready DAO systems on Solana: proposal 治理, voting integrity, treasury controls, 和 deterministic execution/reporting workflows.",
    "duration": "5 weeks",
    "tags": [
      "dao",
      "governance",
      "proposals",
      "voting",
      "treasury",
      "automation"
    ],
    "modules": {
      "mod-45-1": {
        "title": "DAO 治理 Mechanics",
        "description": "Implement 治理 mechanics 使用 explicit proposal lifecycle rules, voting-power logic, 和 deterministic state transitions.",
        "lessons": {
          "lesson-45-1-1": {
            "title": "DAO 治理 Architecture",
            "content": "DAO 治理 architecture is a system of enforceable process rules. Proposal creation, voting, 和 execution must be deterministic, auditable, 和 resistant to manipulation.\n\nA robust 治理 model defines:\n1) proposal lifecycle states 和 transitions,\n2) voter eligibility 和 power calculation,\n3) quorum/approval thresholds by action class,\n4) execution preconditions 和 cancellation paths.\n\n治理 failures usually come from ambiguity: unclear thresholds, inconsistent snapshot timing, or weak transition validation.\n\nState transitions should be explicit 和 testable. Invalid transitions (用于 example executed -> voting) should fail 使用 deterministic errors.\n\nVoting-power logic must also be transparent. Whether delegation, time-weighting, or quadratic models are used, outcomes should be reproducible from public inputs.\n\nDAO tooling quality is measured by predictability under pressure. During contentious proposals, deterministic behavior 和 clear reason codes are what preserve legitimacy.",
            "duration": "45 min"
          },
          "lesson-45-1-2": {
            "title": "Proposal Lifecycle Manager Challenge",
            "content": "Manage DAO proposal states 和 transitions.",
            "duration": "45 min"
          },
          "lesson-45-1-3": {
            "title": "Voting Power Calculator Challenge",
            "content": "Calculate voting power 使用 delegation 和 quadratic options.",
            "duration": "45 min"
          },
          "lesson-45-1-4": {
            "title": "Delegation Manager Challenge",
            "content": "Manage vote delegation between DAO members.",
            "duration": "45 min"
          }
        }
      },
      "mod-45-2": {
        "title": "Treasury 和 Execution",
        "description": "Engineer treasury 和 execution tooling 使用 policy gates, timelock safeguards, 和 auditable automation outcomes.",
        "lessons": {
          "lesson-45-2-1": {
            "title": "DAO Treasury Management",
            "content": "DAO treasury management is where 治理 intent becomes real financial action. Treasury tooling must therefore combine flexibility 使用 strict policy constraints.\n\nCore controls include:\n- spending limits 和 role-based authority,\n- timelock windows 用于 sensitive actions,\n- multisig/escalation paths,\n- deterministic execution logs.\n\nAutomated execution should never hide policy checks. Every executed action should reference the proposal, required approvals, 和 control checks passed.\n\nFailure handling is equally important. If execution fails mid-flow, tooling should expose exact failure stage 和 safe retry/rollback guidance.\n\nTreasury systems should also produce reconciliation artifacts: proposed vs executed amounts, remaining budget, 和 exception records.\n\nOperationally mature DAOs treat treasury automation as regulated process infrastructure: explicit controls, reproducible evidence, 和 clear accountability boundaries.",
            "duration": "45 min"
          },
          "lesson-45-2-2": {
            "title": "Treasury Spending Limit Challenge",
            "content": "Implement spending limits 和 budget tracking 用于 DAO treasury.",
            "duration": "45 min"
          },
          "lesson-45-2-3": {
            "title": "Timelock Executor Challenge",
            "content": "Implement timelock 用于 delayed proposal execution.",
            "duration": "45 min"
          },
          "lesson-45-2-4": {
            "title": "Automated Action Trigger Challenge",
            "content": "Implement automated triggers 用于 DAO actions based on conditions.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-gaming": {
    "title": "Gaming 和 Game State Management",
    "description": "Build production-ready on-chain game systems on Solana: efficient state models, turn integrity, fairness controls, 和 scalable player progression economics.",
    "duration": "5 weeks",
    "tags": [
      "gaming",
      "game-state",
      "randomness",
      "turn-based",
      "progression"
    ],
    "modules": {
      "mod-46-1": {
        "title": "Game State Architecture",
        "description": "设计 game state 和 turn logic 使用 deterministic transitions, storage efficiency, 和 anti-cheat validation boundaries.",
        "lessons": {
          "lesson-46-1-1": {
            "title": "On-Chain Game 设计",
            "content": "On-chain game 设计 on Solana is a systems-engineering tradeoff between fairness, responsiveness, 和 cost. The best designs keep critical rules verifiable while minimizing expensive state writes.\n\nCore architecture decisions:\n1) what state must be on-chain 用于 trust,\n2) what can remain off-chain 用于 speed,\n3) how turn validity is enforced deterministically.\n\nTurn-based mechanics should use explicit state transitions 和 guard checks (current actor, phase, cooldown, resource limits). If transitions are ambiguous, replay 和 dispute resolution become difficult.\n\nState compression 和 compact encoding matter because game loops can generate many updates. Efficient schemas reduce rent 和 compute pressure while preserving auditability.\n\nA production game model should also define anti-cheat boundaries. Even 使用 deterministic logic, you need clear validation 用于 illegal actions, stale turns, 和 duplicate submissions.\n\nReliable game infrastructure is measured by predictable outcomes under stress: same input actions, same resulting state, clear reject reasons 用于 invalid actions.",
            "duration": "45 min"
          },
          "lesson-46-1-2": {
            "title": "Turn Manager Challenge",
            "content": "Implement turn-based game mechanics 使用 action validation.",
            "duration": "45 min"
          },
          "lesson-46-1-3": {
            "title": "Game State Compressor Challenge",
            "content": "Compress game state 用于 efficient on-chain storage.",
            "duration": "45 min"
          },
          "lesson-46-1-4": {
            "title": "Player Progression Tracker Challenge",
            "content": "Track player experience, levels, 和 achievements.",
            "duration": "45 min"
          }
        }
      },
      "mod-46-2": {
        "title": "Randomness 和 Fairness",
        "description": "Implement fairness-oriented randomness 和 integrity controls that keep gameplay auditable 和 dispute-resistant.",
        "lessons": {
          "lesson-46-2-1": {
            "title": "On-Chain Randomness",
            "content": "Randomness is one of the hardest fairness problems in blockchain games because execution is deterministic. Robust designs avoid naive pseudo-randomness tied directly to manipulable context.\n\n实战 fairness patterns include commit-reveal, VRF-backed randomness, 和 delayed-seed schemes. Each has latency/trust tradeoffs:\n- commit-reveal: simple 和 transparent, but requires multi-step interaction,\n- VRF: stronger unpredictability, but introduces oracle/dependency considerations,\n- delayed-seed methods: lower overhead but weaker guarantees under adversarial pressure.\n\nFairness engineering should specify:\n1) who can influence randomness inputs,\n2) when values become immutable,\n3) how unresolved rounds are handled on timeout.\n\nProduction systems should emit deterministic round evidence (commit hash, reveal value, validation result) so disputes can be resolved quickly.\n\nGame fairness is credible when randomness mechanisms are explicit, verifiable, 和 resilient to timing manipulation.",
            "duration": "45 min"
          },
          "lesson-46-2-2": {
            "title": "Commit-Reveal Challenge",
            "content": "Implement commit-reveal scheme 用于 fair randomness.",
            "duration": "45 min"
          },
          "lesson-46-2-3": {
            "title": "Dice Roller Challenge",
            "content": "Implement verifiable dice rolling 使用 randomness.",
            "duration": "45 min"
          },
          "lesson-46-2-4": {
            "title": "Loot Table Challenge",
            "content": "Implement weighted loot tables 用于 game rewards.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-permanent-storage": {
    "title": "Permanent Storage 和 Arweave",
    "description": "Integrate permanent decentralized storage 使用 Solana using Arweave-style workflows: content addressing, manifest integrity, 和 verifiable long-term data access.",
    "duration": "4 weeks",
    "tags": [
      "storage",
      "arweave",
      "permanent",
      "bundling",
      "manifest"
    ],
    "modules": {
      "mod-47-1": {
        "title": "Arweave Fundamentals",
        "description": "Understand permanent-storage architecture 和 build deterministic linking between Solana state 和 external immutable content.",
        "lessons": {
          "lesson-47-1-1": {
            "title": "Permanent Storage Architecture",
            "content": "Permanent storage integration is a data durability contract. On Solana, storing full content on-chain is often impractical, so systems rely on immutable external storage references anchored by on-chain metadata.\n\nA robust architecture defines:\n1) canonical content identifiers,\n2) integrity verification method,\n3) fallback retrieval behavior,\n4) lifecycle policy 用于 metadata updates.\n\nContent-addressed 设计 is critical. If identifiers are not tied to content hash semantics, integrity guarantees weaken 和 replayed/wrong assets can be served.\n\nStorage integration should also separate control-plane 和 data-plane concerns: on-chain records govern ownership/version pointers, while external storage handles large payload persistence.\n\nProduction reliability requires deterministic verification reports (ID format validity, expected hash match, availability checks). This makes failures diagnosable 和 prevents silent corruption.\n\nPermanent storage systems succeed when users can independently verify that referenced content matches what 治理 or protocol state claims.",
            "duration": "45 min"
          },
          "lesson-47-1-2": {
            "title": "交易 ID 验证者 Challenge",
            "content": "Validate Arweave 交易 IDs 和 URLs.",
            "duration": "45 min"
          },
          "lesson-47-1-3": {
            "title": "Storage Cost Estimator Challenge",
            "content": "Estimate Arweave storage costs based on data size.",
            "duration": "45 min"
          },
          "lesson-47-1-4": {
            "title": "Bundle Optimizer Challenge",
            "content": "Optimize data bundling 用于 efficient Arweave uploads.",
            "duration": "45 min"
          }
        }
      },
      "mod-47-2": {
        "title": "Manifests 和 Verification",
        "description": "Work 使用 manifests, verification pipelines, 和 cost/性能 controls 用于 reliable long-lived data serving.",
        "lessons": {
          "lesson-47-2-1": {
            "title": "Arweave Manifests",
            "content": "Manifests turn many stored assets into one navigable root, but they introduce their own integrity responsibilities. A manifest is only trustworthy if path mapping 和 referenced content IDs are validated consistently.\n\nKey safeguards:\n- deterministic path normalization,\n- duplicate/ambiguous key rejection,\n- strict 交易-ID validation,\n- recursive integrity checks 用于 referenced content.\n\nManifest tooling should produce auditable outputs: resolved entries count, missing references, 和 hash verification status by path.\n\nFrom an operational standpoint, cost optimization should not compromise integrity. Bundling strategies, compression, 和 metadata minimization are useful only if verification remains straightforward 和 deterministic.\n\nWell-run permanent-storage pipelines treat manifests as governed artifacts 使用 versioned schema expectations 和 repeatable validation in CI.",
            "duration": "45 min"
          },
          "lesson-47-2-2": {
            "title": "Manifest Builder Challenge",
            "content": "Build 和 parse Arweave manifests.",
            "duration": "45 min"
          },
          "lesson-47-2-3": {
            "title": "Data Verifier Challenge",
            "content": "Verify data integrity 和 availability on Arweave.",
            "duration": "45 min"
          },
          "lesson-47-2-4": {
            "title": "Storage Indexer Challenge",
            "content": "Index 和 query stored data by tags.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-staking-economics": {
    "title": "Staking 和 验证者 Economics",
    "description": "Understand Solana staking 和 验证者 economics 用于 real-world decision-making: delegation strategy, reward dynamics, commission effects, 和 operational sustainability.",
    "duration": "5 weeks",
    "tags": [
      "staking",
      "validator",
      "delegation",
      "rewards",
      "economics"
    ],
    "modules": {
      "mod-48-1": {
        "title": "Staking Fundamentals",
        "description": "学习 native staking mechanics 使用 deterministic reward modeling, 验证者 selection criteria, 和 delegation risk framing.",
        "lessons": {
          "lesson-48-1-1": {
            "title": "Solana Staking Architecture",
            "content": "Solana staking economics is an incentives system connecting delegators, 验证者, 和 network 安全. Good delegation decisions require more than chasing headline APY.\n\nDelegators should evaluate:\n1) 验证者 性能 consistency,\n2) commission policy 和 changes over time,\n3) uptime 和 vote behavior,\n4) concentration risk across the 验证者 set.\n\nReward modeling should be deterministic 和 transparent. Calculations must show gross rewards, commission effects, 和 net delegator outcome under explicit assumptions.\n\nDiversification matters. Concentrating stake purely on top performers can increase ecosystem centralization risk even if short-term yield appears higher.\n\nProduction staking tooling should expose scenario analysis (commission changes, 性能 drops, epoch variance) so delegators can make resilient choices rather than reactive moves.\n\nStaking quality is measured by sustainable net returns plus contribution to healthy 验证者 distribution.",
            "duration": "45 min"
          },
          "lesson-48-1-2": {
            "title": "Staking Rewards Calculator Challenge",
            "content": "Calculate staking rewards 使用 commission 和 inflation.",
            "duration": "45 min"
          },
          "lesson-48-1-3": {
            "title": "验证者 Selector Challenge",
            "content": "Select 验证者 based on 性能 和 commission.",
            "duration": "45 min"
          },
          "lesson-48-1-4": {
            "title": "Stake Rebalancing Challenge",
            "content": "Optimize stake distribution across 验证者.",
            "duration": "45 min"
          }
        }
      },
      "mod-48-2": {
        "title": "验证者 Operations",
        "description": "Analyze 验证者-side economics, operational cost pressure, 和 incentive alignment 用于 long-term network health.",
        "lessons": {
          "lesson-48-2-1": {
            "title": "验证者 Economics",
            "content": "验证者 economics balances revenue opportunities against operational costs 和 reliability obligations. Sustainable 验证者 optimize 用于 long-term trust, not short-term extraction.\n\nRevenue sources include inflation rewards 和 fee-related earnings; cost structure includes hardware, networking, maintenance, 和 operational staffing.\n\nKey operational metrics 用于 验证者 viability:\n- effective uptime 和 vote success,\n- commission competitiveness,\n- stake retention trend,\n- incident frequency 和 recovery quality.\n\nCommission strategy should be explicit 和 predictable. Sudden commission spikes can damage delegator trust 和 long-term stake stability.\n\nEconomic analysis should include downside modeling: reduced stake, higher incident costs, or prolonged 性能 degradation.\n\nHealthy 验证者 economics supports network resilience. Tooling should help operators 和 delegators reason about sustainability, not just peak-period earnings.",
            "duration": "45 min"
          },
          "lesson-48-2-2": {
            "title": "验证者 Profit Calculator Challenge",
            "content": "Calculate 验证者 profitability.",
            "duration": "45 min"
          },
          "lesson-48-2-3": {
            "title": "Epoch Schedule Calculator Challenge",
            "content": "Calculate epoch timing 和 reward distribution schedules.",
            "duration": "45 min"
          },
          "lesson-48-2-4": {
            "title": "Stake 账户 Manager Challenge",
            "content": "Manage stake 账户 lifecycle including activation 和 deactivation.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-account-abstraction": {
    "title": "账户 Abstraction 和 Smart 钱包",
    "description": "Implement smart-钱包/账户-abstraction patterns on Solana 使用 programmable authorization, recovery controls, 和 policy-driven 交易 validation.",
    "duration": "6 weeks",
    "tags": [
      "account-abstraction",
      "smart-wallet",
      "multisig",
      "recovery",
      "session-keys"
    ],
    "modules": {
      "mod-49-1": {
        "title": "Smart 钱包 Fundamentals",
        "description": "Build smart-钱包 fundamentals including multisig 和 social-recovery designs 使用 clear trust 和 failure boundaries.",
        "lessons": {
          "lesson-49-1-1": {
            "title": "账户 Abstraction on Solana",
            "content": "账户 abstraction on Solana shifts control from a single key to programmable policy. Smart 钱包 can enforce richer authorization logic, but policy complexity must be managed carefully.\n\nA robust smart-钱包 设计 defines:\n1) authority model (owners/guardians/delegates),\n2) policy scope (what can be approved automatically vs manually),\n3) recovery path (how access is restored safely).\n\nMultisig 和 social recovery are powerful, but both need deterministic state transitions 和 explicit quorum rules. Ambiguous transitions create lockout or unauthorized-access risk.\n\nSmart-钱包 systems should emit structured authorization evidence 用于 each action: which policy matched, which signers approved, 和 which constraints passed.\n\nProduction reliability depends on clear emergency controls: pause paths, guardian rotation, 和 recovery cooldowns.\n\n账户 abstraction is successful when flexibility increases safety 和 usability together, not when policy logic becomes opaque.",
            "duration": "45 min"
          },
          "lesson-49-1-2": {
            "title": "Multi-Signature 钱包 Challenge",
            "content": "Implement M-of-N multi-signature 钱包.",
            "duration": "45 min"
          },
          "lesson-49-1-3": {
            "title": "Social Recovery Challenge",
            "content": "Implement social recovery 使用 guardians.",
            "duration": "45 min"
          },
          "lesson-49-1-4": {
            "title": "Session Key Manager Challenge",
            "content": "Manage temporary session keys 使用 limited permissions.",
            "duration": "45 min"
          }
        }
      },
      "mod-49-2": {
        "title": "Programmable Validation",
        "description": "Implement programmable validation policies (limits, allowlists, time/risk rules) 使用 deterministic enforcement 和 auditability.",
        "lessons": {
          "lesson-49-2-1": {
            "title": "Custom Validation Rules",
            "content": "Programmable validation is where smart 钱包 deliver real value, but it is also where subtle policy bugs appear.\n\nTypical controls include spending limits, destination allowlists, time windows, 和 risk-score gates. These controls should be deterministic 和 composable, 使用 explicit precedence rules.\n\n设计 principles:\n- fail closed on ambiguous policy matches,\n- keep policy evaluation order stable,\n- attach machine-readable reason codes to approve/reject outcomes.\n\nValidation systems should also support policy explainability. Users 和 auditors need to understand why a 交易 was blocked or approved.\n\n用于 production deployments, policy changes should be versioned 和 test-fixtured. A new rule must be validated against prior known-good scenarios to avoid accidental lockouts or bypasses.\n\nProgrammable 钱包 are strongest when validation logic is transparent, testable, 和 operationally reversible.",
            "duration": "45 min"
          },
          "lesson-49-2-2": {
            "title": "Spending Limit Enforcer Challenge",
            "content": "Enforce daily 和 per-交易 spending limits.",
            "duration": "45 min"
          },
          "lesson-49-2-3": {
            "title": "Whitelist Enforcer Challenge",
            "content": "Enforce destination whitelists 用于 交易.",
            "duration": "45 min"
          },
          "lesson-49-2-4": {
            "title": "Time Lock Enforcer Challenge",
            "content": "Enforce time-based restrictions on 交易.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-pda-mastery": {
    "title": "程序派生地址 Mastery",
    "description": "Master 高级 PDA engineering on Solana: seed schema 设计, bump handling discipline, 和 secure cross-program PDA usage at production scale.",
    "duration": "6 weeks",
    "tags": [
      "pda",
      "program-derived-address",
      "seeds",
      "bump",
      "deterministic"
    ],
    "modules": {
      "mod-50-1": {
        "title": "PDA Fundamentals",
        "description": "Build strong PDA foundations 使用 deterministic derivation, canonical seed composition, 和 collision-resistant namespace strategy.",
        "lessons": {
          "lesson-50-1-1": {
            "title": "程序派生地址",
            "content": "程序派生地址 (PDAs) are deterministic authority 和 state anchors on Solana. Their power comes from predictable derivation; their risk comes from inconsistent seed discipline.\n\nA strong PDA 设计 standard defines:\n1) canonical seed order,\n2) explicit namespace/domain tags,\n3) bump handling rules,\n4) versioning strategy 用于 future evolution.\n\nSeed ambiguity is a common source of bugs. If different handlers derive the same concept 使用 different seed ordering, identity checks become inconsistent 和 安全 assumptions break.\n\nPDA validation should always re-derive expected addresses on the trusted side 和 compare exact keys before mutating state.\n\nProduction teams should document seed recipes as API contracts. Changing recipes without migration planning can orphan state 和 break clients.\n\nPDA mastery is mostly discipline: deterministic derivation everywhere, no implicit conventions, no trust in client-provided derivation claims.",
            "duration": "45 min"
          },
          "lesson-50-1-2": {
            "title": "PDA Generator Challenge",
            "content": "Implement PDA generation 使用 seed validation.",
            "duration": "45 min"
          },
          "lesson-50-1-3": {
            "title": "Seed Composer Challenge",
            "content": "Compose complex seed patterns 用于 different use cases.",
            "duration": "45 min"
          },
          "lesson-50-1-4": {
            "title": "Bump Manager Challenge",
            "content": "Manage bump seeds 用于 PDA verification 和 signing.",
            "duration": "45 min"
          }
        }
      },
      "mod-50-2": {
        "title": "高级 PDA Patterns",
        "description": "Implement 高级 PDA patterns (nested/counter/stateful) while preserving 安全 invariants 和 migration safety.",
        "lessons": {
          "lesson-50-2-1": {
            "title": "PDA 设计 Patterns",
            "content": "高级 PDA patterns solve real scaling 和 composability needs but increase 设计 complexity.\n\nNested PDAs, counter-based PDAs, 和 multi-tenant PDA namespaces each require explicit invariants around uniqueness, lifecycle, 和 authority boundaries.\n\nKey safeguards:\n- monotonic counters 使用 replay protection,\n- collision checks in shared namespaces,\n- explicit ownership checks on all derived-state paths,\n- deterministic migration paths when schema/seed versions evolve.\n\nCross-program PDA interactions must minimize signer scope. invoke_signed should only grant exactly what downstream steps require.\n\nOperationally, 高级 PDA systems need deterministic audit artifacts: derivation inputs, expected outputs, 和 validation results by 指令 path.\n\nComplex PDA architecture is safe when derivation logic remains simple to reason about 和 impossible to interpret ambiguously.",
            "duration": "45 min"
          },
          "lesson-50-2-2": {
            "title": "Nested PDA Generator Challenge",
            "content": "Generate PDAs derived from other PDA addresses.",
            "duration": "45 min"
          },
          "lesson-50-2-3": {
            "title": "Counter PDA Generator Challenge",
            "content": "Generate unique PDAs using incrementing counters.",
            "duration": "45 min"
          },
          "lesson-50-2-4": {
            "title": "PDA Collision Detector Challenge",
            "content": "Detect 和 prevent PDA seed collisions.",
            "duration": "45 min"
          }
        }
      }
    }
  },
  "solana-economics": {
    "title": "Solana Economics 和 Token Flows",
    "description": "Analyze Solana economic dynamics in production context: inflation/fee-burn interplay, staking flows, supply movement, 和 protocol sustainability tradeoffs.",
    "duration": "5 weeks",
    "tags": [
      "economics",
      "inflation",
      "fees",
      "rent",
      "token-flows",
      "sustainability"
    ],
    "modules": {
      "mod-51-1": {
        "title": "Solana Economic Model",
        "description": "Understand Solana macro token economics (inflation, burn, rewards, fees) 使用 deterministic scenario modeling.",
        "lessons": {
          "lesson-51-1-1": {
            "title": "Solana Token Economics",
            "content": "Solana economics is the interaction of issuance, burn, staking rewards, 和 usage demand. Sustainable protocol decisions require understanding these flows as a system, not isolated metrics.\n\nCore mechanisms include:\n1) inflation schedule 和 long-term emission behavior,\n2) fee burn 和 验证者 reward pathways,\n3) staking participation effects on circulating supply.\n\nEconomic analysis should be scenario-driven. Single-point estimates hide risk. Teams should model calm/high-usage/low-usage regimes 和 compare supply-pressure outcomes.\n\nDeterministic calculators are useful 用于 治理 和 product planning because they make assumptions explicit: epoch cadence, fee volume, staking ratio, 和 unlock schedules.\n\nHealthy economic reasoning links network-level flows to protocol-level choices (treasury policy, incentive emissions, fee strategy).\n\nEconomic quality improves when teams publish assumption-driven reports instead of headline narratives.",
            "duration": "45 min"
          },
          "lesson-51-1-2": {
            "title": "Inflation Calculator Challenge",
            "content": "Calculate inflation rate 和 staking rewards over time.",
            "duration": "45 min"
          },
          "lesson-51-1-3": {
            "title": "Fee Burn Calculator Challenge",
            "content": "Calculate fee burns 和 their deflationary impact.",
            "duration": "45 min"
          },
          "lesson-51-1-4": {
            "title": "Rent Economics Calculator Challenge",
            "content": "Calculate rent costs 和 exemption thresholds.",
            "duration": "45 min"
          }
        }
      },
      "mod-51-2": {
        "title": "Token Flow Analysis",
        "description": "Model token flow dynamics 和 sustainability signals using supply categories, unlock events, 和 behavior-driven liquidity effects.",
        "lessons": {
          "lesson-51-2-1": {
            "title": "Token Flow Dynamics",
            "content": "Token flow analysis turns abstract economics into operational insight. The key is to track where tokens are (staked, circulating, locked, treasury, pending unlock) 和 how they move over time.\n\nUseful flow metrics include:\n- net circulating change,\n- staking inflow/outflow trend,\n- unlock cliff concentration,\n- treasury spend velocity.\n\nUnlock events should be modeled 用于 market-impact risk. Large clustered unlocks can create short-term supply shock even when long-term tokenomics is sound.\n\nFlow tooling should support deterministic category accounting 和 conservation checks (total categorized supply consistency).\n\n用于 治理, flow analysis is most valuable when tied to policy actions: adjust emissions, change vesting cadence, alter incentive programs.\n\nSustainable token systems are not static designs; they are continuously monitored flow systems 使用 explicit policy feedback loops.",
            "duration": "45 min"
          },
          "lesson-51-2-2": {
            "title": "Supply Flow Tracker Challenge",
            "content": "Track token supply categories 和 flows.",
            "duration": "45 min"
          },
          "lesson-51-2-3": {
            "title": "Vesting Schedule Impact Challenge",
            "content": "Calculate token unlock impact on supply.",
            "duration": "45 min"
          },
          "lesson-51-2-4": {
            "title": "Protocol Sustainability Score Challenge",
            "content": "Calculate sustainability metrics 用于 tokenomics.",
            "duration": "45 min"
          }
        }
      }
    }
  }
};
